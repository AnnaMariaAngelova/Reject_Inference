{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 54;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 54;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 55;\n",
       "                var nbb_unformatted_code = \"import sys\\n\\nsys.path.insert(\\n    0,\\n    r\\\"C:\\\\Users\\\\Asus\\\\Desktop\\\\Repo\\\\MasterThesis_RI\\\\Python-Real-World-Machine-Learning\\\\Module 2\\\\Chapter 5\\\",\\n)\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Basic Packages\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Sampling\\nfrom modAL.uncertainty import uncertainty_sampling\\nfrom modAL.density import information_density\\n\\n# Modelling\\n# Classification\\nimport statsmodels.api as sm\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom lightgbm import LGBMClassifier\\nfrom sklearn import svm\\nfrom sklearn.svm import SVC\\n\\n# Semi-Supervised Learning\\nfrom sklearn.semi_supervised import (\\n    LabelPropagation,\\n    LabelSpreading,\\n    SelfTrainingClassifier,\\n)\\nfrom modAL.models import ActiveLearner\\n\\n# Chapter 5\\nfrom SelfLearning import SelfLearningModel\\nfrom scikitWQDA import WQDA\\n\\n# Model Selection\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\n\\n# Metrics\\nfrom sklearn.metrics import (\\n    f1_score,\\n    roc_auc_score,\\n    roc_curve,\\n    accuracy_score,\\n    confusion_matrix,\\n    plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n    log_loss,\\n)\\n\\n# Ensembling\\nfrom sklearn.ensemble import *\\n\\n# Balancing\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# Visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Binning\\nimport woeBinningPandas\\n\\n# Create Unique ID\\nimport uuid\";\n",
       "                var nbb_formatted_code = \"import sys\\n\\nsys.path.insert(\\n    0,\\n    r\\\"C:\\\\Users\\\\Asus\\\\Desktop\\\\Repo\\\\MasterThesis_RI\\\\Python-Real-World-Machine-Learning\\\\Module 2\\\\Chapter 5\\\",\\n)\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Basic Packages\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Sampling\\nfrom modAL.uncertainty import uncertainty_sampling\\nfrom modAL.density import information_density\\n\\n# Modelling\\n# Classification\\nimport statsmodels.api as sm\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom lightgbm import LGBMClassifier\\nfrom sklearn import svm\\nfrom sklearn.svm import SVC\\n\\n# Semi-Supervised Learning\\nfrom sklearn.semi_supervised import (\\n    LabelPropagation,\\n    LabelSpreading,\\n    SelfTrainingClassifier,\\n)\\nfrom modAL.models import ActiveLearner\\n\\n# Chapter 5\\nfrom SelfLearning import SelfLearningModel\\nfrom scikitWQDA import WQDA\\n\\n# Model Selection\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\n\\n# Metrics\\nfrom sklearn.metrics import (\\n    f1_score,\\n    roc_auc_score,\\n    roc_curve,\\n    accuracy_score,\\n    confusion_matrix,\\n    plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n    log_loss,\\n)\\n\\n# Ensembling\\nfrom sklearn.ensemble import *\\n\\n# Balancing\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# Visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Binning\\nimport woeBinningPandas\\n\\n# Create Unique ID\\nimport uuid\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 55;\n",
       "                var nbb_unformatted_code = \"import sys\\n\\nsys.path.insert(\\n    0,\\n    r\\\"C:\\\\Users\\\\Asus\\\\Desktop\\\\Repo\\\\MasterThesis_RI\\\\Python-Real-World-Machine-Learning\\\\Module 2\\\\Chapter 5\\\",\\n)\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Basic Packages\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Sampling\\nfrom modAL.uncertainty import uncertainty_sampling\\nfrom modAL.density import information_density\\n\\n# Modelling\\n# Classification\\nimport statsmodels.api as sm\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom lightgbm import LGBMClassifier\\nfrom sklearn import svm\\nfrom sklearn.svm import SVC\\n\\n# Semi-Supervised Learning\\nfrom sklearn.semi_supervised import (\\n    LabelPropagation,\\n    LabelSpreading,\\n    SelfTrainingClassifier,\\n)\\nfrom modAL.models import ActiveLearner\\n\\n# Chapter 5\\nfrom SelfLearning import SelfLearningModel\\nfrom scikitWQDA import WQDA\\n\\n# Model Selection\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\n\\n# Metrics\\nfrom sklearn.metrics import (\\n    f1_score,\\n    roc_auc_score,\\n    roc_curve,\\n    accuracy_score,\\n    confusion_matrix,\\n    plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n    log_loss,\\n)\\n\\n# Ensembling\\nfrom sklearn.ensemble import *\\n\\n# Balancing\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# Visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Binning\\nimport woeBinningPandas\\n\\n# Create Unique ID\\nimport uuid\";\n",
       "                var nbb_formatted_code = \"import sys\\n\\nsys.path.insert(\\n    0,\\n    r\\\"C:\\\\Users\\\\Asus\\\\Desktop\\\\Repo\\\\MasterThesis_RI\\\\Python-Real-World-Machine-Learning\\\\Module 2\\\\Chapter 5\\\",\\n)\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Basic Packages\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Sampling\\nfrom modAL.uncertainty import uncertainty_sampling\\nfrom modAL.density import information_density\\n\\n# Modelling\\n# Classification\\nimport statsmodels.api as sm\\nfrom sklearn.ensemble import IsolationForest\\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom lightgbm import LGBMClassifier\\nfrom sklearn import svm\\nfrom sklearn.svm import SVC\\n\\n# Semi-Supervised Learning\\nfrom sklearn.semi_supervised import (\\n    LabelPropagation,\\n    LabelSpreading,\\n    SelfTrainingClassifier,\\n)\\nfrom modAL.models import ActiveLearner\\n\\n# Chapter 5\\nfrom SelfLearning import SelfLearningModel\\nfrom scikitWQDA import WQDA\\n\\n# Model Selection\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\n\\n# Metrics\\nfrom sklearn.metrics import (\\n    f1_score,\\n    roc_auc_score,\\n    roc_curve,\\n    accuracy_score,\\n    confusion_matrix,\\n    plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n    log_loss,\\n)\\n\\n# Ensembling\\nfrom sklearn.ensemble import *\\n\\n# Balancing\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# Visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Binning\\nimport woeBinningPandas\\n\\n# Create Unique ID\\nimport uuid\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    r\"C:\\Users\\Asus\\Desktop\\Repo\\MasterThesis_RI\\Python-Real-World-Machine-Learning\\Module 2\\Chapter 5\",\n",
    ")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Basic Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sampling\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from modAL.density import information_density\n",
    "\n",
    "# Modelling\n",
    "# Classification\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Semi-Supervised Learning\n",
    "from sklearn.semi_supervised import (\n",
    "    LabelPropagation,\n",
    "    LabelSpreading,\n",
    "    SelfTrainingClassifier,\n",
    ")\n",
    "from modAL.models import ActiveLearner\n",
    "\n",
    "# Chapter 5\n",
    "from SelfLearning import SelfLearningModel\n",
    "from scikitWQDA import WQDA\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    log_loss,\n",
    ")\n",
    "\n",
    "# Ensembling\n",
    "from sklearn.ensemble import *\n",
    "\n",
    "# Balancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Binning\n",
    "import woeBinningPandas\n",
    "\n",
    "# Create Unique ID\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, basic data preprocessing to obtain accepted and rejected training and test samples separately. Save rejected data in two versions: with and without lables. The rejected data without labels is needed for the semi-supervised model. The rejected data without labels is needed to perform evaluation. <br>\n",
    "*Note for improvement: artificial id is currently generated as unique key to be used in merging. We can merge on index instead. Drop for future.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 56;\n",
       "                var nbb_unformatted_code = \"def data_preprocessing(df, accepted_flag, target, train_ratio):\\n    \\\"\\\"\\\"\\n    The goal of this function is to load the original dataset, split it into accepts and rejects,\\n    add ids, which can later be used for merging. For the rejects to further perform train / test split\\n\\n    Parameters\\n    ----------\\n\\n    df : name of the original dataset in quotation marks, csv format\\n    accepted_flag: name of the accepted flag; Binary: 1 if accepted, 0 if rejected\\n    target : name of the target column\\n    train_ratio : percentage used for training; Continuous (0,1)\\n\\n    Return\\n    ------\\n    a : accepted data\\n    r : rejected data\\n    r_dev : rejected trainining data without label\\n    r_test : rejected testing data without label\\n    dfr_dev_with_label: rejected training data with label\\n    dft_test_with_label: rejected training data with label\\n\\n    \\\"\\\"\\\"\\n    # Load data\\n    data = pd.read_csv(\\\"C:/Users/Asus/Desktop/Repo/MasterThesis_RI/Data_09_05/\\\" + df)\\n\\n    # Accepted\\n\\n    ## Create separate dataset with accepts\\n    dfa = data[data[accepted_flag] == 1]\\n    dfa = dfa.drop([accepted_flag], axis=1)\\n    ## Rename target variable as \\\"target\\\"\\n    dfa = dfa.rename(columns={target: \\\"target\\\"})\\n    ## Add id to the dataset, which can later be used for merging\\n    dfa[\\\"id\\\"] = dfa.index.to_series().map(lambda x: uuid.uuid4())\\n\\n    # Rejected\\n\\n    ## Create separate dataset with accepts\\n    dfr = data[data[accepted_flag] == 0]\\n    dfr = dfr.drop([accepted_flag], axis=1)\\n    ## Add id to the dataset, which can later be used for merging\\n    dfr[\\\"id\\\"] = dfr.index.to_series().map(lambda x: uuid.uuid4())\\n    ## Train/Test Split (without labels)\\n    ### Shuffle the dataset\\n    shuffle_df = dfr.sample(frac=1, random_state=42)\\n    ### Define a size for the train set\\n    train_size = int(train_ratio * len(shuffle_df))\\n    ### Split the dataset\\n    dfr_dev = shuffle_df[:train_size]\\n    dfr_test = shuffle_df[train_size:]\\n    ## Save a copy of the rejected data with label\\n    dfr_dev_with_label = dfr_dev\\n    dfr_test_with_label = dfr_test\\n    ## Unlabel the rejects (i.e. drop the target) and save a copy of the rejeted data without label\\n    dfr_dev2 = dfr_dev_with_label.drop([target], axis=1)\\n    dfr_test2 = dfr_test_with_label.drop([target], axis=1)\\n\\n    return dfr_dev_with_label, dfr_test_with_label, dfa, dfr, dfr_dev2, dfr_test2\";\n",
       "                var nbb_formatted_code = \"def data_preprocessing(df, accepted_flag, target, train_ratio):\\n    \\\"\\\"\\\"\\n    The goal of this function is to load the original dataset, split it into accepts and rejects,\\n    add ids, which can later be used for merging. For the rejects to further perform train / test split\\n\\n    Parameters\\n    ----------\\n\\n    df : name of the original dataset in quotation marks, csv format\\n    accepted_flag: name of the accepted flag; Binary: 1 if accepted, 0 if rejected\\n    target : name of the target column\\n    train_ratio : percentage used for training; Continuous (0,1)\\n\\n    Return\\n    ------\\n    a : accepted data\\n    r : rejected data\\n    r_dev : rejected trainining data without label\\n    r_test : rejected testing data without label\\n    dfr_dev_with_label: rejected training data with label\\n    dft_test_with_label: rejected training data with label\\n\\n    \\\"\\\"\\\"\\n    # Load data\\n    data = pd.read_csv(\\\"C:/Users/Asus/Desktop/Repo/MasterThesis_RI/Data_09_05/\\\" + df)\\n\\n    # Accepted\\n\\n    ## Create separate dataset with accepts\\n    dfa = data[data[accepted_flag] == 1]\\n    dfa = dfa.drop([accepted_flag], axis=1)\\n    ## Rename target variable as \\\"target\\\"\\n    dfa = dfa.rename(columns={target: \\\"target\\\"})\\n    ## Add id to the dataset, which can later be used for merging\\n    dfa[\\\"id\\\"] = dfa.index.to_series().map(lambda x: uuid.uuid4())\\n\\n    # Rejected\\n\\n    ## Create separate dataset with accepts\\n    dfr = data[data[accepted_flag] == 0]\\n    dfr = dfr.drop([accepted_flag], axis=1)\\n    ## Add id to the dataset, which can later be used for merging\\n    dfr[\\\"id\\\"] = dfr.index.to_series().map(lambda x: uuid.uuid4())\\n    ## Train/Test Split (without labels)\\n    ### Shuffle the dataset\\n    shuffle_df = dfr.sample(frac=1, random_state=42)\\n    ### Define a size for the train set\\n    train_size = int(train_ratio * len(shuffle_df))\\n    ### Split the dataset\\n    dfr_dev = shuffle_df[:train_size]\\n    dfr_test = shuffle_df[train_size:]\\n    ## Save a copy of the rejected data with label\\n    dfr_dev_with_label = dfr_dev\\n    dfr_test_with_label = dfr_test\\n    ## Unlabel the rejects (i.e. drop the target) and save a copy of the rejeted data without label\\n    dfr_dev2 = dfr_dev_with_label.drop([target], axis=1)\\n    dfr_test2 = dfr_test_with_label.drop([target], axis=1)\\n\\n    return dfr_dev_with_label, dfr_test_with_label, dfa, dfr, dfr_dev2, dfr_test2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 56;\n",
       "                var nbb_unformatted_code = \"def data_preprocessing(df, accepted_flag, target, train_ratio):\\n    \\\"\\\"\\\"\\n    The goal of this function is to load the original dataset, split it into accepts and rejects,\\n    add ids, which can later be used for merging. For the rejects to further perform train / test split\\n\\n    Parameters\\n    ----------\\n\\n    df : name of the original dataset in quotation marks, csv format\\n    accepted_flag: name of the accepted flag; Binary: 1 if accepted, 0 if rejected\\n    target : name of the target column\\n    train_ratio : percentage used for training; Continuous (0,1)\\n\\n    Return\\n    ------\\n    a : accepted data\\n    r : rejected data\\n    r_dev : rejected trainining data without label\\n    r_test : rejected testing data without label\\n    dfr_dev_with_label: rejected training data with label\\n    dft_test_with_label: rejected training data with label\\n\\n    \\\"\\\"\\\"\\n    # Load data\\n    data = pd.read_csv(\\\"C:/Users/Asus/Desktop/Repo/MasterThesis_RI/Data_09_05/\\\" + df)\\n\\n    # Accepted\\n\\n    ## Create separate dataset with accepts\\n    dfa = data[data[accepted_flag] == 1]\\n    dfa = dfa.drop([accepted_flag], axis=1)\\n    ## Rename target variable as \\\"target\\\"\\n    dfa = dfa.rename(columns={target: \\\"target\\\"})\\n    ## Add id to the dataset, which can later be used for merging\\n    dfa[\\\"id\\\"] = dfa.index.to_series().map(lambda x: uuid.uuid4())\\n\\n    # Rejected\\n\\n    ## Create separate dataset with accepts\\n    dfr = data[data[accepted_flag] == 0]\\n    dfr = dfr.drop([accepted_flag], axis=1)\\n    ## Add id to the dataset, which can later be used for merging\\n    dfr[\\\"id\\\"] = dfr.index.to_series().map(lambda x: uuid.uuid4())\\n    ## Train/Test Split (without labels)\\n    ### Shuffle the dataset\\n    shuffle_df = dfr.sample(frac=1, random_state=42)\\n    ### Define a size for the train set\\n    train_size = int(train_ratio * len(shuffle_df))\\n    ### Split the dataset\\n    dfr_dev = shuffle_df[:train_size]\\n    dfr_test = shuffle_df[train_size:]\\n    ## Save a copy of the rejected data with label\\n    dfr_dev_with_label = dfr_dev\\n    dfr_test_with_label = dfr_test\\n    ## Unlabel the rejects (i.e. drop the target) and save a copy of the rejeted data without label\\n    dfr_dev2 = dfr_dev_with_label.drop([target], axis=1)\\n    dfr_test2 = dfr_test_with_label.drop([target], axis=1)\\n\\n    return dfr_dev_with_label, dfr_test_with_label, dfa, dfr, dfr_dev2, dfr_test2\";\n",
       "                var nbb_formatted_code = \"def data_preprocessing(df, accepted_flag, target, train_ratio):\\n    \\\"\\\"\\\"\\n    The goal of this function is to load the original dataset, split it into accepts and rejects,\\n    add ids, which can later be used for merging. For the rejects to further perform train / test split\\n\\n    Parameters\\n    ----------\\n\\n    df : name of the original dataset in quotation marks, csv format\\n    accepted_flag: name of the accepted flag; Binary: 1 if accepted, 0 if rejected\\n    target : name of the target column\\n    train_ratio : percentage used for training; Continuous (0,1)\\n\\n    Return\\n    ------\\n    a : accepted data\\n    r : rejected data\\n    r_dev : rejected trainining data without label\\n    r_test : rejected testing data without label\\n    dfr_dev_with_label: rejected training data with label\\n    dft_test_with_label: rejected training data with label\\n\\n    \\\"\\\"\\\"\\n    # Load data\\n    data = pd.read_csv(\\\"C:/Users/Asus/Desktop/Repo/MasterThesis_RI/Data_09_05/\\\" + df)\\n\\n    # Accepted\\n\\n    ## Create separate dataset with accepts\\n    dfa = data[data[accepted_flag] == 1]\\n    dfa = dfa.drop([accepted_flag], axis=1)\\n    ## Rename target variable as \\\"target\\\"\\n    dfa = dfa.rename(columns={target: \\\"target\\\"})\\n    ## Add id to the dataset, which can later be used for merging\\n    dfa[\\\"id\\\"] = dfa.index.to_series().map(lambda x: uuid.uuid4())\\n\\n    # Rejected\\n\\n    ## Create separate dataset with accepts\\n    dfr = data[data[accepted_flag] == 0]\\n    dfr = dfr.drop([accepted_flag], axis=1)\\n    ## Add id to the dataset, which can later be used for merging\\n    dfr[\\\"id\\\"] = dfr.index.to_series().map(lambda x: uuid.uuid4())\\n    ## Train/Test Split (without labels)\\n    ### Shuffle the dataset\\n    shuffle_df = dfr.sample(frac=1, random_state=42)\\n    ### Define a size for the train set\\n    train_size = int(train_ratio * len(shuffle_df))\\n    ### Split the dataset\\n    dfr_dev = shuffle_df[:train_size]\\n    dfr_test = shuffle_df[train_size:]\\n    ## Save a copy of the rejected data with label\\n    dfr_dev_with_label = dfr_dev\\n    dfr_test_with_label = dfr_test\\n    ## Unlabel the rejects (i.e. drop the target) and save a copy of the rejeted data without label\\n    dfr_dev2 = dfr_dev_with_label.drop([target], axis=1)\\n    dfr_test2 = dfr_test_with_label.drop([target], axis=1)\\n\\n    return dfr_dev_with_label, dfr_test_with_label, dfa, dfr, dfr_dev2, dfr_test2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_preprocessing(df, accepted_flag, target, train_ratio):\n",
    "    \"\"\"\n",
    "    The goal of this function is to load the original dataset, split it into accepts and rejects,\n",
    "    add ids, which can later be used for merging. For the rejects to further perform train / test split\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : name of the original dataset in quotation marks, csv format\n",
    "    accepted_flag: name of the accepted flag; Binary: 1 if accepted, 0 if rejected\n",
    "    target : name of the target column\n",
    "    train_ratio : percentage used for training; Continuous (0,1)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a : accepted data\n",
    "    r : rejected data\n",
    "    r_dev : rejected trainining data without label\n",
    "    r_test : rejected testing data without label\n",
    "    dfr_dev_with_label: rejected training data with label\n",
    "    dft_test_with_label: rejected training data with label\n",
    "\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(\"C:/Users/Asus/Desktop/Repo/MasterThesis_RI/Data_09_05/\" + df)\n",
    "\n",
    "    # Accepted\n",
    "\n",
    "    ## Create separate dataset with accepts\n",
    "    dfa = data[data[accepted_flag] == 1]\n",
    "    dfa = dfa.drop([accepted_flag], axis=1)\n",
    "    ## Rename target variable as \"target\"\n",
    "    dfa = dfa.rename(columns={target: \"target\"})\n",
    "    ## Add id to the dataset, which can later be used for merging\n",
    "    dfa[\"id\"] = dfa.index.to_series().map(lambda x: uuid.uuid4())\n",
    "\n",
    "    # Rejected\n",
    "\n",
    "    ## Create separate dataset with accepts\n",
    "    dfr = data[data[accepted_flag] == 0]\n",
    "    dfr = dfr.drop([accepted_flag], axis=1)\n",
    "    ## Add id to the dataset, which can later be used for merging\n",
    "    dfr[\"id\"] = dfr.index.to_series().map(lambda x: uuid.uuid4())\n",
    "    ## Train/Test Split (without labels)\n",
    "    ### Shuffle the dataset\n",
    "    shuffle_df = dfr.sample(frac=1, random_state=42)\n",
    "    ### Define a size for the train set\n",
    "    train_size = int(train_ratio * len(shuffle_df))\n",
    "    ### Split the dataset\n",
    "    dfr_dev = shuffle_df[:train_size]\n",
    "    dfr_test = shuffle_df[train_size:]\n",
    "    ## Save a copy of the rejected data with label\n",
    "    dfr_dev_with_label = dfr_dev\n",
    "    dfr_test_with_label = dfr_test\n",
    "    ## Unlabel the rejects (i.e. drop the target) and save a copy of the rejeted data without label\n",
    "    dfr_dev2 = dfr_dev_with_label.drop([target], axis=1)\n",
    "    dfr_test2 = dfr_test_with_label.drop([target], axis=1)\n",
    "\n",
    "    return dfr_dev_with_label, dfr_test_with_label, dfa, dfr, dfr_dev2, dfr_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"dfr_dev_with_label, dfr_test_with_label, a, r, r_dev, r_test = data_preprocessing(\\n    \\\"paper_1.csv\\\", \\\"is_accepted\\\", \\\"y\\\", 0.8\\n)\";\n",
       "                var nbb_formatted_code = \"dfr_dev_with_label, dfr_test_with_label, a, r, r_dev, r_test = data_preprocessing(\\n    \\\"paper_1.csv\\\", \\\"is_accepted\\\", \\\"y\\\", 0.8\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfr_dev_with_label, dfr_test_with_label, a, r, r_dev, r_test = data_preprocessing(\n",
    "    \"paper_1.csv\", \"is_accepted\", \"y\", 0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 11)\n",
      "(6000, 11)\n",
      "(4800, 10)\n",
      "(1200, 10)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"print(a.shape)  # accepted\\nprint(r.shape)  # rejected\\nprint(r_dev.shape)  # rejected train\\nprint(r_test.shape)  # rejected test\";\n",
       "                var nbb_formatted_code = \"print(a.shape)  # accepted\\nprint(r.shape)  # rejected\\nprint(r_dev.shape)  # rejected train\\nprint(r_test.shape)  # rejected test\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(a.shape)  # accepted\n",
    "print(r.shape)  # rejected\n",
    "print(r_dev.shape)  # rejected train\n",
    "print(r_test.shape)  # rejected test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below two functions are used to store versions of the reject dataset with and without id. Data without id used for modelling, data with id used for merging. To be dropped in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 67;\n",
       "                var nbb_unformatted_code = \"def select_columns_rejects_without_id(r_dev, r_test, r_dev_mod, r_test_mod):\\n    \\\"\\\"\\\"\\n    The goal of this function is to create rejects datasets (train and test) with the modelling columns only.\\n    Currently hard-coded to work for a dataset with 8 features.\\n\\n    Parameters\\n    ----------\\n\\n    r_dev: rejects training data; Dataframe\\n    r_test: rejects testing data; Dataframe\\n    r_dev_mod: name of the rejecs training data without id; String\\n    r_test_mod: name of the rejects training data without id\\n\\n    Return\\n    ------\\n    r_dev_mod: rejecs training data without id; Dataframe\\n    r_test_mod: rejects training data without id; Dataframe\\n\\n    \\\"\\\"\\\"\\n\\n    r_dev_mod = r_dev.iloc[:, :9]\\n    r_test_mod = r_test.iloc[:, :9]\\n    return r_dev_mod, r_test_mod\";\n",
       "                var nbb_formatted_code = \"def select_columns_rejects_without_id(r_dev, r_test, r_dev_mod, r_test_mod):\\n    \\\"\\\"\\\"\\n    The goal of this function is to create rejects datasets (train and test) with the modelling columns only.\\n    Currently hard-coded to work for a dataset with 8 features.\\n\\n    Parameters\\n    ----------\\n\\n    r_dev: rejects training data; Dataframe\\n    r_test: rejects testing data; Dataframe\\n    r_dev_mod: name of the rejecs training data without id; String\\n    r_test_mod: name of the rejects training data without id\\n\\n    Return\\n    ------\\n    r_dev_mod: rejecs training data without id; Dataframe\\n    r_test_mod: rejects training data without id; Dataframe\\n\\n    \\\"\\\"\\\"\\n\\n    r_dev_mod = r_dev.iloc[:, :9]\\n    r_test_mod = r_test.iloc[:, :9]\\n    return r_dev_mod, r_test_mod\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 67;\n",
       "                var nbb_unformatted_code = \"def select_columns_rejects_without_id(r_dev, r_test, r_dev_mod, r_test_mod):\\n    \\\"\\\"\\\"\\n    The goal of this function is to create rejects datasets (train and test) with the modelling columns only.\\n    Currently hard-coded to work for a dataset with 8 features.\\n\\n    Parameters\\n    ----------\\n\\n    r_dev: rejects training data; Dataframe\\n    r_test: rejects testing data; Dataframe\\n    r_dev_mod: name of the rejecs training data without id; String\\n    r_test_mod: name of the rejects training data without id\\n\\n    Return\\n    ------\\n    r_dev_mod: rejecs training data without id; Dataframe\\n    r_test_mod: rejects training data without id; Dataframe\\n\\n    \\\"\\\"\\\"\\n\\n    r_dev_mod = r_dev.iloc[:, :9]\\n    r_test_mod = r_test.iloc[:, :9]\\n    return r_dev_mod, r_test_mod\";\n",
       "                var nbb_formatted_code = \"def select_columns_rejects_without_id(r_dev, r_test, r_dev_mod, r_test_mod):\\n    \\\"\\\"\\\"\\n    The goal of this function is to create rejects datasets (train and test) with the modelling columns only.\\n    Currently hard-coded to work for a dataset with 8 features.\\n\\n    Parameters\\n    ----------\\n\\n    r_dev: rejects training data; Dataframe\\n    r_test: rejects testing data; Dataframe\\n    r_dev_mod: name of the rejecs training data without id; String\\n    r_test_mod: name of the rejects training data without id\\n\\n    Return\\n    ------\\n    r_dev_mod: rejecs training data without id; Dataframe\\n    r_test_mod: rejects training data without id; Dataframe\\n\\n    \\\"\\\"\\\"\\n\\n    r_dev_mod = r_dev.iloc[:, :9]\\n    r_test_mod = r_test.iloc[:, :9]\\n    return r_dev_mod, r_test_mod\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def select_columns_rejects_without_id(r_dev, r_test, r_dev_mod, r_test_mod):\n",
    "    \"\"\"\n",
    "    The goal of this function is to create rejects datasets (train and test) with the modelling columns only.\n",
    "    Currently hard-coded to work for a dataset with 8 features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    r_dev: rejects training data; Dataframe\n",
    "    r_test: rejects testing data; Dataframe\n",
    "    r_dev_mod: name of the rejecs training data without id; String\n",
    "    r_test_mod: name of the rejects training data without id\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    r_dev_mod: rejecs training data without id; Dataframe\n",
    "    r_test_mod: rejects training data without id; Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    r_dev_mod = r_dev.iloc[:, :9]\n",
    "    r_test_mod = r_test.iloc[:, :9]\n",
    "    return r_dev_mod, r_test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 68;\n",
       "                var nbb_unformatted_code = \"def select_columns_rejects_with_id(r_dev, r_test, r_dev_mod_id, r_test_mod_id):\\n    \\\"\\\"\\\"\\n    The goal of this function is to Create rejects datasets with the modelling columns + id\\n    Currently hard-coded to work for a dataset with 8 features.\\n\\n    Parameters\\n    ----------\\n\\n    r_dev: rejects training data; Dataframe\\n    r_test: rejects testing data; Dataframe\\n    r_dev_mod_id: rejecs training data with id; name of the rejecs training data with id; String\\n    r_test_mod_id: rejects training data with id; name of the rejecs training data with id; String\\n\\n    Return\\n    ------\\n    r_dev_mod_id: rejecs training data with id; Dataframe\\n    r_test_mod_id: rejects training data with id; Dataframe\\n\\n    \\\"\\\"\\\"\\n    r_dev_mod_id = r_dev.iloc[:, :10]\\n    r_test_mod_id = r_test.iloc[:, :10]\\n    return r_dev_mod_id, r_test_mod_id\";\n",
       "                var nbb_formatted_code = \"def select_columns_rejects_with_id(r_dev, r_test, r_dev_mod_id, r_test_mod_id):\\n    \\\"\\\"\\\"\\n    The goal of this function is to Create rejects datasets with the modelling columns + id\\n    Currently hard-coded to work for a dataset with 8 features.\\n\\n    Parameters\\n    ----------\\n\\n    r_dev: rejects training data; Dataframe\\n    r_test: rejects testing data; Dataframe\\n    r_dev_mod_id: rejecs training data with id; name of the rejecs training data with id; String\\n    r_test_mod_id: rejects training data with id; name of the rejecs training data with id; String\\n\\n    Return\\n    ------\\n    r_dev_mod_id: rejecs training data with id; Dataframe\\n    r_test_mod_id: rejects training data with id; Dataframe\\n\\n    \\\"\\\"\\\"\\n    r_dev_mod_id = r_dev.iloc[:, :10]\\n    r_test_mod_id = r_test.iloc[:, :10]\\n    return r_dev_mod_id, r_test_mod_id\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 68;\n",
       "                var nbb_unformatted_code = \"def select_columns_rejects_with_id(r_dev, r_test, r_dev_mod_id, r_test_mod_id):\\n    \\\"\\\"\\\"\\n    The goal of this function is to Create rejects datasets with the modelling columns + id\\n    Currently hard-coded to work for a dataset with 8 features.\\n\\n    Parameters\\n    ----------\\n\\n    r_dev: rejects training data; Dataframe\\n    r_test: rejects testing data; Dataframe\\n    r_dev_mod_id: rejecs training data with id; name of the rejecs training data with id; String\\n    r_test_mod_id: rejects training data with id; name of the rejecs training data with id; String\\n\\n    Return\\n    ------\\n    r_dev_mod_id: rejecs training data with id; Dataframe\\n    r_test_mod_id: rejects training data with id; Dataframe\\n\\n    \\\"\\\"\\\"\\n    r_dev_mod_id = r_dev.iloc[:, :10]\\n    r_test_mod_id = r_test.iloc[:, :10]\\n    return r_dev_mod_id, r_test_mod_id\";\n",
       "                var nbb_formatted_code = \"def select_columns_rejects_with_id(r_dev, r_test, r_dev_mod_id, r_test_mod_id):\\n    \\\"\\\"\\\"\\n    The goal of this function is to Create rejects datasets with the modelling columns + id\\n    Currently hard-coded to work for a dataset with 8 features.\\n\\n    Parameters\\n    ----------\\n\\n    r_dev: rejects training data; Dataframe\\n    r_test: rejects testing data; Dataframe\\n    r_dev_mod_id: rejecs training data with id; name of the rejecs training data with id; String\\n    r_test_mod_id: rejects training data with id; name of the rejecs training data with id; String\\n\\n    Return\\n    ------\\n    r_dev_mod_id: rejecs training data with id; Dataframe\\n    r_test_mod_id: rejects training data with id; Dataframe\\n\\n    \\\"\\\"\\\"\\n    r_dev_mod_id = r_dev.iloc[:, :10]\\n    r_test_mod_id = r_test.iloc[:, :10]\\n    return r_dev_mod_id, r_test_mod_id\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def select_columns_rejects_with_id(r_dev, r_test, r_dev_mod_id, r_test_mod_id):\n",
    "    \"\"\"\n",
    "    The goal of this function is to Create rejects datasets with the modelling columns + id\n",
    "    Currently hard-coded to work for a dataset with 8 features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    r_dev: rejects training data; Dataframe\n",
    "    r_test: rejects testing data; Dataframe\n",
    "    r_dev_mod_id: rejecs training data with id; name of the rejecs training data with id; String\n",
    "    r_test_mod_id: rejects training data with id; name of the rejecs training data with id; String\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    r_dev_mod_id: rejecs training data with id; Dataframe\n",
    "    r_test_mod_id: rejects training data with id; Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    r_dev_mod_id = r_dev.iloc[:, :10]\n",
    "    r_test_mod_id = r_test.iloc[:, :10]\n",
    "    return r_dev_mod_id, r_test_mod_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 69;\n",
       "                var nbb_unformatted_code = \"r_dev_mod, r_test_mod = select_columns_rejects_without_id(\\n    r_dev, r_test, \\\"r_dev_mod\\\", \\\"r_test_mod\\\"\\n)\\nr_dev_mod_id, r_test_mod_id = select_columns_rejects_with_id(\\n    r_dev, r_test, \\\"r_dev_mod_id\\\", \\\"r_test_mod_id\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"r_dev_mod, r_test_mod = select_columns_rejects_without_id(\\n    r_dev, r_test, \\\"r_dev_mod\\\", \\\"r_test_mod\\\"\\n)\\nr_dev_mod_id, r_test_mod_id = select_columns_rejects_with_id(\\n    r_dev, r_test, \\\"r_dev_mod_id\\\", \\\"r_test_mod_id\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 69;\n",
       "                var nbb_unformatted_code = \"r_dev_mod, r_test_mod = select_columns_rejects_without_id(\\n    r_dev, r_test, \\\"r_dev_mod\\\", \\\"r_test_mod\\\"\\n)\\nr_dev_mod_id, r_test_mod_id = select_columns_rejects_with_id(\\n    r_dev, r_test, \\\"r_dev_mod_id\\\", \\\"r_test_mod_id\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"r_dev_mod, r_test_mod = select_columns_rejects_without_id(\\n    r_dev, r_test, \\\"r_dev_mod\\\", \\\"r_test_mod\\\"\\n)\\nr_dev_mod_id, r_test_mod_id = select_columns_rejects_with_id(\\n    r_dev, r_test, \\\"r_dev_mod_id\\\", \\\"r_test_mod_id\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_dev_mod, r_test_mod = select_columns_rejects_without_id(\n",
    "    r_dev, r_test, \"r_dev_mod\", \"r_test_mod\"\n",
    ")\n",
    "r_dev_mod_id, r_test_mod_id = select_columns_rejects_with_id(\n",
    "    r_dev, r_test, \"r_dev_mod_id\", \"r_test_mod_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below two functions continue the data preprocessing. Used to create feature and target data and to split into train and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 70;\n",
       "                var nbb_unformatted_code = \"def create_X_y(data):\\n    \\\"\\\"\\\"\\n    Undersample the data\\n\\n    Parameters\\n    ----------\\n    data : Dataframe\\n\\n    Return\\n    ------\\n    X_res : undersampled data; Dataframe\\n    y_res : undersampled labels; Dataframe\\n\\n    \\\"\\\"\\\"\\n    # Create X and y\\n    X = data.loc[:, data.columns != \\\"target\\\"]\\n    y = data.loc[:, data.columns == \\\"target\\\"]\\n\\n    return X, y\";\n",
       "                var nbb_formatted_code = \"def create_X_y(data):\\n    \\\"\\\"\\\"\\n    Undersample the data\\n\\n    Parameters\\n    ----------\\n    data : Dataframe\\n\\n    Return\\n    ------\\n    X_res : undersampled data; Dataframe\\n    y_res : undersampled labels; Dataframe\\n\\n    \\\"\\\"\\\"\\n    # Create X and y\\n    X = data.loc[:, data.columns != \\\"target\\\"]\\n    y = data.loc[:, data.columns == \\\"target\\\"]\\n\\n    return X, y\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 70;\n",
       "                var nbb_unformatted_code = \"def create_X_y(data):\\n    \\\"\\\"\\\"\\n    Undersample the data\\n\\n    Parameters\\n    ----------\\n    data : Dataframe\\n\\n    Return\\n    ------\\n    X_res : undersampled data; Dataframe\\n    y_res : undersampled labels; Dataframe\\n\\n    \\\"\\\"\\\"\\n    # Create X and y\\n    X = data.loc[:, data.columns != \\\"target\\\"]\\n    y = data.loc[:, data.columns == \\\"target\\\"]\\n\\n    return X, y\";\n",
       "                var nbb_formatted_code = \"def create_X_y(data):\\n    \\\"\\\"\\\"\\n    Undersample the data\\n\\n    Parameters\\n    ----------\\n    data : Dataframe\\n\\n    Return\\n    ------\\n    X_res : undersampled data; Dataframe\\n    y_res : undersampled labels; Dataframe\\n\\n    \\\"\\\"\\\"\\n    # Create X and y\\n    X = data.loc[:, data.columns != \\\"target\\\"]\\n    y = data.loc[:, data.columns == \\\"target\\\"]\\n\\n    return X, y\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_X_y(data):\n",
    "    \"\"\"\n",
    "    Undersample the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Dataframe\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    X_res : undersampled data; Dataframe\n",
    "    y_res : undersampled labels; Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    # Create X and y\n",
    "    X = data.loc[:, data.columns != \"target\"]\n",
    "    y = data.loc[:, data.columns == \"target\"]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 71;\n",
       "                var nbb_unformatted_code = \"def split(X, y):\\n    \\\"\\\"\\\"\\n    Split the data into training and testing sample\\n\\n    Parameters\\n    ----------\\n    X : data\\n    y : labels\\n\\n    Return\\n    ------\\n    X_train : training modelling fields\\n    X_test : test modelling fields\\n    y_train : training labels\\n    y_test : testing labels\\n\\n    \\\"\\\"\\\"\\n    # Train-Test Split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X_res, y_res, test_size=0.2, random_state=7\\n    )\\n    columns = X_train.columns\\n\\n    # Columns\\n    X_train = pd.DataFrame(data=X_train, columns=columns)\\n    y_train = pd.DataFrame(data=y_train, columns=[\\\"target\\\"])\\n\\n    return X_train, X_test, y_train, y_test\";\n",
       "                var nbb_formatted_code = \"def split(X, y):\\n    \\\"\\\"\\\"\\n    Split the data into training and testing sample\\n\\n    Parameters\\n    ----------\\n    X : data\\n    y : labels\\n\\n    Return\\n    ------\\n    X_train : training modelling fields\\n    X_test : test modelling fields\\n    y_train : training labels\\n    y_test : testing labels\\n\\n    \\\"\\\"\\\"\\n    # Train-Test Split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X_res, y_res, test_size=0.2, random_state=7\\n    )\\n    columns = X_train.columns\\n\\n    # Columns\\n    X_train = pd.DataFrame(data=X_train, columns=columns)\\n    y_train = pd.DataFrame(data=y_train, columns=[\\\"target\\\"])\\n\\n    return X_train, X_test, y_train, y_test\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 71;\n",
       "                var nbb_unformatted_code = \"def split(X, y):\\n    \\\"\\\"\\\"\\n    Split the data into training and testing sample\\n\\n    Parameters\\n    ----------\\n    X : data\\n    y : labels\\n\\n    Return\\n    ------\\n    X_train : training modelling fields\\n    X_test : test modelling fields\\n    y_train : training labels\\n    y_test : testing labels\\n\\n    \\\"\\\"\\\"\\n    # Train-Test Split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X_res, y_res, test_size=0.2, random_state=7\\n    )\\n    columns = X_train.columns\\n\\n    # Columns\\n    X_train = pd.DataFrame(data=X_train, columns=columns)\\n    y_train = pd.DataFrame(data=y_train, columns=[\\\"target\\\"])\\n\\n    return X_train, X_test, y_train, y_test\";\n",
       "                var nbb_formatted_code = \"def split(X, y):\\n    \\\"\\\"\\\"\\n    Split the data into training and testing sample\\n\\n    Parameters\\n    ----------\\n    X : data\\n    y : labels\\n\\n    Return\\n    ------\\n    X_train : training modelling fields\\n    X_test : test modelling fields\\n    y_train : training labels\\n    y_test : testing labels\\n\\n    \\\"\\\"\\\"\\n    # Train-Test Split\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X_res, y_res, test_size=0.2, random_state=7\\n    )\\n    columns = X_train.columns\\n\\n    # Columns\\n    X_train = pd.DataFrame(data=X_train, columns=columns)\\n    y_train = pd.DataFrame(data=y_train, columns=[\\\"target\\\"])\\n\\n    return X_train, X_test, y_train, y_test\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split(X, y):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sample\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : data\n",
    "    y : labels\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    X_train : training modelling fields\n",
    "    X_test : test modelling fields\n",
    "    y_train : training labels\n",
    "    y_test : testing labels\n",
    "\n",
    "    \"\"\"\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_res, y_res, test_size=0.2, random_state=7\n",
    "    )\n",
    "    columns = X_train.columns\n",
    "\n",
    "    # Columns\n",
    "    X_train = pd.DataFrame(data=X_train, columns=columns)\n",
    "    y_train = pd.DataFrame(data=y_train, columns=[\"target\"])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 72;\n",
       "                var nbb_unformatted_code = \"X_res, y_res = create_X_y(a)\\nX_train, X_test, y_train, y_test = split(X_res, y_res)\";\n",
       "                var nbb_formatted_code = \"X_res, y_res = create_X_y(a)\\nX_train, X_test, y_train, y_test = split(X_res, y_res)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 72;\n",
       "                var nbb_unformatted_code = \"X_res, y_res = create_X_y(a)\\nX_train, X_test, y_train, y_test = split(X_res, y_res)\";\n",
       "                var nbb_formatted_code = \"X_res, y_res = create_X_y(a)\\nX_train, X_test, y_train, y_test = split(X_res, y_res)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_res, y_res = create_X_y(a)\n",
    "X_train, X_test, y_train, y_test = split(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 73;\n",
       "                var nbb_unformatted_code = \"n = int(0.1 * (len(X_train)))\\nX_train = X_train[:n]\\ny_train = y_train[:n]\";\n",
       "                var nbb_formatted_code = \"n = int(0.1 * (len(X_train)))\\nX_train = X_train[:n]\\ny_train = y_train[:n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 73;\n",
       "                var nbb_unformatted_code = \"n = int(0.1 * (len(X_train)))\\nX_train = X_train[:n]\\ny_train = y_train[:n]\";\n",
       "                var nbb_formatted_code = \"n = int(0.1 * (len(X_train)))\\nX_train = X_train[:n]\\ny_train = y_train[:n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = int(0.1 * (len(X_train)))\n",
    "X_train = X_train[:n]\n",
    "y_train = y_train[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we select the features that will end up in the model. The selection of columns below is subject to iteration based on the modelling outcomes from the logistic regression, i.e. significance (p-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 74;\n",
       "                var nbb_unformatted_code = \"significant_columns = [\\n    \\\"known_col_0\\\",\\n    \\\"known_col_1\\\",\\n    \\\"known_col_3\\\",\\n    \\\"known_col_4\\\",\\n]\";\n",
       "                var nbb_formatted_code = \"significant_columns = [\\n    \\\"known_col_0\\\",\\n    \\\"known_col_1\\\",\\n    \\\"known_col_3\\\",\\n    \\\"known_col_4\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 74;\n",
       "                var nbb_unformatted_code = \"significant_columns = [\\n    \\\"known_col_0\\\",\\n    \\\"known_col_1\\\",\\n    \\\"known_col_3\\\",\\n    \\\"known_col_4\\\",\\n]\";\n",
       "                var nbb_formatted_code = \"significant_columns = [\\n    \\\"known_col_0\\\",\\n    \\\"known_col_1\\\",\\n    \\\"known_col_3\\\",\\n    \\\"known_col_4\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "significant_columns = [\n",
    "    \"known_col_0\",\n",
    "    \"known_col_1\",\n",
    "    \"known_col_3\",\n",
    "    \"known_col_4\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below rows keep only significant columns (or target) in the respective datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 75;\n",
       "                var nbb_unformatted_code = \"# Primary datasets\\nX_train = X_train[significant_columns]\\nX_test_3 = X_test[significant_columns]\\nr_dev_mod = r_dev_mod[significant_columns]\\nr_test_mod = r_test_mod[significant_columns]\\n# Rejects with labels\\ndfr_dev_with_label.rename(columns={\\\"y\\\": \\\"target\\\"}, inplace=True)\\ndfr_dev_with_label_X = dfr_dev_with_label[significant_columns]\\ndfr_dev_with_label_y = dfr_dev_with_label[[\\\"target\\\"]]\\ndfr_test_with_label.rename(columns={\\\"y\\\": \\\"target\\\"}, inplace=True)\\ndfr_test_with_label_X = dfr_test_with_label[significant_columns]\\ndfr_test_with_label_y = dfr_test_with_label[[\\\"target\\\"]]\";\n",
       "                var nbb_formatted_code = \"# Primary datasets\\nX_train = X_train[significant_columns]\\nX_test_3 = X_test[significant_columns]\\nr_dev_mod = r_dev_mod[significant_columns]\\nr_test_mod = r_test_mod[significant_columns]\\n# Rejects with labels\\ndfr_dev_with_label.rename(columns={\\\"y\\\": \\\"target\\\"}, inplace=True)\\ndfr_dev_with_label_X = dfr_dev_with_label[significant_columns]\\ndfr_dev_with_label_y = dfr_dev_with_label[[\\\"target\\\"]]\\ndfr_test_with_label.rename(columns={\\\"y\\\": \\\"target\\\"}, inplace=True)\\ndfr_test_with_label_X = dfr_test_with_label[significant_columns]\\ndfr_test_with_label_y = dfr_test_with_label[[\\\"target\\\"]]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 75;\n",
       "                var nbb_unformatted_code = \"# Primary datasets\\nX_train = X_train[significant_columns]\\nX_test_3 = X_test[significant_columns]\\nr_dev_mod = r_dev_mod[significant_columns]\\nr_test_mod = r_test_mod[significant_columns]\\n# Rejects with labels\\ndfr_dev_with_label.rename(columns={\\\"y\\\": \\\"target\\\"}, inplace=True)\\ndfr_dev_with_label_X = dfr_dev_with_label[significant_columns]\\ndfr_dev_with_label_y = dfr_dev_with_label[[\\\"target\\\"]]\\ndfr_test_with_label.rename(columns={\\\"y\\\": \\\"target\\\"}, inplace=True)\\ndfr_test_with_label_X = dfr_test_with_label[significant_columns]\\ndfr_test_with_label_y = dfr_test_with_label[[\\\"target\\\"]]\";\n",
       "                var nbb_formatted_code = \"# Primary datasets\\nX_train = X_train[significant_columns]\\nX_test_3 = X_test[significant_columns]\\nr_dev_mod = r_dev_mod[significant_columns]\\nr_test_mod = r_test_mod[significant_columns]\\n# Rejects with labels\\ndfr_dev_with_label.rename(columns={\\\"y\\\": \\\"target\\\"}, inplace=True)\\ndfr_dev_with_label_X = dfr_dev_with_label[significant_columns]\\ndfr_dev_with_label_y = dfr_dev_with_label[[\\\"target\\\"]]\\ndfr_test_with_label.rename(columns={\\\"y\\\": \\\"target\\\"}, inplace=True)\\ndfr_test_with_label_X = dfr_test_with_label[significant_columns]\\ndfr_test_with_label_y = dfr_test_with_label[[\\\"target\\\"]]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Primary datasets\n",
    "X_train = X_train[significant_columns]\n",
    "X_test_3 = X_test[significant_columns]\n",
    "r_dev_mod = r_dev_mod[significant_columns]\n",
    "r_test_mod = r_test_mod[significant_columns]\n",
    "# Rejects with labels\n",
    "dfr_dev_with_label.rename(columns={\"y\": \"target\"}, inplace=True)\n",
    "dfr_dev_with_label_X = dfr_dev_with_label[significant_columns]\n",
    "dfr_dev_with_label_y = dfr_dev_with_label[[\"target\"]]\n",
    "dfr_test_with_label.rename(columns={\"y\": \"target\"}, inplace=True)\n",
    "dfr_test_with_label_X = dfr_test_with_label[significant_columns]\n",
    "dfr_test_with_label_y = dfr_test_with_label[[\"target\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the same regression twice with different libraries. statmodels is used to show easier the p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.116354\n",
      "         Iterations 10\n",
      "                          Results: Logit\n",
      "==================================================================\n",
      "Model:              Logit            Pseudo R-squared: 0.823      \n",
      "Dependent Variable: target           AIC:              270.6336   \n",
      "Date:               2021-05-18 09:14 BIC:              295.7390   \n",
      "No. Observations:   1120             Log-Likelihood:   -130.32    \n",
      "Df Model:           4                LL-Null:          -735.65    \n",
      "Df Residuals:       1115             LLR p-value:      7.7481e-261\n",
      "Converged:          1.0000           Scale:            1.0000     \n",
      "No. Iterations:     10.0000                                       \n",
      "-------------------------------------------------------------------\n",
      "                Coef.   Std.Err.     z     P>|z|    [0.025   0.975]\n",
      "-------------------------------------------------------------------\n",
      "const          -2.1415    0.2496  -8.5785  0.0000  -2.6308  -1.6522\n",
      "known_col_0    -1.7719    0.2048  -8.6532  0.0000  -2.1732  -1.3705\n",
      "known_col_1     8.1627    0.7001  11.6592  0.0000   6.7905   9.5349\n",
      "known_col_3    -2.8598    0.3000  -9.5323  0.0000  -3.4479  -2.2718\n",
      "known_col_4     2.9058    0.3338   8.7057  0.0000   2.2516   3.5600\n",
      "==================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 76;\n",
       "                var nbb_unformatted_code = \"# Build Logistic regression\\n# Statmodels\\nX_in = sm.add_constant(X_train.astype(float))\\nlogit_model = sm.Logit(y_train, X_in)\\nresult3 = logit_model.fit()\\nprint(result3.summary2())\";\n",
       "                var nbb_formatted_code = \"# Build Logistic regression\\n# Statmodels\\nX_in = sm.add_constant(X_train.astype(float))\\nlogit_model = sm.Logit(y_train, X_in)\\nresult3 = logit_model.fit()\\nprint(result3.summary2())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 76;\n",
       "                var nbb_unformatted_code = \"# Build Logistic regression\\n# Statmodels\\nX_in = sm.add_constant(X_train.astype(float))\\nlogit_model = sm.Logit(y_train, X_in)\\nresult3 = logit_model.fit()\\nprint(result3.summary2())\";\n",
       "                var nbb_formatted_code = \"# Build Logistic regression\\n# Statmodels\\nX_in = sm.add_constant(X_train.astype(float))\\nlogit_model = sm.Logit(y_train, X_in)\\nresult3 = logit_model.fit()\\nprint(result3.summary2())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build Logistic regression\n",
    "# Statmodels\n",
    "X_in = sm.add_constant(X_train.astype(float))\n",
    "logit_model = sm.Logit(y_train, X_in)\n",
    "result3 = logit_model.fit()\n",
    "print(result3.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 77;\n",
       "                var nbb_unformatted_code = \"def log_reg(X_train, y_train, X_test):\\n    # logreg = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg = RandomForestClassifier()\\n    logreg = LGBMClassifier()\\n    logreg.fit(X_train, y_train.values.ravel())\\n    y_pred = logreg.predict(X_test)\\n    return logreg, y_pred\";\n",
       "                var nbb_formatted_code = \"def log_reg(X_train, y_train, X_test):\\n    # logreg = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg = RandomForestClassifier()\\n    logreg = LGBMClassifier()\\n    logreg.fit(X_train, y_train.values.ravel())\\n    y_pred = logreg.predict(X_test)\\n    return logreg, y_pred\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 77;\n",
       "                var nbb_unformatted_code = \"def log_reg(X_train, y_train, X_test):\\n    # logreg = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg = RandomForestClassifier()\\n    logreg = LGBMClassifier()\\n    logreg.fit(X_train, y_train.values.ravel())\\n    y_pred = logreg.predict(X_test)\\n    return logreg, y_pred\";\n",
       "                var nbb_formatted_code = \"def log_reg(X_train, y_train, X_test):\\n    # logreg = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg = RandomForestClassifier()\\n    logreg = LGBMClassifier()\\n    logreg.fit(X_train, y_train.values.ravel())\\n    y_pred = logreg.predict(X_test)\\n    return logreg, y_pred\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def log_reg(X_train, y_train, X_test):\n",
    "    # logreg = LogisticRegression(fit_intercept=False, penalty=\"none\")\n",
    "    # logreg = RandomForestClassifier()\n",
    "    logreg = LGBMClassifier()\n",
    "    logreg.fit(X_train, y_train.values.ravel())\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    return logreg, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 78;\n",
       "                var nbb_unformatted_code = \"logreg, y_pred = log_reg(X_train, y_train, X_test_3)\";\n",
       "                var nbb_formatted_code = \"logreg, y_pred = log_reg(X_train, y_train, X_test_3)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 78;\n",
       "                var nbb_unformatted_code = \"logreg, y_pred = log_reg(X_train, y_train, X_test_3)\";\n",
       "                var nbb_formatted_code = \"logreg, y_pred = log_reg(X_train, y_train, X_test_3)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logreg, y_pred = log_reg(X_train, y_train, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score Logistic Regression: 0.9246428571428571\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 79;\n",
       "                var nbb_unformatted_code = \"# Plot\\nprint(\\\"Accuracy score Logistic Regression:\\\", logreg.score(X_test_3, y_test))\\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test_3))\\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test_3)[:, 1])\\nplt.figure()\\nplt.plot(fpr, tpr, label=\\\"Logistic Regression (area = %0.2f)\\\" % logit_roc_auc)\\nplt.plot([0, 1], [0, 1], \\\"r--\\\")\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\\"False Positive Rate\\\")\\nplt.ylabel(\\\"True Positive Rate\\\")\\nplt.title(\\\"Receiver operating characteristic\\\")\\nplt.legend(loc=\\\"lower right\\\")\\nplt.savefig(\\\"Log_ROC\\\")\\nplt.show()\";\n",
       "                var nbb_formatted_code = \"# Plot\\nprint(\\\"Accuracy score Logistic Regression:\\\", logreg.score(X_test_3, y_test))\\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test_3))\\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test_3)[:, 1])\\nplt.figure()\\nplt.plot(fpr, tpr, label=\\\"Logistic Regression (area = %0.2f)\\\" % logit_roc_auc)\\nplt.plot([0, 1], [0, 1], \\\"r--\\\")\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\\"False Positive Rate\\\")\\nplt.ylabel(\\\"True Positive Rate\\\")\\nplt.title(\\\"Receiver operating characteristic\\\")\\nplt.legend(loc=\\\"lower right\\\")\\nplt.savefig(\\\"Log_ROC\\\")\\nplt.show()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 79;\n",
       "                var nbb_unformatted_code = \"# Plot\\nprint(\\\"Accuracy score Logistic Regression:\\\", logreg.score(X_test_3, y_test))\\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test_3))\\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test_3)[:, 1])\\nplt.figure()\\nplt.plot(fpr, tpr, label=\\\"Logistic Regression (area = %0.2f)\\\" % logit_roc_auc)\\nplt.plot([0, 1], [0, 1], \\\"r--\\\")\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\\"False Positive Rate\\\")\\nplt.ylabel(\\\"True Positive Rate\\\")\\nplt.title(\\\"Receiver operating characteristic\\\")\\nplt.legend(loc=\\\"lower right\\\")\\nplt.savefig(\\\"Log_ROC\\\")\\nplt.show()\";\n",
       "                var nbb_formatted_code = \"# Plot\\nprint(\\\"Accuracy score Logistic Regression:\\\", logreg.score(X_test_3, y_test))\\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test_3))\\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test_3)[:, 1])\\nplt.figure()\\nplt.plot(fpr, tpr, label=\\\"Logistic Regression (area = %0.2f)\\\" % logit_roc_auc)\\nplt.plot([0, 1], [0, 1], \\\"r--\\\")\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\\"False Positive Rate\\\")\\nplt.ylabel(\\\"True Positive Rate\\\")\\nplt.title(\\\"Receiver operating characteristic\\\")\\nplt.legend(loc=\\\"lower right\\\")\\nplt.savefig(\\\"Log_ROC\\\")\\nplt.show()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "print(\"Accuracy score Logistic Regression:\", logreg.score(X_test_3, y_test))\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test_3))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test_3)[:, 1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"Log_ROC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default rate and conservative default rate are used for future cut-off calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 80;\n",
       "                var nbb_unformatted_code = \"dr = len(y_test[y_test[\\\"target\\\"] == 1]) / (\\n    len(y_test[y_test[\\\"target\\\"] == 1]) + len(y_test[y_test[\\\"target\\\"] == 0])\\n)\\nconservative_dr = (\\n    1.1\\n    * len(y_test[y_test[\\\"target\\\"] == 1])\\n    / (len(y_test[y_test[\\\"target\\\"] == 1]) + len(y_test[y_test[\\\"target\\\"] == 0]))\\n)\";\n",
       "                var nbb_formatted_code = \"dr = len(y_test[y_test[\\\"target\\\"] == 1]) / (\\n    len(y_test[y_test[\\\"target\\\"] == 1]) + len(y_test[y_test[\\\"target\\\"] == 0])\\n)\\nconservative_dr = (\\n    1.1\\n    * len(y_test[y_test[\\\"target\\\"] == 1])\\n    / (len(y_test[y_test[\\\"target\\\"] == 1]) + len(y_test[y_test[\\\"target\\\"] == 0]))\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 80;\n",
       "                var nbb_unformatted_code = \"dr = len(y_test[y_test[\\\"target\\\"] == 1]) / (\\n    len(y_test[y_test[\\\"target\\\"] == 1]) + len(y_test[y_test[\\\"target\\\"] == 0])\\n)\\nconservative_dr = (\\n    1.1\\n    * len(y_test[y_test[\\\"target\\\"] == 1])\\n    / (len(y_test[y_test[\\\"target\\\"] == 1]) + len(y_test[y_test[\\\"target\\\"] == 0]))\\n)\";\n",
       "                var nbb_formatted_code = \"dr = len(y_test[y_test[\\\"target\\\"] == 1]) / (\\n    len(y_test[y_test[\\\"target\\\"] == 1]) + len(y_test[y_test[\\\"target\\\"] == 0])\\n)\\nconservative_dr = (\\n    1.1\\n    * len(y_test[y_test[\\\"target\\\"] == 1])\\n    / (len(y_test[y_test[\\\"target\\\"] == 1]) + len(y_test[y_test[\\\"target\\\"] == 0]))\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dr = len(y_test[y_test[\"target\"] == 1]) / (\n",
    "    len(y_test[y_test[\"target\"] == 1]) + len(y_test[y_test[\"target\"] == 0])\n",
    ")\n",
    "conservative_dr = (\n",
    "    1.1\n",
    "    * len(y_test[y_test[\"target\"] == 1])\n",
    "    / (len(y_test[y_test[\"target\"] == 1]) + len(y_test[y_test[\"target\"] == 0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rejects, another step of data preporcessing is applied via Isolation Forest model. The goal is to remove outliers. The isolation forest is trained on all accepts and is used to evaluate the similarity of the rejects. Then the rejects that are found to be the most and least similar to the accepts are dropped. The contaimination parameter determines how many observations are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 81;\n",
       "                var nbb_unformatted_code = \"def isolation_forest(X_train, r_dev_mod, r_test_mod):\\n    \\\"\\\"\\\"\\n    The goal of this function is to filter the outliers from the rejected sample.\\n\\n    Parameters\\n    ----------\\n    X_train: accepts training data; Dataframe\\n    r_dev_mod: rejects modelling data prior outlier treatment; Dataframe\\n    r_test_mod: rejects testinf data prior outlier treatment; Dataframe\\n\\n    Return\\n    ------\\n    r_dev_mod: rejects modelling data post outlier treatment; Dataframe\\n    r_test_mod: rejects training data prior outlier treatment; Dataframe\\n\\n    \\\"\\\"\\\"\\n\\n    # Build Isolation forest model\\n    isf = IsolationForest(\\n        n_estimators=50, max_samples=\\\"auto\\\", contamination=float(0.02), max_features=1.0\\n    )\\n    isf.fit(X_train)\\n    rej_isf = isf.predict(r_dev_mod)\\n    # Add scores and anomaly columns to rejected train\\n    r_dev_mod[\\\"scores\\\"] = isf.decision_function(r_dev_mod)\\n    r_dev_mod[\\\"anomaly\\\"] = isf.predict(\\n        r_dev_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n    )\\n    # Print number of non-outliers and outliers\\n    print(\\n        \\\"Rejected Train. Number of non-outliers is:\\\", np.sum(r_dev_mod[\\\"anomaly\\\"] == 1)\\n    )\\n    print(\\\"Rejected Train. Number of outliers is:\\\", np.sum(r_dev_mod[\\\"anomaly\\\"] == -1))\\n    # Drop all outliers\\n    r_dev_mod = r_dev_mod[r_dev_mod.anomaly != -1]\\n    # Delete columns related to the outliers\\n    r_dev_mod = r_dev_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n\\n    # Add scores and anomaly columns to rejected test\\n    r_test_mod[\\\"scores\\\"] = isf.decision_function(r_test_mod)\\n    r_test_mod[\\\"anomaly\\\"] = isf.predict(\\n        r_test_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n    )\\n    # Print number of non-outliers and outliers\\n    print(\\n        \\\"Rejected Test. Number of non-outliers is:\\\", np.sum(r_test_mod[\\\"anomaly\\\"] == 1)\\n    )\\n    print(\\\"Rejected Test. Number of outliers is:\\\", np.sum(r_test_mod[\\\"anomaly\\\"] == -1))\\n    # Drop all outliers\\n    r_test_mod = r_test_mod[r_test_mod.anomaly != -1]\\n    # Delete columns related to the outliers\\n    r_test_mod = r_test_mod[\\n        [\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]\\n    ]\\n\\n    return r_dev_mod, r_test_mod\";\n",
       "                var nbb_formatted_code = \"def isolation_forest(X_train, r_dev_mod, r_test_mod):\\n    \\\"\\\"\\\"\\n    The goal of this function is to filter the outliers from the rejected sample.\\n\\n    Parameters\\n    ----------\\n    X_train: accepts training data; Dataframe\\n    r_dev_mod: rejects modelling data prior outlier treatment; Dataframe\\n    r_test_mod: rejects testinf data prior outlier treatment; Dataframe\\n\\n    Return\\n    ------\\n    r_dev_mod: rejects modelling data post outlier treatment; Dataframe\\n    r_test_mod: rejects training data prior outlier treatment; Dataframe\\n\\n    \\\"\\\"\\\"\\n\\n    # Build Isolation forest model\\n    isf = IsolationForest(\\n        n_estimators=50, max_samples=\\\"auto\\\", contamination=float(0.02), max_features=1.0\\n    )\\n    isf.fit(X_train)\\n    rej_isf = isf.predict(r_dev_mod)\\n    # Add scores and anomaly columns to rejected train\\n    r_dev_mod[\\\"scores\\\"] = isf.decision_function(r_dev_mod)\\n    r_dev_mod[\\\"anomaly\\\"] = isf.predict(\\n        r_dev_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n    )\\n    # Print number of non-outliers and outliers\\n    print(\\n        \\\"Rejected Train. Number of non-outliers is:\\\", np.sum(r_dev_mod[\\\"anomaly\\\"] == 1)\\n    )\\n    print(\\\"Rejected Train. Number of outliers is:\\\", np.sum(r_dev_mod[\\\"anomaly\\\"] == -1))\\n    # Drop all outliers\\n    r_dev_mod = r_dev_mod[r_dev_mod.anomaly != -1]\\n    # Delete columns related to the outliers\\n    r_dev_mod = r_dev_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n\\n    # Add scores and anomaly columns to rejected test\\n    r_test_mod[\\\"scores\\\"] = isf.decision_function(r_test_mod)\\n    r_test_mod[\\\"anomaly\\\"] = isf.predict(\\n        r_test_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n    )\\n    # Print number of non-outliers and outliers\\n    print(\\n        \\\"Rejected Test. Number of non-outliers is:\\\", np.sum(r_test_mod[\\\"anomaly\\\"] == 1)\\n    )\\n    print(\\\"Rejected Test. Number of outliers is:\\\", np.sum(r_test_mod[\\\"anomaly\\\"] == -1))\\n    # Drop all outliers\\n    r_test_mod = r_test_mod[r_test_mod.anomaly != -1]\\n    # Delete columns related to the outliers\\n    r_test_mod = r_test_mod[\\n        [\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]\\n    ]\\n\\n    return r_dev_mod, r_test_mod\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 81;\n",
       "                var nbb_unformatted_code = \"def isolation_forest(X_train, r_dev_mod, r_test_mod):\\n    \\\"\\\"\\\"\\n    The goal of this function is to filter the outliers from the rejected sample.\\n\\n    Parameters\\n    ----------\\n    X_train: accepts training data; Dataframe\\n    r_dev_mod: rejects modelling data prior outlier treatment; Dataframe\\n    r_test_mod: rejects testinf data prior outlier treatment; Dataframe\\n\\n    Return\\n    ------\\n    r_dev_mod: rejects modelling data post outlier treatment; Dataframe\\n    r_test_mod: rejects training data prior outlier treatment; Dataframe\\n\\n    \\\"\\\"\\\"\\n\\n    # Build Isolation forest model\\n    isf = IsolationForest(\\n        n_estimators=50, max_samples=\\\"auto\\\", contamination=float(0.02), max_features=1.0\\n    )\\n    isf.fit(X_train)\\n    rej_isf = isf.predict(r_dev_mod)\\n    # Add scores and anomaly columns to rejected train\\n    r_dev_mod[\\\"scores\\\"] = isf.decision_function(r_dev_mod)\\n    r_dev_mod[\\\"anomaly\\\"] = isf.predict(\\n        r_dev_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n    )\\n    # Print number of non-outliers and outliers\\n    print(\\n        \\\"Rejected Train. Number of non-outliers is:\\\", np.sum(r_dev_mod[\\\"anomaly\\\"] == 1)\\n    )\\n    print(\\\"Rejected Train. Number of outliers is:\\\", np.sum(r_dev_mod[\\\"anomaly\\\"] == -1))\\n    # Drop all outliers\\n    r_dev_mod = r_dev_mod[r_dev_mod.anomaly != -1]\\n    # Delete columns related to the outliers\\n    r_dev_mod = r_dev_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n\\n    # Add scores and anomaly columns to rejected test\\n    r_test_mod[\\\"scores\\\"] = isf.decision_function(r_test_mod)\\n    r_test_mod[\\\"anomaly\\\"] = isf.predict(\\n        r_test_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n    )\\n    # Print number of non-outliers and outliers\\n    print(\\n        \\\"Rejected Test. Number of non-outliers is:\\\", np.sum(r_test_mod[\\\"anomaly\\\"] == 1)\\n    )\\n    print(\\\"Rejected Test. Number of outliers is:\\\", np.sum(r_test_mod[\\\"anomaly\\\"] == -1))\\n    # Drop all outliers\\n    r_test_mod = r_test_mod[r_test_mod.anomaly != -1]\\n    # Delete columns related to the outliers\\n    r_test_mod = r_test_mod[\\n        [\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]\\n    ]\\n\\n    return r_dev_mod, r_test_mod\";\n",
       "                var nbb_formatted_code = \"def isolation_forest(X_train, r_dev_mod, r_test_mod):\\n    \\\"\\\"\\\"\\n    The goal of this function is to filter the outliers from the rejected sample.\\n\\n    Parameters\\n    ----------\\n    X_train: accepts training data; Dataframe\\n    r_dev_mod: rejects modelling data prior outlier treatment; Dataframe\\n    r_test_mod: rejects testinf data prior outlier treatment; Dataframe\\n\\n    Return\\n    ------\\n    r_dev_mod: rejects modelling data post outlier treatment; Dataframe\\n    r_test_mod: rejects training data prior outlier treatment; Dataframe\\n\\n    \\\"\\\"\\\"\\n\\n    # Build Isolation forest model\\n    isf = IsolationForest(\\n        n_estimators=50, max_samples=\\\"auto\\\", contamination=float(0.02), max_features=1.0\\n    )\\n    isf.fit(X_train)\\n    rej_isf = isf.predict(r_dev_mod)\\n    # Add scores and anomaly columns to rejected train\\n    r_dev_mod[\\\"scores\\\"] = isf.decision_function(r_dev_mod)\\n    r_dev_mod[\\\"anomaly\\\"] = isf.predict(\\n        r_dev_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n    )\\n    # Print number of non-outliers and outliers\\n    print(\\n        \\\"Rejected Train. Number of non-outliers is:\\\", np.sum(r_dev_mod[\\\"anomaly\\\"] == 1)\\n    )\\n    print(\\\"Rejected Train. Number of outliers is:\\\", np.sum(r_dev_mod[\\\"anomaly\\\"] == -1))\\n    # Drop all outliers\\n    r_dev_mod = r_dev_mod[r_dev_mod.anomaly != -1]\\n    # Delete columns related to the outliers\\n    r_dev_mod = r_dev_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n\\n    # Add scores and anomaly columns to rejected test\\n    r_test_mod[\\\"scores\\\"] = isf.decision_function(r_test_mod)\\n    r_test_mod[\\\"anomaly\\\"] = isf.predict(\\n        r_test_mod[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]]\\n    )\\n    # Print number of non-outliers and outliers\\n    print(\\n        \\\"Rejected Test. Number of non-outliers is:\\\", np.sum(r_test_mod[\\\"anomaly\\\"] == 1)\\n    )\\n    print(\\\"Rejected Test. Number of outliers is:\\\", np.sum(r_test_mod[\\\"anomaly\\\"] == -1))\\n    # Drop all outliers\\n    r_test_mod = r_test_mod[r_test_mod.anomaly != -1]\\n    # Delete columns related to the outliers\\n    r_test_mod = r_test_mod[\\n        [\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\"]\\n    ]\\n\\n    return r_dev_mod, r_test_mod\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def isolation_forest(X_train, r_dev_mod, r_test_mod):\n",
    "    \"\"\"\n",
    "    The goal of this function is to filter the outliers from the rejected sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: accepts training data; Dataframe\n",
    "    r_dev_mod: rejects modelling data prior outlier treatment; Dataframe\n",
    "    r_test_mod: rejects testinf data prior outlier treatment; Dataframe\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    r_dev_mod: rejects modelling data post outlier treatment; Dataframe\n",
    "    r_test_mod: rejects training data prior outlier treatment; Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Build Isolation forest model\n",
    "    isf = IsolationForest(\n",
    "        n_estimators=50, max_samples=\"auto\", contamination=float(0.02), max_features=1.0\n",
    "    )\n",
    "    isf.fit(X_train)\n",
    "    rej_isf = isf.predict(r_dev_mod)\n",
    "    # Add scores and anomaly columns to rejected train\n",
    "    r_dev_mod[\"scores\"] = isf.decision_function(r_dev_mod)\n",
    "    r_dev_mod[\"anomaly\"] = isf.predict(\n",
    "        r_dev_mod[[\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\"]]\n",
    "    )\n",
    "    # Print number of non-outliers and outliers\n",
    "    print(\n",
    "        \"Rejected Train. Number of non-outliers is:\", np.sum(r_dev_mod[\"anomaly\"] == 1)\n",
    "    )\n",
    "    print(\"Rejected Train. Number of outliers is:\", np.sum(r_dev_mod[\"anomaly\"] == -1))\n",
    "    # Drop all outliers\n",
    "    r_dev_mod = r_dev_mod[r_dev_mod.anomaly != -1]\n",
    "    # Delete columns related to the outliers\n",
    "    r_dev_mod = r_dev_mod[[\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\"]]\n",
    "\n",
    "    # Add scores and anomaly columns to rejected test\n",
    "    r_test_mod[\"scores\"] = isf.decision_function(r_test_mod)\n",
    "    r_test_mod[\"anomaly\"] = isf.predict(\n",
    "        r_test_mod[[\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\"]]\n",
    "    )\n",
    "    # Print number of non-outliers and outliers\n",
    "    print(\n",
    "        \"Rejected Test. Number of non-outliers is:\", np.sum(r_test_mod[\"anomaly\"] == 1)\n",
    "    )\n",
    "    print(\"Rejected Test. Number of outliers is:\", np.sum(r_test_mod[\"anomaly\"] == -1))\n",
    "    # Drop all outliers\n",
    "    r_test_mod = r_test_mod[r_test_mod.anomaly != -1]\n",
    "    # Delete columns related to the outliers\n",
    "    r_test_mod = r_test_mod[\n",
    "        [\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\"]\n",
    "    ]\n",
    "\n",
    "    return r_dev_mod, r_test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 82;\n",
       "                var nbb_unformatted_code = \"#r_dev_mod, r_test_mod = isolation_forest(X_train, r_dev_mod, r_test_mod)\";\n",
       "                var nbb_formatted_code = \"# r_dev_mod, r_test_mod = isolation_forest(X_train, r_dev_mod, r_test_mod)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 82;\n",
       "                var nbb_unformatted_code = \"#r_dev_mod, r_test_mod = isolation_forest(X_train, r_dev_mod, r_test_mod)\";\n",
       "                var nbb_formatted_code = \"# r_dev_mod, r_test_mod = isolation_forest(X_train, r_dev_mod, r_test_mod)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# r_dev_mod, r_test_mod = isolation_forest(X_train, r_dev_mod, r_test_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 83;\n",
       "                var nbb_unformatted_code = \"def pred(y_test, X_test, X_test_3, model):\\n    # Test set with labels\\n    test_labels = pd.merge(\\n        y_test,\\n        X_test,\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n    # Predictions on testset\\n    test_pred = model.predict_proba(X_test_3)[:, 1]\\n    test_pred2 = pd.DataFrame(data=test_pred, columns=[\\\"prediction\\\"])\\n    test_pred2[\\\"count\\\"] = test_pred2.groupby(\\\"prediction\\\")[\\\"prediction\\\"].transform(\\n        \\\"count\\\"\\n    )\\n    test_pred2.groupby([\\\"prediction\\\"]).count()\\n    test_pred2.describe()\\n\\n    # Join predictions with test new\\n    pred_test_kgb = pd.DataFrame(\\n        data=test_pred, columns=[\\\"prediction_beforeRI\\\"], index=y_test.index.copy()\\n    )\\n    pred_test1 = pd.merge(\\n        test_labels,\\n        pred_test_kgb[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    return pred_test1\";\n",
       "                var nbb_formatted_code = \"def pred(y_test, X_test, X_test_3, model):\\n    # Test set with labels\\n    test_labels = pd.merge(\\n        y_test,\\n        X_test,\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n    # Predictions on testset\\n    test_pred = model.predict_proba(X_test_3)[:, 1]\\n    test_pred2 = pd.DataFrame(data=test_pred, columns=[\\\"prediction\\\"])\\n    test_pred2[\\\"count\\\"] = test_pred2.groupby(\\\"prediction\\\")[\\\"prediction\\\"].transform(\\n        \\\"count\\\"\\n    )\\n    test_pred2.groupby([\\\"prediction\\\"]).count()\\n    test_pred2.describe()\\n\\n    # Join predictions with test new\\n    pred_test_kgb = pd.DataFrame(\\n        data=test_pred, columns=[\\\"prediction_beforeRI\\\"], index=y_test.index.copy()\\n    )\\n    pred_test1 = pd.merge(\\n        test_labels,\\n        pred_test_kgb[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    return pred_test1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 83;\n",
       "                var nbb_unformatted_code = \"def pred(y_test, X_test, X_test_3, model):\\n    # Test set with labels\\n    test_labels = pd.merge(\\n        y_test,\\n        X_test,\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n    # Predictions on testset\\n    test_pred = model.predict_proba(X_test_3)[:, 1]\\n    test_pred2 = pd.DataFrame(data=test_pred, columns=[\\\"prediction\\\"])\\n    test_pred2[\\\"count\\\"] = test_pred2.groupby(\\\"prediction\\\")[\\\"prediction\\\"].transform(\\n        \\\"count\\\"\\n    )\\n    test_pred2.groupby([\\\"prediction\\\"]).count()\\n    test_pred2.describe()\\n\\n    # Join predictions with test new\\n    pred_test_kgb = pd.DataFrame(\\n        data=test_pred, columns=[\\\"prediction_beforeRI\\\"], index=y_test.index.copy()\\n    )\\n    pred_test1 = pd.merge(\\n        test_labels,\\n        pred_test_kgb[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    return pred_test1\";\n",
       "                var nbb_formatted_code = \"def pred(y_test, X_test, X_test_3, model):\\n    # Test set with labels\\n    test_labels = pd.merge(\\n        y_test,\\n        X_test,\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n    # Predictions on testset\\n    test_pred = model.predict_proba(X_test_3)[:, 1]\\n    test_pred2 = pd.DataFrame(data=test_pred, columns=[\\\"prediction\\\"])\\n    test_pred2[\\\"count\\\"] = test_pred2.groupby(\\\"prediction\\\")[\\\"prediction\\\"].transform(\\n        \\\"count\\\"\\n    )\\n    test_pred2.groupby([\\\"prediction\\\"]).count()\\n    test_pred2.describe()\\n\\n    # Join predictions with test new\\n    pred_test_kgb = pd.DataFrame(\\n        data=test_pred, columns=[\\\"prediction_beforeRI\\\"], index=y_test.index.copy()\\n    )\\n    pred_test1 = pd.merge(\\n        test_labels,\\n        pred_test_kgb[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    return pred_test1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pred(y_test, X_test, X_test_3, model):\n",
    "    # Test set with labels\n",
    "    test_labels = pd.merge(\n",
    "        y_test,\n",
    "        X_test,\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    # Predictions on testset\n",
    "    test_pred = model.predict_proba(X_test_3)[:, 1]\n",
    "    test_pred2 = pd.DataFrame(data=test_pred, columns=[\"prediction\"])\n",
    "    test_pred2[\"count\"] = test_pred2.groupby(\"prediction\")[\"prediction\"].transform(\n",
    "        \"count\"\n",
    "    )\n",
    "    test_pred2.groupby([\"prediction\"]).count()\n",
    "    test_pred2.describe()\n",
    "\n",
    "    # Join predictions with test new\n",
    "    pred_test_kgb = pd.DataFrame(\n",
    "        data=test_pred, columns=[\"prediction_beforeRI\"], index=y_test.index.copy()\n",
    "    )\n",
    "    pred_test1 = pd.merge(\n",
    "        test_labels,\n",
    "        pred_test_kgb[[\"prediction_beforeRI\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    return pred_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 84;\n",
       "                var nbb_unformatted_code = \"# Predictions on Accepted Test Sample\\npred_test1 = pred(y_test, X_test, X_test_3, logreg)\\n# Predictions on Rejected Test Sample\\npred_dev_rej = pred(\\n    dfr_dev_with_label_y, dfr_dev_with_label, dfr_dev_with_label_X, logreg\\n)\\npred_test_rej = pred(\\n    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\\n)\\npred_rej1 = pred(\\n    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\\n)\";\n",
       "                var nbb_formatted_code = \"# Predictions on Accepted Test Sample\\npred_test1 = pred(y_test, X_test, X_test_3, logreg)\\n# Predictions on Rejected Test Sample\\npred_dev_rej = pred(\\n    dfr_dev_with_label_y, dfr_dev_with_label, dfr_dev_with_label_X, logreg\\n)\\npred_test_rej = pred(\\n    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\\n)\\npred_rej1 = pred(\\n    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 84;\n",
       "                var nbb_unformatted_code = \"# Predictions on Accepted Test Sample\\npred_test1 = pred(y_test, X_test, X_test_3, logreg)\\n# Predictions on Rejected Test Sample\\npred_dev_rej = pred(\\n    dfr_dev_with_label_y, dfr_dev_with_label, dfr_dev_with_label_X, logreg\\n)\\npred_test_rej = pred(\\n    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\\n)\\npred_rej1 = pred(\\n    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\\n)\";\n",
       "                var nbb_formatted_code = \"# Predictions on Accepted Test Sample\\npred_test1 = pred(y_test, X_test, X_test_3, logreg)\\n# Predictions on Rejected Test Sample\\npred_dev_rej = pred(\\n    dfr_dev_with_label_y, dfr_dev_with_label, dfr_dev_with_label_X, logreg\\n)\\npred_test_rej = pred(\\n    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\\n)\\npred_rej1 = pred(\\n    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predictions on Accepted Test Sample\n",
    "pred_test1 = pred(y_test, X_test, X_test_3, logreg)\n",
    "# Predictions on Rejected Test Sample\n",
    "pred_dev_rej = pred(\n",
    "    dfr_dev_with_label_y, dfr_dev_with_label, dfr_dev_with_label_X, logreg\n",
    ")\n",
    "pred_test_rej = pred(\n",
    "    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\n",
    ")\n",
    "pred_rej1 = pred(\n",
    "    dfr_test_with_label_y, dfr_test_with_label, dfr_test_with_label_X, logreg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 85;\n",
       "                var nbb_unformatted_code = \"# Prediction before RI\\nri1_train_rej = pred_dev_rej[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\nri1_train_rej[\\\"prediction_beforeRI\\\"] = ri1_train_rej[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < ri1_train_rej[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\nri1_test_rej = pred_test_rej[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\nri1_test_rej[\\\"prediction_beforeRI\\\"] = ri1_test_rej[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < ri1_test_rej[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\npred_rej1 = pred_rej1[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\npred_rej1[\\\"prediction_beforeRI\\\"] = pred_rej1[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < pred_rej1[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\nri1_train_rej.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\\nri1_test_rej.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\\npred_rej1.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\";\n",
       "                var nbb_formatted_code = \"# Prediction before RI\\nri1_train_rej = pred_dev_rej[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\nri1_train_rej[\\\"prediction_beforeRI\\\"] = ri1_train_rej[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < ri1_train_rej[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\nri1_test_rej = pred_test_rej[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\nri1_test_rej[\\\"prediction_beforeRI\\\"] = ri1_test_rej[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < ri1_test_rej[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\npred_rej1 = pred_rej1[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\npred_rej1[\\\"prediction_beforeRI\\\"] = pred_rej1[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < pred_rej1[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\nri1_train_rej.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\\nri1_test_rej.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\\npred_rej1.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 85;\n",
       "                var nbb_unformatted_code = \"# Prediction before RI\\nri1_train_rej = pred_dev_rej[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\nri1_train_rej[\\\"prediction_beforeRI\\\"] = ri1_train_rej[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < ri1_train_rej[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\nri1_test_rej = pred_test_rej[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\nri1_test_rej[\\\"prediction_beforeRI\\\"] = ri1_test_rej[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < ri1_test_rej[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\npred_rej1 = pred_rej1[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\npred_rej1[\\\"prediction_beforeRI\\\"] = pred_rej1[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < pred_rej1[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\nri1_train_rej.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\\nri1_test_rej.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\\npred_rej1.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\";\n",
       "                var nbb_formatted_code = \"# Prediction before RI\\nri1_train_rej = pred_dev_rej[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\nri1_train_rej[\\\"prediction_beforeRI\\\"] = ri1_train_rej[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < ri1_train_rej[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\nri1_test_rej = pred_test_rej[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\nri1_test_rej[\\\"prediction_beforeRI\\\"] = ri1_test_rej[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < ri1_test_rej[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\npred_rej1 = pred_rej1[[\\\"target_x\\\", \\\"id\\\", \\\"prediction_beforeRI\\\"]]\\npred_rej1[\\\"prediction_beforeRI\\\"] = pred_rej1[\\\"prediction_beforeRI\\\"].apply(\\n    lambda x: 0\\n    if (x < pred_rej1[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr))\\n    else 1\\n)\\nri1_train_rej.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\\nri1_test_rej.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\\npred_rej1.rename(columns={\\\"target_x\\\": \\\"target\\\"}, inplace=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prediction before RI\n",
    "ri1_train_rej = pred_dev_rej[[\"target_x\", \"id\", \"prediction_beforeRI\"]]\n",
    "ri1_train_rej[\"prediction_beforeRI\"] = ri1_train_rej[\"prediction_beforeRI\"].apply(\n",
    "    lambda x: 0\n",
    "    if (x < ri1_train_rej[\"prediction_beforeRI\"].quantile(q=1 - conservative_dr))\n",
    "    else 1\n",
    ")\n",
    "ri1_test_rej = pred_test_rej[[\"target_x\", \"id\", \"prediction_beforeRI\"]]\n",
    "ri1_test_rej[\"prediction_beforeRI\"] = ri1_test_rej[\"prediction_beforeRI\"].apply(\n",
    "    lambda x: 0\n",
    "    if (x < ri1_test_rej[\"prediction_beforeRI\"].quantile(q=1 - conservative_dr))\n",
    "    else 1\n",
    ")\n",
    "pred_rej1 = pred_rej1[[\"target_x\", \"id\", \"prediction_beforeRI\"]]\n",
    "pred_rej1[\"prediction_beforeRI\"] = pred_rej1[\"prediction_beforeRI\"].apply(\n",
    "    lambda x: 0\n",
    "    if (x < pred_rej1[\"prediction_beforeRI\"].quantile(q=1 - conservative_dr))\n",
    "    else 1\n",
    ")\n",
    "ri1_train_rej.rename(columns={\"target_x\": \"target\"}, inplace=True)\n",
    "ri1_test_rej.rename(columns={\"target_x\": \"target\"}, inplace=True)\n",
    "pred_rej1.rename(columns={\"target_x\": \"target\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions (Labelling Rejects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 86;\n",
       "                var nbb_unformatted_code = \"def predictions1(model):\\n    # Join predictions with train new\\n    pred = model.predict_proba(r_dev_mod)[:, 1]\\n    pred2 = pd.DataFrame(\\n        data=pred,\\n        columns=[\\\"prediction2\\\"],\\n        index=r_dev_mod.index.copy(),\\n    )\\n\\n    # Set cut-off\\n    q1 = pred2[\\\"prediction2\\\"].quantile(q=1 - conservative_dr)\\n    pred2[\\\"prediction_beforeRI\\\"] = pred2[\\\"prediction2\\\"].apply(\\n        lambda x: 0 if (x < q1) else 1\\n    )\\n    outcome = pd.merge(\\n        r_dev_mod_id,\\n        pred2[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n    # pred_test1.dropna(subset=[\\\"prediction_beforeRI\\\"], inplace=True)\\n    outcome = outcome[[\\\"id\\\", \\\"prediction_beforeRI\\\"]]\\n    return outcome\";\n",
       "                var nbb_formatted_code = \"def predictions1(model):\\n    # Join predictions with train new\\n    pred = model.predict_proba(r_dev_mod)[:, 1]\\n    pred2 = pd.DataFrame(\\n        data=pred,\\n        columns=[\\\"prediction2\\\"],\\n        index=r_dev_mod.index.copy(),\\n    )\\n\\n    # Set cut-off\\n    q1 = pred2[\\\"prediction2\\\"].quantile(q=1 - conservative_dr)\\n    pred2[\\\"prediction_beforeRI\\\"] = pred2[\\\"prediction2\\\"].apply(\\n        lambda x: 0 if (x < q1) else 1\\n    )\\n    outcome = pd.merge(\\n        r_dev_mod_id,\\n        pred2[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n    # pred_test1.dropna(subset=[\\\"prediction_beforeRI\\\"], inplace=True)\\n    outcome = outcome[[\\\"id\\\", \\\"prediction_beforeRI\\\"]]\\n    return outcome\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 86;\n",
       "                var nbb_unformatted_code = \"def predictions1(model):\\n    # Join predictions with train new\\n    pred = model.predict_proba(r_dev_mod)[:, 1]\\n    pred2 = pd.DataFrame(\\n        data=pred,\\n        columns=[\\\"prediction2\\\"],\\n        index=r_dev_mod.index.copy(),\\n    )\\n\\n    # Set cut-off\\n    q1 = pred2[\\\"prediction2\\\"].quantile(q=1 - conservative_dr)\\n    pred2[\\\"prediction_beforeRI\\\"] = pred2[\\\"prediction2\\\"].apply(\\n        lambda x: 0 if (x < q1) else 1\\n    )\\n    outcome = pd.merge(\\n        r_dev_mod_id,\\n        pred2[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n    # pred_test1.dropna(subset=[\\\"prediction_beforeRI\\\"], inplace=True)\\n    outcome = outcome[[\\\"id\\\", \\\"prediction_beforeRI\\\"]]\\n    return outcome\";\n",
       "                var nbb_formatted_code = \"def predictions1(model):\\n    # Join predictions with train new\\n    pred = model.predict_proba(r_dev_mod)[:, 1]\\n    pred2 = pd.DataFrame(\\n        data=pred,\\n        columns=[\\\"prediction2\\\"],\\n        index=r_dev_mod.index.copy(),\\n    )\\n\\n    # Set cut-off\\n    q1 = pred2[\\\"prediction2\\\"].quantile(q=1 - conservative_dr)\\n    pred2[\\\"prediction_beforeRI\\\"] = pred2[\\\"prediction2\\\"].apply(\\n        lambda x: 0 if (x < q1) else 1\\n    )\\n    outcome = pd.merge(\\n        r_dev_mod_id,\\n        pred2[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n    # pred_test1.dropna(subset=[\\\"prediction_beforeRI\\\"], inplace=True)\\n    outcome = outcome[[\\\"id\\\", \\\"prediction_beforeRI\\\"]]\\n    return outcome\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predictions1(model):\n",
    "    # Join predictions with train new\n",
    "    pred = model.predict_proba(r_dev_mod)[:, 1]\n",
    "    pred2 = pd.DataFrame(\n",
    "        data=pred,\n",
    "        columns=[\"prediction2\"],\n",
    "        index=r_dev_mod.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Set cut-off\n",
    "    q1 = pred2[\"prediction2\"].quantile(q=1 - conservative_dr)\n",
    "    pred2[\"prediction_beforeRI\"] = pred2[\"prediction2\"].apply(\n",
    "        lambda x: 0 if (x < q1) else 1\n",
    "    )\n",
    "    outcome = pd.merge(\n",
    "        r_dev_mod_id,\n",
    "        pred2[[\"prediction_beforeRI\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    # pred_test1.dropna(subset=[\"prediction_beforeRI\"], inplace=True)\n",
    "    outcome = outcome[[\"id\", \"prediction_beforeRI\"]]\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 87;\n",
       "                var nbb_unformatted_code = \"ri1_train = predictions1(logreg)  # Logistic Regression\";\n",
       "                var nbb_formatted_code = \"ri1_train = predictions1(logreg)  # Logistic Regression\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 87;\n",
       "                var nbb_unformatted_code = \"ri1_train = predictions1(logreg)  # Logistic Regression\";\n",
       "                var nbb_formatted_code = \"ri1_train = predictions1(logreg)  # Logistic Regression\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ri1_train = predictions1(logreg)  # Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 88;\n",
       "                var nbb_unformatted_code = \"def print_results(data, state, prediction):\\n    print(\\n        \\\"The number of accurately classified cases \\\",\\n        state,\\n        \\\" is: \\\",\\n        data[\\n            (data.target == 1) & (data[prediction] == 1)\\n            | (data.target == 0) & (data[prediction] == 0)\\n        ].shape[0],\\n    )\\n    print(\\n        \\\"The number of misclassified cases \\\",\\n        state,\\n        \\\" is: \\\",\\n        data[\\n            (data.target == 1) & (data[prediction] == 0)\\n            | (data.target == 0) & (data[prediction] == 1)\\n        ].shape[0],\\n    )\";\n",
       "                var nbb_formatted_code = \"def print_results(data, state, prediction):\\n    print(\\n        \\\"The number of accurately classified cases \\\",\\n        state,\\n        \\\" is: \\\",\\n        data[\\n            (data.target == 1) & (data[prediction] == 1)\\n            | (data.target == 0) & (data[prediction] == 0)\\n        ].shape[0],\\n    )\\n    print(\\n        \\\"The number of misclassified cases \\\",\\n        state,\\n        \\\" is: \\\",\\n        data[\\n            (data.target == 1) & (data[prediction] == 0)\\n            | (data.target == 0) & (data[prediction] == 1)\\n        ].shape[0],\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 88;\n",
       "                var nbb_unformatted_code = \"def print_results(data, state, prediction):\\n    print(\\n        \\\"The number of accurately classified cases \\\",\\n        state,\\n        \\\" is: \\\",\\n        data[\\n            (data.target == 1) & (data[prediction] == 1)\\n            | (data.target == 0) & (data[prediction] == 0)\\n        ].shape[0],\\n    )\\n    print(\\n        \\\"The number of misclassified cases \\\",\\n        state,\\n        \\\" is: \\\",\\n        data[\\n            (data.target == 1) & (data[prediction] == 0)\\n            | (data.target == 0) & (data[prediction] == 1)\\n        ].shape[0],\\n    )\";\n",
       "                var nbb_formatted_code = \"def print_results(data, state, prediction):\\n    print(\\n        \\\"The number of accurately classified cases \\\",\\n        state,\\n        \\\" is: \\\",\\n        data[\\n            (data.target == 1) & (data[prediction] == 1)\\n            | (data.target == 0) & (data[prediction] == 0)\\n        ].shape[0],\\n    )\\n    print(\\n        \\\"The number of misclassified cases \\\",\\n        state,\\n        \\\" is: \\\",\\n        data[\\n            (data.target == 1) & (data[prediction] == 0)\\n            | (data.target == 0) & (data[prediction] == 1)\\n        ].shape[0],\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_results(data, state, prediction):\n",
    "    print(\n",
    "        \"The number of accurately classified cases \",\n",
    "        state,\n",
    "        \" is: \",\n",
    "        data[\n",
    "            (data.target == 1) & (data[prediction] == 1)\n",
    "            | (data.target == 0) & (data[prediction] == 0)\n",
    "        ].shape[0],\n",
    "    )\n",
    "    print(\n",
    "        \"The number of misclassified cases \",\n",
    "        state,\n",
    "        \" is: \",\n",
    "        data[\n",
    "            (data.target == 1) & (data[prediction] == 0)\n",
    "            | (data.target == 0) & (data[prediction] == 1)\n",
    "        ].shape[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 89;\n",
       "                var nbb_unformatted_code = \"def log_loss_fun(\\n    category,\\n    data,\\n    y_true,\\n    y_pred,\\n):\\n    print(category, \\\" :\\\", log_loss(data[y_true], data[y_pred]))\";\n",
       "                var nbb_formatted_code = \"def log_loss_fun(\\n    category,\\n    data,\\n    y_true,\\n    y_pred,\\n):\\n    print(category, \\\" :\\\", log_loss(data[y_true], data[y_pred]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 89;\n",
       "                var nbb_unformatted_code = \"def log_loss_fun(\\n    category,\\n    data,\\n    y_true,\\n    y_pred,\\n):\\n    print(category, \\\" :\\\", log_loss(data[y_true], data[y_pred]))\";\n",
       "                var nbb_formatted_code = \"def log_loss_fun(\\n    category,\\n    data,\\n    y_true,\\n    y_pred,\\n):\\n    print(category, \\\" :\\\", log_loss(data[y_true], data[y_pred]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def log_loss_fun(\n",
    "    category,\n",
    "    data,\n",
    "    y_true,\n",
    "    y_pred,\n",
    "):\n",
    "    print(category, \" :\", log_loss(data[y_true], data[y_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 90;\n",
       "                var nbb_unformatted_code = \"def all_metrics(pred_label, true_label, model):\\n    \\\"\\\"\\\"\\\"\\\"\\n    pred_label = predicted label of the model\\n    true_label = true label\\n    model = model name\\n    \\\"\\\"\\\" \\\"\\\"\\n\\n    # F1 score\\n    f1_stat = f1_score(pred_label, true_label, average=\\\"weighted\\\")\\n\\n    # Confusion matrix\\n    cm = confusion_matrix(pred_label, true_label, labels=model.classes_)\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\\n    fig = disp.plot()\\n\\n    return print(\\\"F1_stat \\\", model, \\\"is: \\\", f1_stat, fig)\";\n",
       "                var nbb_formatted_code = \"def all_metrics(pred_label, true_label, model):\\n    \\\"\\\"\\\"\\\"\\\"\\n    pred_label = predicted label of the model\\n    true_label = true label\\n    model = model name\\n    \\\"\\\"\\\" \\\"\\\"\\n\\n    # F1 score\\n    f1_stat = f1_score(pred_label, true_label, average=\\\"weighted\\\")\\n\\n    # Confusion matrix\\n    cm = confusion_matrix(pred_label, true_label, labels=model.classes_)\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\\n    fig = disp.plot()\\n\\n    return print(\\\"F1_stat \\\", model, \\\"is: \\\", f1_stat, fig)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 90;\n",
       "                var nbb_unformatted_code = \"def all_metrics(pred_label, true_label, model):\\n    \\\"\\\"\\\"\\\"\\\"\\n    pred_label = predicted label of the model\\n    true_label = true label\\n    model = model name\\n    \\\"\\\"\\\" \\\"\\\"\\n\\n    # F1 score\\n    f1_stat = f1_score(pred_label, true_label, average=\\\"weighted\\\")\\n\\n    # Confusion matrix\\n    cm = confusion_matrix(pred_label, true_label, labels=model.classes_)\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\\n    fig = disp.plot()\\n\\n    return print(\\\"F1_stat \\\", model, \\\"is: \\\", f1_stat, fig)\";\n",
       "                var nbb_formatted_code = \"def all_metrics(pred_label, true_label, model):\\n    \\\"\\\"\\\"\\\"\\\"\\n    pred_label = predicted label of the model\\n    true_label = true label\\n    model = model name\\n    \\\"\\\"\\\" \\\"\\\"\\n\\n    # F1 score\\n    f1_stat = f1_score(pred_label, true_label, average=\\\"weighted\\\")\\n\\n    # Confusion matrix\\n    cm = confusion_matrix(pred_label, true_label, labels=model.classes_)\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\\n    fig = disp.plot()\\n\\n    return print(\\\"F1_stat \\\", model, \\\"is: \\\", f1_stat, fig)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def all_metrics(pred_label, true_label, model):\n",
    "    \"\"\"\"\"\n",
    "    pred_label = predicted label of the model\n",
    "    true_label = true label\n",
    "    model = model name\n",
    "    \"\"\" \"\"\n",
    "\n",
    "    # F1 score\n",
    "    f1_stat = f1_score(pred_label, true_label, average=\"weighted\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(pred_label, true_label, labels=model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    fig = disp.plot()\n",
    "\n",
    "    return print(\"F1_stat \", model, \"is: \", f1_stat, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 91;\n",
       "                var nbb_unformatted_code = \"def evaluation(ri_data):  # ri1_train, ri2_train, etc..\\n\\n    # TRAIN NEW\\n    # Join labels to train set\\n    # Accepts\\n    train_accepts = pd.merge(\\n        X_train,\\n        y_train[[\\\"target\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    train_accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    # Rejects\\n    train_rejects = pd.merge(\\n        r_dev_mod,\\n        ri_data[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Align naming\\n    train_rejects.rename(columns={\\\"prediction_beforeRI\\\": \\\"target\\\"}, inplace=True)\\n\\n    # Create X and y\\n    X_res_rej, y_res_rej = create_X_y(train_rejects)\\n\\n    #     # Sample a matching number of observations from the accepts as the size of rejects\\n    #     # Shuffle the dataset\\n    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\\n    #     # Define a size for the train set\\n    #     train_size = int(0.25 * len(shuffle_df))\\n    #     train_accepts = shuffle_df[:train_size]\\n    #     print(train_accepts.shape)\\n\\n    # Concatenate Train Accepts and Train Rejects\\n    train_new = pd.concat([train_accepts, train_rejects])\\n\\n    # Flag\\n    train_new[\\\"Flag\\\"] = train_new[\\\"Flag1\\\"].apply(\\n        lambda x: \\\"Accept\\\" if x == \\\"Accept\\\" else \\\"Reject\\\"\\n    )\\n    train_new = train_new.drop(columns=[\\\"Flag1\\\"])\\n\\n    # Retrain KGB Model\\n\\n    # Split\\n    X_new = train_new[significant_columns]\\n    y_new = train_new[\\\"target\\\"]\\n    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\\n        X_new, y_new, test_size=0.2, random_state=42\\n    )\\n\\n    # Keep only columns for modelling\\n    os_data_X_2_new = X_train_new[significant_columns]\\n    X_test_2_new = X_test_new[significant_columns]\\n\\n    # Build Logistic regression\\n    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg2 = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # logreg2 = RandomForestClassifier()\\n    logreg2 = LGBMClassifier()\\n    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\\n\\n    # Predictions\\n    pred_test2 = pred(y_test, X_test, X_test_3, logreg2)\\n    pred_test2.rename(\\n        columns={\\\"prediction_beforeRI\\\": \\\"prediction_baseline\\\"}, inplace=True\\n    )\\n\\n    # Merge original and baseline predictions\\n    pred_test_final = pd.merge(\\n        pred_test1[[\\\"target\\\", \\\"prediction_beforeRI\\\"]],\\n        pred_test2[[\\\"prediction_baseline\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff 50percentile of the distribution\\n    q1 = pred_test_final[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = pred_test_final[\\\"prediction_baseline\\\"].quantile(q=1 - conservative_dr)\\n    #     print(q1)\\n    #     print(q2)\\n    pred_test_final[\\\"prediction_beforeRI_binary\\\"] = pred_test_final[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    pred_test_final[\\\"prediction_baseline\\\"] = pred_test_final[\\n        \\\"prediction_baseline\\\"\\n    ].apply(lambda x: 0 if (x < q2) else 1)\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final, \\\"target\\\", \\\"prediction_baseline\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final, \\\"with baseline\\\", \\\"prediction_baseline\\\")\\n    return pred_test_final\";\n",
       "                var nbb_formatted_code = \"def evaluation(ri_data):  # ri1_train, ri2_train, etc..\\n\\n    # TRAIN NEW\\n    # Join labels to train set\\n    # Accepts\\n    train_accepts = pd.merge(\\n        X_train,\\n        y_train[[\\\"target\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    train_accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    # Rejects\\n    train_rejects = pd.merge(\\n        r_dev_mod,\\n        ri_data[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Align naming\\n    train_rejects.rename(columns={\\\"prediction_beforeRI\\\": \\\"target\\\"}, inplace=True)\\n\\n    # Create X and y\\n    X_res_rej, y_res_rej = create_X_y(train_rejects)\\n\\n    #     # Sample a matching number of observations from the accepts as the size of rejects\\n    #     # Shuffle the dataset\\n    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\\n    #     # Define a size for the train set\\n    #     train_size = int(0.25 * len(shuffle_df))\\n    #     train_accepts = shuffle_df[:train_size]\\n    #     print(train_accepts.shape)\\n\\n    # Concatenate Train Accepts and Train Rejects\\n    train_new = pd.concat([train_accepts, train_rejects])\\n\\n    # Flag\\n    train_new[\\\"Flag\\\"] = train_new[\\\"Flag1\\\"].apply(\\n        lambda x: \\\"Accept\\\" if x == \\\"Accept\\\" else \\\"Reject\\\"\\n    )\\n    train_new = train_new.drop(columns=[\\\"Flag1\\\"])\\n\\n    # Retrain KGB Model\\n\\n    # Split\\n    X_new = train_new[significant_columns]\\n    y_new = train_new[\\\"target\\\"]\\n    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\\n        X_new, y_new, test_size=0.2, random_state=42\\n    )\\n\\n    # Keep only columns for modelling\\n    os_data_X_2_new = X_train_new[significant_columns]\\n    X_test_2_new = X_test_new[significant_columns]\\n\\n    # Build Logistic regression\\n    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg2 = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # logreg2 = RandomForestClassifier()\\n    logreg2 = LGBMClassifier()\\n    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\\n\\n    # Predictions\\n    pred_test2 = pred(y_test, X_test, X_test_3, logreg2)\\n    pred_test2.rename(\\n        columns={\\\"prediction_beforeRI\\\": \\\"prediction_baseline\\\"}, inplace=True\\n    )\\n\\n    # Merge original and baseline predictions\\n    pred_test_final = pd.merge(\\n        pred_test1[[\\\"target\\\", \\\"prediction_beforeRI\\\"]],\\n        pred_test2[[\\\"prediction_baseline\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff 50percentile of the distribution\\n    q1 = pred_test_final[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = pred_test_final[\\\"prediction_baseline\\\"].quantile(q=1 - conservative_dr)\\n    #     print(q1)\\n    #     print(q2)\\n    pred_test_final[\\\"prediction_beforeRI_binary\\\"] = pred_test_final[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    pred_test_final[\\\"prediction_baseline\\\"] = pred_test_final[\\n        \\\"prediction_baseline\\\"\\n    ].apply(lambda x: 0 if (x < q2) else 1)\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final, \\\"target\\\", \\\"prediction_baseline\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final, \\\"with baseline\\\", \\\"prediction_baseline\\\")\\n    return pred_test_final\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 91;\n",
       "                var nbb_unformatted_code = \"def evaluation(ri_data):  # ri1_train, ri2_train, etc..\\n\\n    # TRAIN NEW\\n    # Join labels to train set\\n    # Accepts\\n    train_accepts = pd.merge(\\n        X_train,\\n        y_train[[\\\"target\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    train_accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    # Rejects\\n    train_rejects = pd.merge(\\n        r_dev_mod,\\n        ri_data[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Align naming\\n    train_rejects.rename(columns={\\\"prediction_beforeRI\\\": \\\"target\\\"}, inplace=True)\\n\\n    # Create X and y\\n    X_res_rej, y_res_rej = create_X_y(train_rejects)\\n\\n    #     # Sample a matching number of observations from the accepts as the size of rejects\\n    #     # Shuffle the dataset\\n    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\\n    #     # Define a size for the train set\\n    #     train_size = int(0.25 * len(shuffle_df))\\n    #     train_accepts = shuffle_df[:train_size]\\n    #     print(train_accepts.shape)\\n\\n    # Concatenate Train Accepts and Train Rejects\\n    train_new = pd.concat([train_accepts, train_rejects])\\n\\n    # Flag\\n    train_new[\\\"Flag\\\"] = train_new[\\\"Flag1\\\"].apply(\\n        lambda x: \\\"Accept\\\" if x == \\\"Accept\\\" else \\\"Reject\\\"\\n    )\\n    train_new = train_new.drop(columns=[\\\"Flag1\\\"])\\n\\n    # Retrain KGB Model\\n\\n    # Split\\n    X_new = train_new[significant_columns]\\n    y_new = train_new[\\\"target\\\"]\\n    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\\n        X_new, y_new, test_size=0.2, random_state=42\\n    )\\n\\n    # Keep only columns for modelling\\n    os_data_X_2_new = X_train_new[significant_columns]\\n    X_test_2_new = X_test_new[significant_columns]\\n\\n    # Build Logistic regression\\n    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg2 = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # logreg2 = RandomForestClassifier()\\n    logreg2 = LGBMClassifier()\\n    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\\n\\n    # Predictions\\n    pred_test2 = pred(y_test, X_test, X_test_3, logreg2)\\n    pred_test2.rename(\\n        columns={\\\"prediction_beforeRI\\\": \\\"prediction_baseline\\\"}, inplace=True\\n    )\\n\\n    # Merge original and baseline predictions\\n    pred_test_final = pd.merge(\\n        pred_test1[[\\\"target\\\", \\\"prediction_beforeRI\\\"]],\\n        pred_test2[[\\\"prediction_baseline\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff 50percentile of the distribution\\n    q1 = pred_test_final[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = pred_test_final[\\\"prediction_baseline\\\"].quantile(q=1 - conservative_dr)\\n    #     print(q1)\\n    #     print(q2)\\n    pred_test_final[\\\"prediction_beforeRI_binary\\\"] = pred_test_final[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    pred_test_final[\\\"prediction_baseline\\\"] = pred_test_final[\\n        \\\"prediction_baseline\\\"\\n    ].apply(lambda x: 0 if (x < q2) else 1)\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final, \\\"target\\\", \\\"prediction_baseline\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final, \\\"with baseline\\\", \\\"prediction_baseline\\\")\\n    return pred_test_final\";\n",
       "                var nbb_formatted_code = \"def evaluation(ri_data):  # ri1_train, ri2_train, etc..\\n\\n    # TRAIN NEW\\n    # Join labels to train set\\n    # Accepts\\n    train_accepts = pd.merge(\\n        X_train,\\n        y_train[[\\\"target\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    train_accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    # Rejects\\n    train_rejects = pd.merge(\\n        r_dev_mod,\\n        ri_data[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Align naming\\n    train_rejects.rename(columns={\\\"prediction_beforeRI\\\": \\\"target\\\"}, inplace=True)\\n\\n    # Create X and y\\n    X_res_rej, y_res_rej = create_X_y(train_rejects)\\n\\n    #     # Sample a matching number of observations from the accepts as the size of rejects\\n    #     # Shuffle the dataset\\n    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\\n    #     # Define a size for the train set\\n    #     train_size = int(0.25 * len(shuffle_df))\\n    #     train_accepts = shuffle_df[:train_size]\\n    #     print(train_accepts.shape)\\n\\n    # Concatenate Train Accepts and Train Rejects\\n    train_new = pd.concat([train_accepts, train_rejects])\\n\\n    # Flag\\n    train_new[\\\"Flag\\\"] = train_new[\\\"Flag1\\\"].apply(\\n        lambda x: \\\"Accept\\\" if x == \\\"Accept\\\" else \\\"Reject\\\"\\n    )\\n    train_new = train_new.drop(columns=[\\\"Flag1\\\"])\\n\\n    # Retrain KGB Model\\n\\n    # Split\\n    X_new = train_new[significant_columns]\\n    y_new = train_new[\\\"target\\\"]\\n    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\\n        X_new, y_new, test_size=0.2, random_state=42\\n    )\\n\\n    # Keep only columns for modelling\\n    os_data_X_2_new = X_train_new[significant_columns]\\n    X_test_2_new = X_test_new[significant_columns]\\n\\n    # Build Logistic regression\\n    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg2 = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # logreg2 = RandomForestClassifier()\\n    logreg2 = LGBMClassifier()\\n    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\\n\\n    # Predictions\\n    pred_test2 = pred(y_test, X_test, X_test_3, logreg2)\\n    pred_test2.rename(\\n        columns={\\\"prediction_beforeRI\\\": \\\"prediction_baseline\\\"}, inplace=True\\n    )\\n\\n    # Merge original and baseline predictions\\n    pred_test_final = pd.merge(\\n        pred_test1[[\\\"target\\\", \\\"prediction_beforeRI\\\"]],\\n        pred_test2[[\\\"prediction_baseline\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff 50percentile of the distribution\\n    q1 = pred_test_final[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = pred_test_final[\\\"prediction_baseline\\\"].quantile(q=1 - conservative_dr)\\n    #     print(q1)\\n    #     print(q2)\\n    pred_test_final[\\\"prediction_beforeRI_binary\\\"] = pred_test_final[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    pred_test_final[\\\"prediction_baseline\\\"] = pred_test_final[\\n        \\\"prediction_baseline\\\"\\n    ].apply(lambda x: 0 if (x < q2) else 1)\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final, \\\"target\\\", \\\"prediction_baseline\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final, \\\"with baseline\\\", \\\"prediction_baseline\\\")\\n    return pred_test_final\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluation(ri_data):  # ri1_train, ri2_train, etc..\n",
    "\n",
    "    # TRAIN NEW\n",
    "    # Join labels to train set\n",
    "    # Accepts\n",
    "    train_accepts = pd.merge(\n",
    "        X_train,\n",
    "        y_train[[\"target\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    train_accepts[\"Flag1\"] = \"Accept\"\n",
    "\n",
    "    # Rejects\n",
    "    train_rejects = pd.merge(\n",
    "        r_dev_mod,\n",
    "        ri_data[[\"prediction_beforeRI\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Align naming\n",
    "    train_rejects.rename(columns={\"prediction_beforeRI\": \"target\"}, inplace=True)\n",
    "\n",
    "    # Create X and y\n",
    "    X_res_rej, y_res_rej = create_X_y(train_rejects)\n",
    "\n",
    "    #     # Sample a matching number of observations from the accepts as the size of rejects\n",
    "    #     # Shuffle the dataset\n",
    "    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\n",
    "    #     # Define a size for the train set\n",
    "    #     train_size = int(0.25 * len(shuffle_df))\n",
    "    #     train_accepts = shuffle_df[:train_size]\n",
    "    #     print(train_accepts.shape)\n",
    "\n",
    "    # Concatenate Train Accepts and Train Rejects\n",
    "    train_new = pd.concat([train_accepts, train_rejects])\n",
    "\n",
    "    # Flag\n",
    "    train_new[\"Flag\"] = train_new[\"Flag1\"].apply(\n",
    "        lambda x: \"Accept\" if x == \"Accept\" else \"Reject\"\n",
    "    )\n",
    "    train_new = train_new.drop(columns=[\"Flag1\"])\n",
    "\n",
    "    # Retrain KGB Model\n",
    "\n",
    "    # Split\n",
    "    X_new = train_new[significant_columns]\n",
    "    y_new = train_new[\"target\"]\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "        X_new, y_new, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Keep only columns for modelling\n",
    "    os_data_X_2_new = X_train_new[significant_columns]\n",
    "    X_test_2_new = X_test_new[significant_columns]\n",
    "\n",
    "    # Build Logistic regression\n",
    "    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\"none\")\n",
    "    # logreg2 = GradientBoostingClassifier(criterion=\"mse\")\n",
    "    # logreg2 = RandomForestClassifier()\n",
    "    logreg2 = LGBMClassifier()\n",
    "    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\n",
    "\n",
    "    # Predictions\n",
    "    pred_test2 = pred(y_test, X_test, X_test_3, logreg2)\n",
    "    pred_test2.rename(\n",
    "        columns={\"prediction_beforeRI\": \"prediction_baseline\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    # Merge original and baseline predictions\n",
    "    pred_test_final = pd.merge(\n",
    "        pred_test1[[\"target\", \"prediction_beforeRI\"]],\n",
    "        pred_test2[[\"prediction_baseline\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff 50percentile of the distribution\n",
    "    q1 = pred_test_final[\"prediction_beforeRI\"].quantile(q=1 - conservative_dr)\n",
    "    q2 = pred_test_final[\"prediction_baseline\"].quantile(q=1 - conservative_dr)\n",
    "    #     print(q1)\n",
    "    #     print(q2)\n",
    "    pred_test_final[\"prediction_beforeRI_binary\"] = pred_test_final[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(lambda x: 0 if (x < q1) else 1)\n",
    "    pred_test_final[\"prediction_baseline\"] = pred_test_final[\n",
    "        \"prediction_baseline\"\n",
    "    ].apply(lambda x: 0 if (x < q2) else 1)\n",
    "\n",
    "    #     # Log Loss\n",
    "    #     log_loss_fun(\"Before\", pred_test_final, \"target\", \"prediction_beforeRI_binary\")\n",
    "    #     log_loss_fun(\"After\", pred_test_final, \"target\", \"prediction_baseline\")\n",
    "\n",
    "    #     # Numbers of accurately classified and misclassified cases\n",
    "    #     print_results(pred_test_final, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    #     print_results(pred_test_final, \"with baseline\", \"prediction_baseline\")\n",
    "    return pred_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: The higher the DR the lower the quantile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 92;\n",
       "                var nbb_unformatted_code = \"def evaluation_rejects(ri_data):  # ri1_train, ri2_train, etc..\\n\\n    # TRAIN NEW\\n    # Join labels to train set\\n    # Accepts\\n    train_accepts = pd.merge(\\n        X_train,\\n        y_train[[\\\"target\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    train_accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    # Rejects\\n    train_rejects = pd.merge(\\n        r_dev_mod,\\n        ri_data[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Align naming\\n    train_rejects.rename(columns={\\\"prediction_beforeRI\\\": \\\"target\\\"}, inplace=True)\\n\\n    # Create X and y for rejects\\n    X_res_rej, y_res_rej = create_X_y(train_rejects)\\n\\n    #     # Sample a matching number of observations from the accepts as the size of rejetcs\\n    #     ## Shuffle the dataset\\n    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\\n    #     ## Define a size for the train set\\n    #     train_size = int(0.03 * len(shuffle_df))\\n    #     train_accepts = shuffle_df[:train_size]\\n    #     print(train_accepts.shape)\\n\\n    # Concatenate Train Accepts and Train Rejects\\n    train_new = pd.concat([train_accepts, train_rejects])\\n\\n    # Flag\\n    train_new[\\\"Flag\\\"] = train_new[\\\"Flag1\\\"].apply(\\n        lambda x: \\\"Accept\\\" if x == \\\"Accept\\\" else \\\"Reject\\\"\\n    )\\n    train_new = train_new.drop(columns=[\\\"Flag1\\\"])\\n\\n    # Retrain KGB Model\\n\\n    # Split\\n    X_new = train_new[significant_columns]\\n    y_new = train_new[\\\"target\\\"]\\n    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\\n        X_new, y_new, test_size=0.2, random_state=42\\n    )\\n\\n    # Keep only columns for modelling\\n    os_data_X_2_new = X_train_new[significant_columns]\\n    X_test_2_new = X_test_new[significant_columns]\\n\\n    # Build Logistic regression\\n    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg2 = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # logreg2 = RandomForestClassifier()\\n    logreg2 = LGBMClassifier()\\n    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\\n\\n    # Predictions\\n    pred_rej2 = pred(\\n        dfr_test_with_label_y, dfr_test_with_label_X, dfr_test_with_label_X, logreg2\\n    )\\n    pred_rej2.rename(\\n        columns={\\\"prediction_beforeRI\\\": \\\"prediction_baseline\\\"}, inplace=True\\n    )\\n\\n    # Merge original and baseline predictions\\n    pred_test_final = pd.merge(\\n        pred_rej1[[\\\"target\\\", \\\"prediction_beforeRI\\\"]],\\n        pred_rej2[[\\\"prediction_baseline\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff: median of the distribution\\n    q1 = pred_test_final[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = pred_test_final[\\\"prediction_baseline\\\"].quantile(q=1 - conservative_dr)\\n    #     print(q1)\\n    #     print(q2)\\n    pred_test_final[\\\"prediction_beforeRI_binary\\\"] = pred_test_final[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    pred_test_final[\\\"prediction_baseline\\\"] = pred_test_final[\\n        \\\"prediction_baseline\\\"\\n    ].apply(lambda x: 0 if (x < q2) else 1)\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final, \\\"target\\\", \\\"prediction_baseline\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final, \\\"with baseline\\\", \\\"prediction_baseline\\\")\\n    return pred_test_final\";\n",
       "                var nbb_formatted_code = \"def evaluation_rejects(ri_data):  # ri1_train, ri2_train, etc..\\n\\n    # TRAIN NEW\\n    # Join labels to train set\\n    # Accepts\\n    train_accepts = pd.merge(\\n        X_train,\\n        y_train[[\\\"target\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    train_accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    # Rejects\\n    train_rejects = pd.merge(\\n        r_dev_mod,\\n        ri_data[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Align naming\\n    train_rejects.rename(columns={\\\"prediction_beforeRI\\\": \\\"target\\\"}, inplace=True)\\n\\n    # Create X and y for rejects\\n    X_res_rej, y_res_rej = create_X_y(train_rejects)\\n\\n    #     # Sample a matching number of observations from the accepts as the size of rejetcs\\n    #     ## Shuffle the dataset\\n    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\\n    #     ## Define a size for the train set\\n    #     train_size = int(0.03 * len(shuffle_df))\\n    #     train_accepts = shuffle_df[:train_size]\\n    #     print(train_accepts.shape)\\n\\n    # Concatenate Train Accepts and Train Rejects\\n    train_new = pd.concat([train_accepts, train_rejects])\\n\\n    # Flag\\n    train_new[\\\"Flag\\\"] = train_new[\\\"Flag1\\\"].apply(\\n        lambda x: \\\"Accept\\\" if x == \\\"Accept\\\" else \\\"Reject\\\"\\n    )\\n    train_new = train_new.drop(columns=[\\\"Flag1\\\"])\\n\\n    # Retrain KGB Model\\n\\n    # Split\\n    X_new = train_new[significant_columns]\\n    y_new = train_new[\\\"target\\\"]\\n    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\\n        X_new, y_new, test_size=0.2, random_state=42\\n    )\\n\\n    # Keep only columns for modelling\\n    os_data_X_2_new = X_train_new[significant_columns]\\n    X_test_2_new = X_test_new[significant_columns]\\n\\n    # Build Logistic regression\\n    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg2 = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # logreg2 = RandomForestClassifier()\\n    logreg2 = LGBMClassifier()\\n    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\\n\\n    # Predictions\\n    pred_rej2 = pred(\\n        dfr_test_with_label_y, dfr_test_with_label_X, dfr_test_with_label_X, logreg2\\n    )\\n    pred_rej2.rename(\\n        columns={\\\"prediction_beforeRI\\\": \\\"prediction_baseline\\\"}, inplace=True\\n    )\\n\\n    # Merge original and baseline predictions\\n    pred_test_final = pd.merge(\\n        pred_rej1[[\\\"target\\\", \\\"prediction_beforeRI\\\"]],\\n        pred_rej2[[\\\"prediction_baseline\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff: median of the distribution\\n    q1 = pred_test_final[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = pred_test_final[\\\"prediction_baseline\\\"].quantile(q=1 - conservative_dr)\\n    #     print(q1)\\n    #     print(q2)\\n    pred_test_final[\\\"prediction_beforeRI_binary\\\"] = pred_test_final[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    pred_test_final[\\\"prediction_baseline\\\"] = pred_test_final[\\n        \\\"prediction_baseline\\\"\\n    ].apply(lambda x: 0 if (x < q2) else 1)\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final, \\\"target\\\", \\\"prediction_baseline\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final, \\\"with baseline\\\", \\\"prediction_baseline\\\")\\n    return pred_test_final\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 92;\n",
       "                var nbb_unformatted_code = \"def evaluation_rejects(ri_data):  # ri1_train, ri2_train, etc..\\n\\n    # TRAIN NEW\\n    # Join labels to train set\\n    # Accepts\\n    train_accepts = pd.merge(\\n        X_train,\\n        y_train[[\\\"target\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    train_accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    # Rejects\\n    train_rejects = pd.merge(\\n        r_dev_mod,\\n        ri_data[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Align naming\\n    train_rejects.rename(columns={\\\"prediction_beforeRI\\\": \\\"target\\\"}, inplace=True)\\n\\n    # Create X and y for rejects\\n    X_res_rej, y_res_rej = create_X_y(train_rejects)\\n\\n    #     # Sample a matching number of observations from the accepts as the size of rejetcs\\n    #     ## Shuffle the dataset\\n    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\\n    #     ## Define a size for the train set\\n    #     train_size = int(0.03 * len(shuffle_df))\\n    #     train_accepts = shuffle_df[:train_size]\\n    #     print(train_accepts.shape)\\n\\n    # Concatenate Train Accepts and Train Rejects\\n    train_new = pd.concat([train_accepts, train_rejects])\\n\\n    # Flag\\n    train_new[\\\"Flag\\\"] = train_new[\\\"Flag1\\\"].apply(\\n        lambda x: \\\"Accept\\\" if x == \\\"Accept\\\" else \\\"Reject\\\"\\n    )\\n    train_new = train_new.drop(columns=[\\\"Flag1\\\"])\\n\\n    # Retrain KGB Model\\n\\n    # Split\\n    X_new = train_new[significant_columns]\\n    y_new = train_new[\\\"target\\\"]\\n    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\\n        X_new, y_new, test_size=0.2, random_state=42\\n    )\\n\\n    # Keep only columns for modelling\\n    os_data_X_2_new = X_train_new[significant_columns]\\n    X_test_2_new = X_test_new[significant_columns]\\n\\n    # Build Logistic regression\\n    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg2 = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # logreg2 = RandomForestClassifier()\\n    logreg2 = LGBMClassifier()\\n    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\\n\\n    # Predictions\\n    pred_rej2 = pred(\\n        dfr_test_with_label_y, dfr_test_with_label_X, dfr_test_with_label_X, logreg2\\n    )\\n    pred_rej2.rename(\\n        columns={\\\"prediction_beforeRI\\\": \\\"prediction_baseline\\\"}, inplace=True\\n    )\\n\\n    # Merge original and baseline predictions\\n    pred_test_final = pd.merge(\\n        pred_rej1[[\\\"target\\\", \\\"prediction_beforeRI\\\"]],\\n        pred_rej2[[\\\"prediction_baseline\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff: median of the distribution\\n    q1 = pred_test_final[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = pred_test_final[\\\"prediction_baseline\\\"].quantile(q=1 - conservative_dr)\\n    #     print(q1)\\n    #     print(q2)\\n    pred_test_final[\\\"prediction_beforeRI_binary\\\"] = pred_test_final[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    pred_test_final[\\\"prediction_baseline\\\"] = pred_test_final[\\n        \\\"prediction_baseline\\\"\\n    ].apply(lambda x: 0 if (x < q2) else 1)\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final, \\\"target\\\", \\\"prediction_baseline\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final, \\\"with baseline\\\", \\\"prediction_baseline\\\")\\n    return pred_test_final\";\n",
       "                var nbb_formatted_code = \"def evaluation_rejects(ri_data):  # ri1_train, ri2_train, etc..\\n\\n    # TRAIN NEW\\n    # Join labels to train set\\n    # Accepts\\n    train_accepts = pd.merge(\\n        X_train,\\n        y_train[[\\\"target\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    train_accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    # Rejects\\n    train_rejects = pd.merge(\\n        r_dev_mod,\\n        ri_data[[\\\"prediction_beforeRI\\\"]],\\n        how=\\\"left\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Align naming\\n    train_rejects.rename(columns={\\\"prediction_beforeRI\\\": \\\"target\\\"}, inplace=True)\\n\\n    # Create X and y for rejects\\n    X_res_rej, y_res_rej = create_X_y(train_rejects)\\n\\n    #     # Sample a matching number of observations from the accepts as the size of rejetcs\\n    #     ## Shuffle the dataset\\n    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\\n    #     ## Define a size for the train set\\n    #     train_size = int(0.03 * len(shuffle_df))\\n    #     train_accepts = shuffle_df[:train_size]\\n    #     print(train_accepts.shape)\\n\\n    # Concatenate Train Accepts and Train Rejects\\n    train_new = pd.concat([train_accepts, train_rejects])\\n\\n    # Flag\\n    train_new[\\\"Flag\\\"] = train_new[\\\"Flag1\\\"].apply(\\n        lambda x: \\\"Accept\\\" if x == \\\"Accept\\\" else \\\"Reject\\\"\\n    )\\n    train_new = train_new.drop(columns=[\\\"Flag1\\\"])\\n\\n    # Retrain KGB Model\\n\\n    # Split\\n    X_new = train_new[significant_columns]\\n    y_new = train_new[\\\"target\\\"]\\n    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\\n        X_new, y_new, test_size=0.2, random_state=42\\n    )\\n\\n    # Keep only columns for modelling\\n    os_data_X_2_new = X_train_new[significant_columns]\\n    X_test_2_new = X_test_new[significant_columns]\\n\\n    # Build Logistic regression\\n    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\\\"none\\\")\\n    # logreg2 = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # logreg2 = RandomForestClassifier()\\n    logreg2 = LGBMClassifier()\\n    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\\n\\n    # Predictions\\n    pred_rej2 = pred(\\n        dfr_test_with_label_y, dfr_test_with_label_X, dfr_test_with_label_X, logreg2\\n    )\\n    pred_rej2.rename(\\n        columns={\\\"prediction_beforeRI\\\": \\\"prediction_baseline\\\"}, inplace=True\\n    )\\n\\n    # Merge original and baseline predictions\\n    pred_test_final = pd.merge(\\n        pred_rej1[[\\\"target\\\", \\\"prediction_beforeRI\\\"]],\\n        pred_rej2[[\\\"prediction_baseline\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff: median of the distribution\\n    q1 = pred_test_final[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = pred_test_final[\\\"prediction_baseline\\\"].quantile(q=1 - conservative_dr)\\n    #     print(q1)\\n    #     print(q2)\\n    pred_test_final[\\\"prediction_beforeRI_binary\\\"] = pred_test_final[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    pred_test_final[\\\"prediction_baseline\\\"] = pred_test_final[\\n        \\\"prediction_baseline\\\"\\n    ].apply(lambda x: 0 if (x < q2) else 1)\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final, \\\"target\\\", \\\"prediction_baseline\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final, \\\"with baseline\\\", \\\"prediction_baseline\\\")\\n    return pred_test_final\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluation_rejects(ri_data):  # ri1_train, ri2_train, etc..\n",
    "\n",
    "    # TRAIN NEW\n",
    "    # Join labels to train set\n",
    "    # Accepts\n",
    "    train_accepts = pd.merge(\n",
    "        X_train,\n",
    "        y_train[[\"target\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    train_accepts[\"Flag1\"] = \"Accept\"\n",
    "\n",
    "    # Rejects\n",
    "    train_rejects = pd.merge(\n",
    "        r_dev_mod,\n",
    "        ri_data[[\"prediction_beforeRI\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Align naming\n",
    "    train_rejects.rename(columns={\"prediction_beforeRI\": \"target\"}, inplace=True)\n",
    "\n",
    "    # Create X and y for rejects\n",
    "    X_res_rej, y_res_rej = create_X_y(train_rejects)\n",
    "\n",
    "    #     # Sample a matching number of observations from the accepts as the size of rejetcs\n",
    "    #     ## Shuffle the dataset\n",
    "    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\n",
    "    #     ## Define a size for the train set\n",
    "    #     train_size = int(0.03 * len(shuffle_df))\n",
    "    #     train_accepts = shuffle_df[:train_size]\n",
    "    #     print(train_accepts.shape)\n",
    "\n",
    "    # Concatenate Train Accepts and Train Rejects\n",
    "    train_new = pd.concat([train_accepts, train_rejects])\n",
    "\n",
    "    # Flag\n",
    "    train_new[\"Flag\"] = train_new[\"Flag1\"].apply(\n",
    "        lambda x: \"Accept\" if x == \"Accept\" else \"Reject\"\n",
    "    )\n",
    "    train_new = train_new.drop(columns=[\"Flag1\"])\n",
    "\n",
    "    # Retrain KGB Model\n",
    "\n",
    "    # Split\n",
    "    X_new = train_new[significant_columns]\n",
    "    y_new = train_new[\"target\"]\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "        X_new, y_new, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Keep only columns for modelling\n",
    "    os_data_X_2_new = X_train_new[significant_columns]\n",
    "    X_test_2_new = X_test_new[significant_columns]\n",
    "\n",
    "    # Build Logistic regression\n",
    "    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\"none\")\n",
    "    # logreg2 = GradientBoostingClassifier(criterion=\"mse\")\n",
    "    # logreg2 = RandomForestClassifier()\n",
    "    logreg2 = LGBMClassifier()\n",
    "    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\n",
    "\n",
    "    # Predictions\n",
    "    pred_rej2 = pred(\n",
    "        dfr_test_with_label_y, dfr_test_with_label_X, dfr_test_with_label_X, logreg2\n",
    "    )\n",
    "    pred_rej2.rename(\n",
    "        columns={\"prediction_beforeRI\": \"prediction_baseline\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    # Merge original and baseline predictions\n",
    "    pred_test_final = pd.merge(\n",
    "        pred_rej1[[\"target\", \"prediction_beforeRI\"]],\n",
    "        pred_rej2[[\"prediction_baseline\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff: median of the distribution\n",
    "    q1 = pred_test_final[\"prediction_beforeRI\"].quantile(q=1 - conservative_dr)\n",
    "    q2 = pred_test_final[\"prediction_baseline\"].quantile(q=1 - conservative_dr)\n",
    "    #     print(q1)\n",
    "    #     print(q2)\n",
    "    pred_test_final[\"prediction_beforeRI_binary\"] = pred_test_final[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(lambda x: 0 if (x < q1) else 1)\n",
    "    pred_test_final[\"prediction_baseline\"] = pred_test_final[\n",
    "        \"prediction_baseline\"\n",
    "    ].apply(lambda x: 0 if (x < q2) else 1)\n",
    "\n",
    "    #     # Log Loss\n",
    "    #     log_loss_fun(\"Before\", pred_test_final, \"target\", \"prediction_beforeRI_binary\")\n",
    "    #     log_loss_fun(\"After\", pred_test_final, \"target\", \"prediction_baseline\")\n",
    "\n",
    "    #     # Numbers of accurately classified and misclassified cases\n",
    "    #     print_results(pred_test_final, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    #     print_results(pred_test_final, \"with baseline\", \"prediction_baseline\")\n",
    "    return pred_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kickout Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 112;\n",
       "                var nbb_unformatted_code = \"def flag_df(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_baseline\\\"] == 1\\n    ):\\n        return \\\"KB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_baseline\\\"] == 1\\n    ):\\n        return \\\"KG\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_baseline\\\"] == 0\\n    ):\\n        return \\\"IG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_baseline\\\"] == 0\\n    ):\\n        return \\\"IB\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_baseline\\\"] == 1\\n    ):\\n        return \\\"KB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_baseline\\\"] == 1\\n    ):\\n        return \\\"KG\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_baseline\\\"] == 0\\n    ):\\n        return \\\"IG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_baseline\\\"] == 0\\n    ):\\n        return \\\"IB\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 112;\n",
       "                var nbb_unformatted_code = \"def flag_df(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_baseline\\\"] == 1\\n    ):\\n        return \\\"KB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_baseline\\\"] == 1\\n    ):\\n        return \\\"KG\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_baseline\\\"] == 0\\n    ):\\n        return \\\"IG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_baseline\\\"] == 0\\n    ):\\n        return \\\"IB\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_baseline\\\"] == 1\\n    ):\\n        return \\\"KB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_baseline\\\"] == 1\\n    ):\\n        return \\\"KG\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_baseline\\\"] == 0\\n    ):\\n        return \\\"IG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_baseline\\\"] == 0\\n    ):\\n        return \\\"IB\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flag_df(df):\n",
    "\n",
    "    # Flag kicked out bad cases (want more of these)\n",
    "    if (\n",
    "        df[\"target\"] == 1\n",
    "        and df[\"prediction_beforeRI_binary\"] == 0\n",
    "        and df[\"prediction_baseline\"] == 1\n",
    "    ):\n",
    "        return \"KB\"\n",
    "\n",
    "    # Flag kicked out good cases (want less of these)\n",
    "    elif (\n",
    "        df[\"target\"] == 0\n",
    "        and df[\"prediction_beforeRI_binary\"] == 0\n",
    "        and df[\"prediction_baseline\"] == 1\n",
    "    ):\n",
    "        return \"KG\"\n",
    "\n",
    "    # Flag kicked in good cases (want more of these)\n",
    "    elif (\n",
    "        df[\"target\"] == 0\n",
    "        and df[\"prediction_beforeRI_binary\"] == 1\n",
    "        and df[\"prediction_baseline\"] == 0\n",
    "    ):\n",
    "        return \"IG\"\n",
    "\n",
    "    # Flag kicked in bad cases (want less of these)\n",
    "    elif (\n",
    "        df[\"target\"] == 1\n",
    "        and df[\"prediction_beforeRI_binary\"] == 1\n",
    "        and df[\"prediction_baseline\"] == 0\n",
    "    ):\n",
    "        return \"IB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 113;\n",
       "                var nbb_unformatted_code = \"def kickout(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"KB\\\" in df.values:\\n        kb = counts.KB  # want more of these\\n    else:\\n        kb = 0\\n    if \\\"KG\\\" in df.values:\\n        kg = counts.KG  # want less of these\\n    else:\\n        kg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want more of these\\n    else:\\n        ig = 0\\n\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    # Counts of number of actual bad cases\\n    sb = df[df[\\\"target\\\"] == 1].shape[0]\\n    sg = df[df[\\\"target\\\"] == 0].shape[0]\\n\\n    # Target\\n    counts_target = df[\\\"target\\\"].value_counts()\\n    total_bads = counts_target[0]\\n    total_goods = counts_target[1]\\n\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    # Calculate kickout metric\\n    kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * pb)\\n    kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg * pg)\\n    weighted_total = kickout + kickin\\n\\n    return (\\n        print(\\n            \\\"Kickout + Kickin Metric:\\\",\\n            round(weighted_total, 3),\\n        ),\\n    )\";\n",
       "                var nbb_formatted_code = \"def kickout(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"KB\\\" in df.values:\\n        kb = counts.KB  # want more of these\\n    else:\\n        kb = 0\\n    if \\\"KG\\\" in df.values:\\n        kg = counts.KG  # want less of these\\n    else:\\n        kg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want more of these\\n    else:\\n        ig = 0\\n\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    # Counts of number of actual bad cases\\n    sb = df[df[\\\"target\\\"] == 1].shape[0]\\n    sg = df[df[\\\"target\\\"] == 0].shape[0]\\n\\n    # Target\\n    counts_target = df[\\\"target\\\"].value_counts()\\n    total_bads = counts_target[0]\\n    total_goods = counts_target[1]\\n\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    # Calculate kickout metric\\n    kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * pb)\\n    kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg * pg)\\n    weighted_total = kickout + kickin\\n\\n    return (\\n        print(\\n            \\\"Kickout + Kickin Metric:\\\",\\n            round(weighted_total, 3),\\n        ),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 113;\n",
       "                var nbb_unformatted_code = \"def kickout(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"KB\\\" in df.values:\\n        kb = counts.KB  # want more of these\\n    else:\\n        kb = 0\\n    if \\\"KG\\\" in df.values:\\n        kg = counts.KG  # want less of these\\n    else:\\n        kg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want more of these\\n    else:\\n        ig = 0\\n\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    # Counts of number of actual bad cases\\n    sb = df[df[\\\"target\\\"] == 1].shape[0]\\n    sg = df[df[\\\"target\\\"] == 0].shape[0]\\n\\n    # Target\\n    counts_target = df[\\\"target\\\"].value_counts()\\n    total_bads = counts_target[0]\\n    total_goods = counts_target[1]\\n\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    # Calculate kickout metric\\n    kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * pb)\\n    kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg * pg)\\n    weighted_total = kickout + kickin\\n\\n    return (\\n        print(\\n            \\\"Kickout + Kickin Metric:\\\",\\n            round(weighted_total, 3),\\n        ),\\n    )\";\n",
       "                var nbb_formatted_code = \"def kickout(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"KB\\\" in df.values:\\n        kb = counts.KB  # want more of these\\n    else:\\n        kb = 0\\n    if \\\"KG\\\" in df.values:\\n        kg = counts.KG  # want less of these\\n    else:\\n        kg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want more of these\\n    else:\\n        ig = 0\\n\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    # Counts of number of actual bad cases\\n    sb = df[df[\\\"target\\\"] == 1].shape[0]\\n    sg = df[df[\\\"target\\\"] == 0].shape[0]\\n\\n    # Target\\n    counts_target = df[\\\"target\\\"].value_counts()\\n    total_bads = counts_target[0]\\n    total_goods = counts_target[1]\\n\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    # Calculate kickout metric\\n    kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * pb)\\n    kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg * pg)\\n    weighted_total = kickout + kickin\\n\\n    return (\\n        print(\\n            \\\"Kickout + Kickin Metric:\\\",\\n            round(weighted_total, 3),\\n        ),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def kickout(df):\n",
    "\n",
    "    # Counts of kickout bad and kickout good\n",
    "    counts = df[\"Flag\"].value_counts()\n",
    "    if \"KB\" in df.values:\n",
    "        kb = counts.KB  # want more of these\n",
    "    else:\n",
    "        kb = 0\n",
    "    if \"KG\" in df.values:\n",
    "        kg = counts.KG  # want less of these\n",
    "    else:\n",
    "        kg = 0\n",
    "\n",
    "    if \"IG\" in df.values:\n",
    "        ig = counts.IG  # want more of these\n",
    "    else:\n",
    "        ig = 0\n",
    "\n",
    "    if \"IB\" in df.values:\n",
    "        ib = counts.IB  # want less of these\n",
    "    else:\n",
    "        ib = 0\n",
    "\n",
    "    # Counts of number of actual bad cases\n",
    "    sb = df[df[\"target\"] == 1].shape[0]\n",
    "    sg = df[df[\"target\"] == 0].shape[0]\n",
    "\n",
    "    # Target\n",
    "    counts_target = df[\"target\"].value_counts()\n",
    "    total_bads = counts_target[0]\n",
    "    total_goods = counts_target[1]\n",
    "\n",
    "    total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "    total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "    pb = total_bads / (total_bads + total_goods)\n",
    "    pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "    # Calculate kickout metric\n",
    "    kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * pb)\n",
    "    kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg * pg)\n",
    "    weighted_total = kickout + kickin\n",
    "\n",
    "    return (\n",
    "        print(\n",
    "            \"Kickout + Kickin Metric:\",\n",
    "            round(weighted_total, 3),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 114;\n",
       "                var nbb_unformatted_code = \"def flag_df_beforeRI(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_beforeRI_binary\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_beforeRI_binary\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_beforeRI_binary\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_beforeRI_binary\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df_beforeRI(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_beforeRI_binary\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_beforeRI_binary\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_beforeRI_binary\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_beforeRI_binary\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 114;\n",
       "                var nbb_unformatted_code = \"def flag_df_beforeRI(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_beforeRI_binary\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_beforeRI_binary\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_beforeRI_binary\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_beforeRI_binary\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df_beforeRI(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_beforeRI_binary\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_beforeRI_binary\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_beforeRI_binary\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_beforeRI_binary\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flag_df_beforeRI(df):\n",
    "\n",
    "    # Flag kicked out bad cases (want more of these)\n",
    "    if df[\"target\"] == 1 and df[\"prediction_beforeRI_binary\"] == 1:\n",
    "        return \"CB\"\n",
    "\n",
    "    # Flag kicked out good cases (want less of these)\n",
    "    elif df[\"target\"] == 1 and df[\"prediction_beforeRI_binary\"] == 0:\n",
    "        return \"IB\"\n",
    "\n",
    "    # Flag kicked in good cases (want more of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_beforeRI_binary\"] == 0:\n",
    "        return \"CG\"\n",
    "\n",
    "    # Flag kicked in bad cases (want less of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_beforeRI_binary\"] == 1:\n",
    "        return \"IG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 115;\n",
       "                var nbb_unformatted_code = \"def kickout_beforeRI(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_formatted_code = \"def kickout_beforeRI(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 115;\n",
       "                var nbb_unformatted_code = \"def kickout_beforeRI(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_formatted_code = \"def kickout_beforeRI(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def kickout_beforeRI(df):\n",
    "\n",
    "    # Counts of kickout bad and kickout good\n",
    "    counts = df[\"Flag\"].value_counts()\n",
    "    if \"CB\" in df.values:\n",
    "        cb = counts.CB  # want more of these\n",
    "    else:\n",
    "        cb = 0\n",
    "    if \"IB\" in df.values:\n",
    "        ib = counts.IB  # want less of these\n",
    "    else:\n",
    "        ib = 0\n",
    "\n",
    "    if \"CG\" in df.values:\n",
    "        cg = counts.CG  # want more of these\n",
    "    else:\n",
    "        cg = 0\n",
    "\n",
    "    if \"IG\" in df.values:\n",
    "        ig = counts.IG  # want less of these\n",
    "    else:\n",
    "        ig = 0\n",
    "\n",
    "    # Target\n",
    "    total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "    total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "    pb = total_bads / (total_bads + total_goods)\n",
    "    pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\n",
    "    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\n",
    "    weighted_total = kickout + kickin\n",
    "    return print(\n",
    "        \"Kickout + Kickin Metric:\",\n",
    "        round(weighted_total, 3),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 116;\n",
       "                var nbb_unformatted_code = \"def flag_df_baseline(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_baseline\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_baseline\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_baseline\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_baseline\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df_baseline(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_baseline\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_baseline\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_baseline\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_baseline\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 116;\n",
       "                var nbb_unformatted_code = \"def flag_df_baseline(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_baseline\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_baseline\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_baseline\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_baseline\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df_baseline(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_baseline\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_baseline\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_baseline\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_baseline\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flag_df_baseline(df):\n",
    "\n",
    "    # Flag kicked out bad cases (want more of these)\n",
    "    if df[\"target\"] == 1 and df[\"prediction_baseline\"] == 1:\n",
    "        return \"CB\"\n",
    "\n",
    "    # Flag kicked out good cases (want less of these)\n",
    "    elif df[\"target\"] == 1 and df[\"prediction_baseline\"] == 0:\n",
    "        return \"IB\"\n",
    "\n",
    "    # Flag kicked in good cases (want more of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_baseline\"] == 0:\n",
    "        return \"CG\"\n",
    "\n",
    "    # Flag kicked in bad cases (want less of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_baseline\"] == 1:\n",
    "        return \"IG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 117;\n",
       "                var nbb_unformatted_code = \"def kickout_baseline(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_formatted_code = \"def kickout_baseline(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 117;\n",
       "                var nbb_unformatted_code = \"def kickout_baseline(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_formatted_code = \"def kickout_baseline(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def kickout_baseline(df):\n",
    "\n",
    "    # Counts of kickout bad and kickout good\n",
    "    counts = df[\"Flag\"].value_counts()\n",
    "    if \"CB\" in df.values:\n",
    "        cb = counts.CB  # want more of these\n",
    "    else:\n",
    "        cb = 0\n",
    "    if \"IB\" in df.values:\n",
    "        ib = counts.IB  # want less of these\n",
    "    else:\n",
    "        ib = 0\n",
    "\n",
    "    if \"CG\" in df.values:\n",
    "        cg = counts.CG  # want more of these\n",
    "    else:\n",
    "        cg = 0\n",
    "\n",
    "    if \"IG\" in df.values:\n",
    "        ig = counts.IG  # want less of these\n",
    "    else:\n",
    "        ig = 0\n",
    "\n",
    "    # Target\n",
    "    total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "    total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "    pb = total_bads / (total_bads + total_goods)\n",
    "    pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\n",
    "    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\n",
    "    weighted_total = kickout + kickin\n",
    "    return print(\n",
    "        \"Kickout + Kickin Metric:\",\n",
    "        round(weighted_total, 3),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 118;\n",
       "                var nbb_unformatted_code = \"outcome_a = evaluation(ri1_train)  # Simple Augmentation - Log Reg\";\n",
       "                var nbb_formatted_code = \"outcome_a = evaluation(ri1_train)  # Simple Augmentation - Log Reg\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 118;\n",
       "                var nbb_unformatted_code = \"outcome_a = evaluation(ri1_train)  # Simple Augmentation - Log Reg\";\n",
       "                var nbb_formatted_code = \"outcome_a = evaluation(ri1_train)  # Simple Augmentation - Log Reg\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outcome_a = evaluation(ri1_train)  # Simple Augmentation - Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kickout + Kickin Metric: 0.834\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 119;\n",
       "                var nbb_unformatted_code = \"outcome_a[\\\"Flag\\\"] = outcome_a.apply(flag_df_beforeRI, axis=1)\\nkickout_beforeRI(outcome_a)\";\n",
       "                var nbb_formatted_code = \"outcome_a[\\\"Flag\\\"] = outcome_a.apply(flag_df_beforeRI, axis=1)\\nkickout_beforeRI(outcome_a)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 119;\n",
       "                var nbb_unformatted_code = \"outcome_a[\\\"Flag\\\"] = outcome_a.apply(flag_df_beforeRI, axis=1)\\nkickout_beforeRI(outcome_a)\";\n",
       "                var nbb_formatted_code = \"outcome_a[\\\"Flag\\\"] = outcome_a.apply(flag_df_beforeRI, axis=1)\\nkickout_beforeRI(outcome_a)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outcome_a[\"Flag\"] = outcome_a.apply(flag_df_beforeRI, axis=1)\n",
    "kickout_beforeRI(outcome_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kickout + Kickin Metric: 0.825\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 120;\n",
       "                var nbb_unformatted_code = \"outcome_a[\\\"Flag\\\"] = outcome_a.apply(flag_df_baseline, axis=1)\\nkickout_baseline(outcome_a)\";\n",
       "                var nbb_formatted_code = \"outcome_a[\\\"Flag\\\"] = outcome_a.apply(flag_df_baseline, axis=1)\\nkickout_baseline(outcome_a)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 120;\n",
       "                var nbb_unformatted_code = \"outcome_a[\\\"Flag\\\"] = outcome_a.apply(flag_df_baseline, axis=1)\\nkickout_baseline(outcome_a)\";\n",
       "                var nbb_formatted_code = \"outcome_a[\\\"Flag\\\"] = outcome_a.apply(flag_df_baseline, axis=1)\\nkickout_baseline(outcome_a)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outcome_a[\"Flag\"] = outcome_a.apply(flag_df_baseline, axis=1)\n",
    "kickout_baseline(outcome_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 121;\n",
       "                var nbb_unformatted_code = \"outcome_b = evaluation_rejects(ri1_train_rej)  # Simple Augmentation - Log Reg\";\n",
       "                var nbb_formatted_code = \"outcome_b = evaluation_rejects(ri1_train_rej)  # Simple Augmentation - Log Reg\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 121;\n",
       "                var nbb_unformatted_code = \"outcome_b = evaluation_rejects(ri1_train_rej)  # Simple Augmentation - Log Reg\";\n",
       "                var nbb_formatted_code = \"outcome_b = evaluation_rejects(ri1_train_rej)  # Simple Augmentation - Log Reg\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outcome_b = evaluation_rejects(ri1_train_rej)  # Simple Augmentation - Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kickout + Kickin Metric: 0.735\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 122;\n",
       "                var nbb_unformatted_code = \"outcome_b[\\\"Flag\\\"] = outcome_b.apply(flag_df_beforeRI, axis=1)\\nkickout_beforeRI(outcome_b)\";\n",
       "                var nbb_formatted_code = \"outcome_b[\\\"Flag\\\"] = outcome_b.apply(flag_df_beforeRI, axis=1)\\nkickout_beforeRI(outcome_b)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 122;\n",
       "                var nbb_unformatted_code = \"outcome_b[\\\"Flag\\\"] = outcome_b.apply(flag_df_beforeRI, axis=1)\\nkickout_beforeRI(outcome_b)\";\n",
       "                var nbb_formatted_code = \"outcome_b[\\\"Flag\\\"] = outcome_b.apply(flag_df_beforeRI, axis=1)\\nkickout_beforeRI(outcome_b)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outcome_b[\"Flag\"] = outcome_b.apply(flag_df_beforeRI, axis=1)\n",
    "kickout_beforeRI(outcome_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kickout + Kickin Metric: 0.745\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 123;\n",
       "                var nbb_unformatted_code = \"outcome_b[\\\"Flag\\\"] = outcome_b.apply(flag_df_baseline, axis=1)\\nkickout_baseline(outcome_b)\";\n",
       "                var nbb_formatted_code = \"outcome_b[\\\"Flag\\\"] = outcome_b.apply(flag_df_baseline, axis=1)\\nkickout_baseline(outcome_b)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 123;\n",
       "                var nbb_unformatted_code = \"outcome_b[\\\"Flag\\\"] = outcome_b.apply(flag_df_baseline, axis=1)\\nkickout_baseline(outcome_b)\";\n",
       "                var nbb_formatted_code = \"outcome_b[\\\"Flag\\\"] = outcome_b.apply(flag_df_baseline, axis=1)\\nkickout_baseline(outcome_b)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outcome_b[\"Flag\"] = outcome_b.apply(flag_df_baseline, axis=1)\n",
    "kickout_baseline(outcome_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on existing literature semi-supervised learning is chosen for RI. The method is suitable for the problem because the labels of the accepted population (good/bad) are known and the labels of the rejected population are unknown. Without ignoring the inherent bias between accepts and rejects, semi-supervised methods use both labelled and unlabelled data during fit. <br> **1. Data preparation:** the goal is to create initial dataframe, which contains the known training data and training labels of the accepts and the known training data of the rejects. The training labels of the rejects are unknown, and are therefore labelled with a default value of -1. <br> **2. Train/Test Split** The resulting dataset is again split into explanatory variables and target in order to fit the model. <br> **3. Fit model:** The semi-supervised model is fit. Three types of models are tested: Self Training, Label Spreading and Label Propagation <br> **4. Predictions:** Predictions are made using the known testing data and testing labels of the accepts and the test lables of the rejects <br> **5. Evaluation:** The results of the model before and after reject inference are compared  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 124;\n",
       "                var nbb_unformatted_code = \"def ssl_prep(X_accept, y_accept, X_reject):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train_acc : training data of accepted population\\n    y_train_acc: training lables of accepted population\\n    X_train_rej: training data of rejected population\\n\\n    Return\\n    ------\\n    df : data of accepted and rejected population\\n\\n    \\\"\\\"\\\"\\n    # Merge explanatory and target in accepts\\n    accepts = pd.merge(\\n        X_accept, y_accept, how=\\\"left\\\", left_index=True, right_index=True\\n    )\\n    # Create accept flag\\n    accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    #     # Sample a matching number of observations from the rejects as the size of accepts\\n    #     ## Shuffle the dataset\\n    #     shuffle_df = X_reject.sample(frac=1)\\n    #     ## Define a size for the train set\\n    #     train_size = int(0.25 * len(shuffle_df))\\n    #     train_rejects = shuffle_df[:train_size]\\n\\n    train_rejects = X_reject\\n\\n    # Merge accepts and rejects\\n    df = pd.concat([accepts, train_rejects])\\n\\n    # If accepted use accept label, if rejected use -1 (default value for unlabelled entries) - hard-coded for now\\n    conditions = [\\n        (df[\\\"Flag1\\\"] == \\\"Accept\\\") & (df[\\\"target\\\"] == 1),\\n        (df[\\\"Flag1\\\"] == \\\"Accept\\\") & (df[\\\"target\\\"] == 0),\\n    ]\\n    choices = [1, 0]\\n\\n    # New target is called unlabel\\n    df[\\\"unlabel\\\"] = np.select(conditions, choices, -1)\\n\\n    # Select columns for modelling - hard-coded for now - can be moved outside of the function\\n    df = df[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\", \\\"unlabel\\\"]]\\n\\n    return df\";\n",
       "                var nbb_formatted_code = \"def ssl_prep(X_accept, y_accept, X_reject):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train_acc : training data of accepted population\\n    y_train_acc: training lables of accepted population\\n    X_train_rej: training data of rejected population\\n\\n    Return\\n    ------\\n    df : data of accepted and rejected population\\n\\n    \\\"\\\"\\\"\\n    # Merge explanatory and target in accepts\\n    accepts = pd.merge(\\n        X_accept, y_accept, how=\\\"left\\\", left_index=True, right_index=True\\n    )\\n    # Create accept flag\\n    accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    #     # Sample a matching number of observations from the rejects as the size of accepts\\n    #     ## Shuffle the dataset\\n    #     shuffle_df = X_reject.sample(frac=1)\\n    #     ## Define a size for the train set\\n    #     train_size = int(0.25 * len(shuffle_df))\\n    #     train_rejects = shuffle_df[:train_size]\\n\\n    train_rejects = X_reject\\n\\n    # Merge accepts and rejects\\n    df = pd.concat([accepts, train_rejects])\\n\\n    # If accepted use accept label, if rejected use -1 (default value for unlabelled entries) - hard-coded for now\\n    conditions = [\\n        (df[\\\"Flag1\\\"] == \\\"Accept\\\") & (df[\\\"target\\\"] == 1),\\n        (df[\\\"Flag1\\\"] == \\\"Accept\\\") & (df[\\\"target\\\"] == 0),\\n    ]\\n    choices = [1, 0]\\n\\n    # New target is called unlabel\\n    df[\\\"unlabel\\\"] = np.select(conditions, choices, -1)\\n\\n    # Select columns for modelling - hard-coded for now - can be moved outside of the function\\n    df = df[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\", \\\"unlabel\\\"]]\\n\\n    return df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 124;\n",
       "                var nbb_unformatted_code = \"def ssl_prep(X_accept, y_accept, X_reject):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train_acc : training data of accepted population\\n    y_train_acc: training lables of accepted population\\n    X_train_rej: training data of rejected population\\n\\n    Return\\n    ------\\n    df : data of accepted and rejected population\\n\\n    \\\"\\\"\\\"\\n    # Merge explanatory and target in accepts\\n    accepts = pd.merge(\\n        X_accept, y_accept, how=\\\"left\\\", left_index=True, right_index=True\\n    )\\n    # Create accept flag\\n    accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    #     # Sample a matching number of observations from the rejects as the size of accepts\\n    #     ## Shuffle the dataset\\n    #     shuffle_df = X_reject.sample(frac=1)\\n    #     ## Define a size for the train set\\n    #     train_size = int(0.25 * len(shuffle_df))\\n    #     train_rejects = shuffle_df[:train_size]\\n\\n    train_rejects = X_reject\\n\\n    # Merge accepts and rejects\\n    df = pd.concat([accepts, train_rejects])\\n\\n    # If accepted use accept label, if rejected use -1 (default value for unlabelled entries) - hard-coded for now\\n    conditions = [\\n        (df[\\\"Flag1\\\"] == \\\"Accept\\\") & (df[\\\"target\\\"] == 1),\\n        (df[\\\"Flag1\\\"] == \\\"Accept\\\") & (df[\\\"target\\\"] == 0),\\n    ]\\n    choices = [1, 0]\\n\\n    # New target is called unlabel\\n    df[\\\"unlabel\\\"] = np.select(conditions, choices, -1)\\n\\n    # Select columns for modelling - hard-coded for now - can be moved outside of the function\\n    df = df[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\", \\\"unlabel\\\"]]\\n\\n    return df\";\n",
       "                var nbb_formatted_code = \"def ssl_prep(X_accept, y_accept, X_reject):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train_acc : training data of accepted population\\n    y_train_acc: training lables of accepted population\\n    X_train_rej: training data of rejected population\\n\\n    Return\\n    ------\\n    df : data of accepted and rejected population\\n\\n    \\\"\\\"\\\"\\n    # Merge explanatory and target in accepts\\n    accepts = pd.merge(\\n        X_accept, y_accept, how=\\\"left\\\", left_index=True, right_index=True\\n    )\\n    # Create accept flag\\n    accepts[\\\"Flag1\\\"] = \\\"Accept\\\"\\n\\n    #     # Sample a matching number of observations from the rejects as the size of accepts\\n    #     ## Shuffle the dataset\\n    #     shuffle_df = X_reject.sample(frac=1)\\n    #     ## Define a size for the train set\\n    #     train_size = int(0.25 * len(shuffle_df))\\n    #     train_rejects = shuffle_df[:train_size]\\n\\n    train_rejects = X_reject\\n\\n    # Merge accepts and rejects\\n    df = pd.concat([accepts, train_rejects])\\n\\n    # If accepted use accept label, if rejected use -1 (default value for unlabelled entries) - hard-coded for now\\n    conditions = [\\n        (df[\\\"Flag1\\\"] == \\\"Accept\\\") & (df[\\\"target\\\"] == 1),\\n        (df[\\\"Flag1\\\"] == \\\"Accept\\\") & (df[\\\"target\\\"] == 0),\\n    ]\\n    choices = [1, 0]\\n\\n    # New target is called unlabel\\n    df[\\\"unlabel\\\"] = np.select(conditions, choices, -1)\\n\\n    # Select columns for modelling - hard-coded for now - can be moved outside of the function\\n    df = df[[\\\"known_col_0\\\", \\\"known_col_1\\\", \\\"known_col_3\\\", \\\"known_col_4\\\", \\\"unlabel\\\"]]\\n\\n    return df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ssl_prep(X_accept, y_accept, X_reject):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X_train_acc : training data of accepted population\n",
    "    y_train_acc: training lables of accepted population\n",
    "    X_train_rej: training data of rejected population\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df : data of accepted and rejected population\n",
    "\n",
    "    \"\"\"\n",
    "    # Merge explanatory and target in accepts\n",
    "    accepts = pd.merge(\n",
    "        X_accept, y_accept, how=\"left\", left_index=True, right_index=True\n",
    "    )\n",
    "    # Create accept flag\n",
    "    accepts[\"Flag1\"] = \"Accept\"\n",
    "\n",
    "    #     # Sample a matching number of observations from the rejects as the size of accepts\n",
    "    #     ## Shuffle the dataset\n",
    "    #     shuffle_df = X_reject.sample(frac=1)\n",
    "    #     ## Define a size for the train set\n",
    "    #     train_size = int(0.25 * len(shuffle_df))\n",
    "    #     train_rejects = shuffle_df[:train_size]\n",
    "\n",
    "    train_rejects = X_reject\n",
    "\n",
    "    # Merge accepts and rejects\n",
    "    df = pd.concat([accepts, train_rejects])\n",
    "\n",
    "    # If accepted use accept label, if rejected use -1 (default value for unlabelled entries) - hard-coded for now\n",
    "    conditions = [\n",
    "        (df[\"Flag1\"] == \"Accept\") & (df[\"target\"] == 1),\n",
    "        (df[\"Flag1\"] == \"Accept\") & (df[\"target\"] == 0),\n",
    "    ]\n",
    "    choices = [1, 0]\n",
    "\n",
    "    # New target is called unlabel\n",
    "    df[\"unlabel\"] = np.select(conditions, choices, -1)\n",
    "\n",
    "    # Select columns for modelling - hard-coded for now - can be moved outside of the function\n",
    "    df = df[[\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\", \"unlabel\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 125;\n",
       "                var nbb_unformatted_code = \"def ssl_split(df, target):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    df : dataframe of accepted and rejected population, including data and labels\\n    target: string name of the target column, should be passed in quotation marks (e.g. \\\"target\\\")\\n\\n    Return\\n    ------\\n    X_train: training data of accepted and rejected population, ready to be fed into the semi-supervised model\\n    y_train: training labels of accepted and rejected population, ready to be fed into the semi-supervised model\\n\\n    \\\"\\\"\\\"\\n\\n    X_train = df.loc[:, df.columns != target]\\n    y_train = df.loc[:, df.columns == target]\\n    return X_train, y_train\";\n",
       "                var nbb_formatted_code = \"def ssl_split(df, target):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    df : dataframe of accepted and rejected population, including data and labels\\n    target: string name of the target column, should be passed in quotation marks (e.g. \\\"target\\\")\\n\\n    Return\\n    ------\\n    X_train: training data of accepted and rejected population, ready to be fed into the semi-supervised model\\n    y_train: training labels of accepted and rejected population, ready to be fed into the semi-supervised model\\n\\n    \\\"\\\"\\\"\\n\\n    X_train = df.loc[:, df.columns != target]\\n    y_train = df.loc[:, df.columns == target]\\n    return X_train, y_train\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 125;\n",
       "                var nbb_unformatted_code = \"def ssl_split(df, target):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    df : dataframe of accepted and rejected population, including data and labels\\n    target: string name of the target column, should be passed in quotation marks (e.g. \\\"target\\\")\\n\\n    Return\\n    ------\\n    X_train: training data of accepted and rejected population, ready to be fed into the semi-supervised model\\n    y_train: training labels of accepted and rejected population, ready to be fed into the semi-supervised model\\n\\n    \\\"\\\"\\\"\\n\\n    X_train = df.loc[:, df.columns != target]\\n    y_train = df.loc[:, df.columns == target]\\n    return X_train, y_train\";\n",
       "                var nbb_formatted_code = \"def ssl_split(df, target):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    df : dataframe of accepted and rejected population, including data and labels\\n    target: string name of the target column, should be passed in quotation marks (e.g. \\\"target\\\")\\n\\n    Return\\n    ------\\n    X_train: training data of accepted and rejected population, ready to be fed into the semi-supervised model\\n    y_train: training labels of accepted and rejected population, ready to be fed into the semi-supervised model\\n\\n    \\\"\\\"\\\"\\n\\n    X_train = df.loc[:, df.columns != target]\\n    y_train = df.loc[:, df.columns == target]\\n    return X_train, y_train\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ssl_split(df, target):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : dataframe of accepted and rejected population, including data and labels\n",
    "    target: string name of the target column, should be passed in quotation marks (e.g. \"target\")\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    X_train: training data of accepted and rejected population, ready to be fed into the semi-supervised model\n",
    "    y_train: training labels of accepted and rejected population, ready to be fed into the semi-supervised model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = df.loc[:, df.columns != target]\n",
    "    y_train = df.loc[:, df.columns == target]\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 126;\n",
       "                var nbb_unformatted_code = \"# def ssl_model_cotraining(X_train, y_train, model):\\n#     gb = GradientBoostingClassifier()\\n#     lr = LogisticRegression(fit_intercept=True, penalty=\\\"none\\\")\\n#     model = CTClassifier(gb, lr, random_state=42)\\n#     ctc = model.fit_predict([X_train.to_list(), X_train.to_list()], y_train.to_numpy)\\n#     return ctc\";\n",
       "                var nbb_formatted_code = \"# def ssl_model_cotraining(X_train, y_train, model):\\n#     gb = GradientBoostingClassifier()\\n#     lr = LogisticRegression(fit_intercept=True, penalty=\\\"none\\\")\\n#     model = CTClassifier(gb, lr, random_state=42)\\n#     ctc = model.fit_predict([X_train.to_list(), X_train.to_list()], y_train.to_numpy)\\n#     return ctc\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 126;\n",
       "                var nbb_unformatted_code = \"# def ssl_model_cotraining(X_train, y_train, model):\\n#     gb = GradientBoostingClassifier()\\n#     lr = LogisticRegression(fit_intercept=True, penalty=\\\"none\\\")\\n#     model = CTClassifier(gb, lr, random_state=42)\\n#     ctc = model.fit_predict([X_train.to_list(), X_train.to_list()], y_train.to_numpy)\\n#     return ctc\";\n",
       "                var nbb_formatted_code = \"# def ssl_model_cotraining(X_train, y_train, model):\\n#     gb = GradientBoostingClassifier()\\n#     lr = LogisticRegression(fit_intercept=True, penalty=\\\"none\\\")\\n#     model = CTClassifier(gb, lr, random_state=42)\\n#     ctc = model.fit_predict([X_train.to_list(), X_train.to_list()], y_train.to_numpy)\\n#     return ctc\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def ssl_model_cotraining(X_train, y_train, model):\n",
    "#     gb = GradientBoostingClassifier()\n",
    "#     lr = LogisticRegression(fit_intercept=True, penalty=\"none\")\n",
    "#     model = CTClassifier(gb, lr, random_state=42)\n",
    "#     ctc = model.fit_predict([X_train.to_list(), X_train.to_list()], y_train.to_numpy)\n",
    "#     return ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 127;\n",
       "                var nbb_unformatted_code = \"def ssl_model_selftraining(X_train, y_train, model):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train : training data of accepted and rejected population\\n    y_train : training lables of accepted population (0,1) and rejected population (-1)\\n    model : semi-supervised learning model from sklearn (Self-Training Classifier)\\n\\n    Return\\n    ------\\n\\n    ssl: trained semi-supervised learning model\\n\\n    \\\"\\\"\\\"\\n\\n    # Fit SSL moodel\\n    # base = RandomForestClassifier()\\n    base = LGBMClassifier()\\n    # base = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # base = LogisticRegression(fit_intercept=True, penalty=\\\"none\\\")\\n    model = model(base)\\n    labels = np.copy(y_train)\\n    data = np.copy(X_train)\\n    ssl = model.fit(data, labels)\\n    return ssl\";\n",
       "                var nbb_formatted_code = \"def ssl_model_selftraining(X_train, y_train, model):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train : training data of accepted and rejected population\\n    y_train : training lables of accepted population (0,1) and rejected population (-1)\\n    model : semi-supervised learning model from sklearn (Self-Training Classifier)\\n\\n    Return\\n    ------\\n\\n    ssl: trained semi-supervised learning model\\n\\n    \\\"\\\"\\\"\\n\\n    # Fit SSL moodel\\n    # base = RandomForestClassifier()\\n    base = LGBMClassifier()\\n    # base = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # base = LogisticRegression(fit_intercept=True, penalty=\\\"none\\\")\\n    model = model(base)\\n    labels = np.copy(y_train)\\n    data = np.copy(X_train)\\n    ssl = model.fit(data, labels)\\n    return ssl\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 127;\n",
       "                var nbb_unformatted_code = \"def ssl_model_selftraining(X_train, y_train, model):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train : training data of accepted and rejected population\\n    y_train : training lables of accepted population (0,1) and rejected population (-1)\\n    model : semi-supervised learning model from sklearn (Self-Training Classifier)\\n\\n    Return\\n    ------\\n\\n    ssl: trained semi-supervised learning model\\n\\n    \\\"\\\"\\\"\\n\\n    # Fit SSL moodel\\n    # base = RandomForestClassifier()\\n    base = LGBMClassifier()\\n    # base = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # base = LogisticRegression(fit_intercept=True, penalty=\\\"none\\\")\\n    model = model(base)\\n    labels = np.copy(y_train)\\n    data = np.copy(X_train)\\n    ssl = model.fit(data, labels)\\n    return ssl\";\n",
       "                var nbb_formatted_code = \"def ssl_model_selftraining(X_train, y_train, model):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train : training data of accepted and rejected population\\n    y_train : training lables of accepted population (0,1) and rejected population (-1)\\n    model : semi-supervised learning model from sklearn (Self-Training Classifier)\\n\\n    Return\\n    ------\\n\\n    ssl: trained semi-supervised learning model\\n\\n    \\\"\\\"\\\"\\n\\n    # Fit SSL moodel\\n    # base = RandomForestClassifier()\\n    base = LGBMClassifier()\\n    # base = GradientBoostingClassifier(criterion=\\\"mse\\\")\\n    # base = LogisticRegression(fit_intercept=True, penalty=\\\"none\\\")\\n    model = model(base)\\n    labels = np.copy(y_train)\\n    data = np.copy(X_train)\\n    ssl = model.fit(data, labels)\\n    return ssl\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ssl_model_selftraining(X_train, y_train, model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X_train : training data of accepted and rejected population\n",
    "    y_train : training lables of accepted population (0,1) and rejected population (-1)\n",
    "    model : semi-supervised learning model from sklearn (Self-Training Classifier)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    ssl: trained semi-supervised learning model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit SSL moodel\n",
    "    # base = RandomForestClassifier()\n",
    "    base = LGBMClassifier()\n",
    "    # base = GradientBoostingClassifier(criterion=\"mse\")\n",
    "    # base = LogisticRegression(fit_intercept=True, penalty=\"none\")\n",
    "    model = model(base)\n",
    "    labels = np.copy(y_train)\n",
    "    data = np.copy(X_train)\n",
    "    ssl = model.fit(data, labels)\n",
    "    return ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 128;\n",
       "                var nbb_unformatted_code = \"def ssl_model_label(X_train, y_train, model):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train : training data of accepted and rejected population\\n    y_train : training lables of accepted population (0,1) and rejected population (-1)\\n    model : semi-supervised learning model from sklearn (Label Propagation, Label Spreading)\\n\\n    Return\\n    ------\\n\\n    ssl: trained semi-supervised learning model\\n\\n    \\\"\\\"\\\"\\n\\n    # Fit SSL moodel\\n    model = model()\\n    labels = np.copy(y_train)\\n    data = np.copy(X_train)\\n    ssl = model.fit(data, labels)\\n    return ssl\";\n",
       "                var nbb_formatted_code = \"def ssl_model_label(X_train, y_train, model):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train : training data of accepted and rejected population\\n    y_train : training lables of accepted population (0,1) and rejected population (-1)\\n    model : semi-supervised learning model from sklearn (Label Propagation, Label Spreading)\\n\\n    Return\\n    ------\\n\\n    ssl: trained semi-supervised learning model\\n\\n    \\\"\\\"\\\"\\n\\n    # Fit SSL moodel\\n    model = model()\\n    labels = np.copy(y_train)\\n    data = np.copy(X_train)\\n    ssl = model.fit(data, labels)\\n    return ssl\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 128;\n",
       "                var nbb_unformatted_code = \"def ssl_model_label(X_train, y_train, model):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train : training data of accepted and rejected population\\n    y_train : training lables of accepted population (0,1) and rejected population (-1)\\n    model : semi-supervised learning model from sklearn (Label Propagation, Label Spreading)\\n\\n    Return\\n    ------\\n\\n    ssl: trained semi-supervised learning model\\n\\n    \\\"\\\"\\\"\\n\\n    # Fit SSL moodel\\n    model = model()\\n    labels = np.copy(y_train)\\n    data = np.copy(X_train)\\n    ssl = model.fit(data, labels)\\n    return ssl\";\n",
       "                var nbb_formatted_code = \"def ssl_model_label(X_train, y_train, model):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    X_train : training data of accepted and rejected population\\n    y_train : training lables of accepted population (0,1) and rejected population (-1)\\n    model : semi-supervised learning model from sklearn (Label Propagation, Label Spreading)\\n\\n    Return\\n    ------\\n\\n    ssl: trained semi-supervised learning model\\n\\n    \\\"\\\"\\\"\\n\\n    # Fit SSL moodel\\n    model = model()\\n    labels = np.copy(y_train)\\n    data = np.copy(X_train)\\n    ssl = model.fit(data, labels)\\n    return ssl\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ssl_model_label(X_train, y_train, model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X_train : training data of accepted and rejected population\n",
    "    y_train : training lables of accepted population (0,1) and rejected population (-1)\n",
    "    model : semi-supervised learning model from sklearn (Label Propagation, Label Spreading)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    ssl: trained semi-supervised learning model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit SSL moodel\n",
    "    model = model()\n",
    "    labels = np.copy(y_train)\n",
    "    data = np.copy(X_train)\n",
    "    ssl = model.fit(data, labels)\n",
    "    return ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 129;\n",
       "                var nbb_unformatted_code = \"def ssl_predictions_oth(ssl, X_test):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n    y_pred = ssl.predict_proba(X_test)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_ssl_cont\\\"],\\n        index=X_test.index.copy(),\\n    )\\n\\n    # Set quantile\\n    q = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\n    pred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n        lambda x: 0 if (x < q) else 1\\n    )\\n\\n    # Merge Baseline and SSL prediction\\n    pred_test_final2 = pd.merge(\\n        outcome_a[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n        pred_test[[\\\"prediction_ssl\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final2, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final2, \\\"target\\\", \\\"prediction_ssl\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final2, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final2, \\\"after RI\\\", \\\"prediction_ssl\\\")\\n    return pred_test_final2\";\n",
       "                var nbb_formatted_code = \"def ssl_predictions_oth(ssl, X_test):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n    y_pred = ssl.predict_proba(X_test)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_ssl_cont\\\"],\\n        index=X_test.index.copy(),\\n    )\\n\\n    # Set quantile\\n    q = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\n    pred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n        lambda x: 0 if (x < q) else 1\\n    )\\n\\n    # Merge Baseline and SSL prediction\\n    pred_test_final2 = pd.merge(\\n        outcome_a[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n        pred_test[[\\\"prediction_ssl\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final2, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final2, \\\"target\\\", \\\"prediction_ssl\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final2, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final2, \\\"after RI\\\", \\\"prediction_ssl\\\")\\n    return pred_test_final2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 129;\n",
       "                var nbb_unformatted_code = \"def ssl_predictions_oth(ssl, X_test):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n    y_pred = ssl.predict_proba(X_test)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_ssl_cont\\\"],\\n        index=X_test.index.copy(),\\n    )\\n\\n    # Set quantile\\n    q = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\n    pred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n        lambda x: 0 if (x < q) else 1\\n    )\\n\\n    # Merge Baseline and SSL prediction\\n    pred_test_final2 = pd.merge(\\n        outcome_a[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n        pred_test[[\\\"prediction_ssl\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final2, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final2, \\\"target\\\", \\\"prediction_ssl\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final2, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final2, \\\"after RI\\\", \\\"prediction_ssl\\\")\\n    return pred_test_final2\";\n",
       "                var nbb_formatted_code = \"def ssl_predictions_oth(ssl, X_test):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n    y_pred = ssl.predict_proba(X_test)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_ssl_cont\\\"],\\n        index=X_test.index.copy(),\\n    )\\n\\n    # Set quantile\\n    q = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\n    pred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n        lambda x: 0 if (x < q) else 1\\n    )\\n\\n    # Merge Baseline and SSL prediction\\n    pred_test_final2 = pd.merge(\\n        outcome_a[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n        pred_test[[\\\"prediction_ssl\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final2, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final2, \\\"target\\\", \\\"prediction_ssl\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final2, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final2, \\\"after RI\\\", \\\"prediction_ssl\\\")\\n    return pred_test_final2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ssl_predictions_oth(ssl, X_test):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ssl : trained semi-supervised learning model\n",
    "    X_test : testing data of accepted and rejected population for predictions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    Predictions before RI (binary)\n",
    "    Predictions after RI (binary)\n",
    "\n",
    "    \"\"\"\n",
    "    # Make Predictions\n",
    "    y_pred = ssl.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Convert y_pred array to pandas dataframe\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        columns=[\"prediction_ssl_cont\"],\n",
    "        index=X_test.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Set quantile\n",
    "    q = pred_test[\"prediction_ssl_cont\"].quantile(q=1 - conservative_dr)\n",
    "    pred_test[\"prediction_ssl\"] = pred_test[\"prediction_ssl_cont\"].apply(\n",
    "        lambda x: 0 if (x < q) else 1\n",
    "    )\n",
    "\n",
    "    # Merge Baseline and SSL prediction\n",
    "    pred_test_final2 = pd.merge(\n",
    "        outcome_a[[\"target\", \"prediction_baseline\", \"prediction_beforeRI_binary\"]],\n",
    "        pred_test[[\"prediction_ssl\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    #     # Log Loss\n",
    "    #     log_loss_fun(\"Before\", pred_test_final2, \"target\", \"prediction_beforeRI_binary\")\n",
    "    #     log_loss_fun(\"After\", pred_test_final2, \"target\", \"prediction_ssl\")\n",
    "\n",
    "    #     # Numbers of accurately classified and misclassified cases\n",
    "    #     print_results(pred_test_final2, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    #     print_results(pred_test_final2, \"after RI\", \"prediction_ssl\")\n",
    "    return pred_test_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 130;\n",
       "                var nbb_unformatted_code = \"def ssl_predictions_oth_rej(ssl, dfr_test_with_label_X):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n\\n    y_pred = ssl.predict_proba(dfr_test_with_label_X)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_ssl_cont\\\"],\\n        index=dfr_test_with_label_X.index.copy(),\\n    )\\n\\n    # Set quantile\\n    q = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\n    pred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n        lambda x: 0 if (x < q) else 1\\n    )\\n\\n    # Merge Baseline and SSL prediction\\n    pred_test_final2 = pd.merge(\\n        outcome_b[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n        pred_test[[\\\"prediction_ssl\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final2, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final2, \\\"target\\\", \\\"prediction_ssl\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final2, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final2, \\\"after RI\\\", \\\"prediction_ssl\\\")\\n    return pred_test_final2\";\n",
       "                var nbb_formatted_code = \"def ssl_predictions_oth_rej(ssl, dfr_test_with_label_X):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n\\n    y_pred = ssl.predict_proba(dfr_test_with_label_X)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_ssl_cont\\\"],\\n        index=dfr_test_with_label_X.index.copy(),\\n    )\\n\\n    # Set quantile\\n    q = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\n    pred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n        lambda x: 0 if (x < q) else 1\\n    )\\n\\n    # Merge Baseline and SSL prediction\\n    pred_test_final2 = pd.merge(\\n        outcome_b[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n        pred_test[[\\\"prediction_ssl\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final2, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final2, \\\"target\\\", \\\"prediction_ssl\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final2, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final2, \\\"after RI\\\", \\\"prediction_ssl\\\")\\n    return pred_test_final2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 130;\n",
       "                var nbb_unformatted_code = \"def ssl_predictions_oth_rej(ssl, dfr_test_with_label_X):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n\\n    y_pred = ssl.predict_proba(dfr_test_with_label_X)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_ssl_cont\\\"],\\n        index=dfr_test_with_label_X.index.copy(),\\n    )\\n\\n    # Set quantile\\n    q = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\n    pred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n        lambda x: 0 if (x < q) else 1\\n    )\\n\\n    # Merge Baseline and SSL prediction\\n    pred_test_final2 = pd.merge(\\n        outcome_b[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n        pred_test[[\\\"prediction_ssl\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final2, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final2, \\\"target\\\", \\\"prediction_ssl\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final2, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final2, \\\"after RI\\\", \\\"prediction_ssl\\\")\\n    return pred_test_final2\";\n",
       "                var nbb_formatted_code = \"def ssl_predictions_oth_rej(ssl, dfr_test_with_label_X):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n\\n    y_pred = ssl.predict_proba(dfr_test_with_label_X)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_ssl_cont\\\"],\\n        index=dfr_test_with_label_X.index.copy(),\\n    )\\n\\n    # Set quantile\\n    q = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\n    pred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n        lambda x: 0 if (x < q) else 1\\n    )\\n\\n    # Merge Baseline and SSL prediction\\n    pred_test_final2 = pd.merge(\\n        outcome_b[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n        pred_test[[\\\"prediction_ssl\\\"]],\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    #     # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", pred_test_final2, \\\"target\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     log_loss_fun(\\\"After\\\", pred_test_final2, \\\"target\\\", \\\"prediction_ssl\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(pred_test_final2, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(pred_test_final2, \\\"after RI\\\", \\\"prediction_ssl\\\")\\n    return pred_test_final2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ssl_predictions_oth_rej(ssl, dfr_test_with_label_X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ssl : trained semi-supervised learning model\n",
    "    X_test : testing data of accepted and rejected population for predictions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    Predictions before RI (binary)\n",
    "    Predictions after RI (binary)\n",
    "\n",
    "    \"\"\"\n",
    "    # Make Predictions\n",
    "\n",
    "    y_pred = ssl.predict_proba(dfr_test_with_label_X)[:, 1]\n",
    "\n",
    "    # Convert y_pred array to pandas dataframe\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        columns=[\"prediction_ssl_cont\"],\n",
    "        index=dfr_test_with_label_X.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Set quantile\n",
    "    q = pred_test[\"prediction_ssl_cont\"].quantile(q=1 - conservative_dr)\n",
    "    pred_test[\"prediction_ssl\"] = pred_test[\"prediction_ssl_cont\"].apply(\n",
    "        lambda x: 0 if (x < q) else 1\n",
    "    )\n",
    "\n",
    "    # Merge Baseline and SSL prediction\n",
    "    pred_test_final2 = pd.merge(\n",
    "        outcome_b[[\"target\", \"prediction_baseline\", \"prediction_beforeRI_binary\"]],\n",
    "        pred_test[[\"prediction_ssl\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    #     # Log Loss\n",
    "    #     log_loss_fun(\"Before\", pred_test_final2, \"target\", \"prediction_beforeRI_binary\")\n",
    "    #     log_loss_fun(\"After\", pred_test_final2, \"target\", \"prediction_ssl\")\n",
    "\n",
    "    #     # Numbers of accurately classified and misclassified cases\n",
    "    #     print_results(pred_test_final2, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    #     print_results(pred_test_final2, \"after RI\", \"prediction_ssl\")\n",
    "    return pred_test_final2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kickout Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 131;\n",
       "                var nbb_unformatted_code = \"def flag_df(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_ssl\\\"] == 1\\n    ):\\n        return \\\"KB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_ssl\\\"] == 1\\n    ):\\n        return \\\"KG\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_ssl\\\"] == 0\\n    ):\\n        return \\\"IG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_ssl\\\"] == 0\\n    ):\\n        return \\\"IB\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_ssl\\\"] == 1\\n    ):\\n        return \\\"KB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_ssl\\\"] == 1\\n    ):\\n        return \\\"KG\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_ssl\\\"] == 0\\n    ):\\n        return \\\"IG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_ssl\\\"] == 0\\n    ):\\n        return \\\"IB\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 131;\n",
       "                var nbb_unformatted_code = \"def flag_df(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_ssl\\\"] == 1\\n    ):\\n        return \\\"KB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_ssl\\\"] == 1\\n    ):\\n        return \\\"KG\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_ssl\\\"] == 0\\n    ):\\n        return \\\"IG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_ssl\\\"] == 0\\n    ):\\n        return \\\"IB\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_ssl\\\"] == 1\\n    ):\\n        return \\\"KB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 0\\n        and df[\\\"prediction_ssl\\\"] == 1\\n    ):\\n        return \\\"KG\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif (\\n        df[\\\"target\\\"] == 0\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_ssl\\\"] == 0\\n    ):\\n        return \\\"IG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif (\\n        df[\\\"target\\\"] == 1\\n        and df[\\\"prediction_beforeRI_binary\\\"] == 1\\n        and df[\\\"prediction_ssl\\\"] == 0\\n    ):\\n        return \\\"IB\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flag_df(df):\n",
    "\n",
    "    # Flag kicked out bad cases (want more of these)\n",
    "    if (\n",
    "        df[\"target\"] == 1\n",
    "        and df[\"prediction_beforeRI_binary\"] == 0\n",
    "        and df[\"prediction_ssl\"] == 1\n",
    "    ):\n",
    "        return \"KB\"\n",
    "\n",
    "    # Flag kicked out good cases (want less of these)\n",
    "    elif (\n",
    "        df[\"target\"] == 0\n",
    "        and df[\"prediction_beforeRI_binary\"] == 0\n",
    "        and df[\"prediction_ssl\"] == 1\n",
    "    ):\n",
    "        return \"KG\"\n",
    "\n",
    "    # Flag kicked in good cases (want more of these)\n",
    "    elif (\n",
    "        df[\"target\"] == 0\n",
    "        and df[\"prediction_beforeRI_binary\"] == 1\n",
    "        and df[\"prediction_ssl\"] == 0\n",
    "    ):\n",
    "        return \"IG\"\n",
    "\n",
    "    # Flag kicked in bad cases (want less of these)\n",
    "    elif (\n",
    "        df[\"target\"] == 1\n",
    "        and df[\"prediction_beforeRI_binary\"] == 1\n",
    "        and df[\"prediction_ssl\"] == 0\n",
    "    ):\n",
    "        return \"IB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 132;\n",
       "                var nbb_unformatted_code = \"def flag_df_ssl(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_ssl\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_ssl\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_ssl\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_ssl\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df_ssl(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_ssl\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_ssl\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_ssl\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_ssl\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 132;\n",
       "                var nbb_unformatted_code = \"def flag_df_ssl(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_ssl\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_ssl\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_ssl\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_ssl\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_formatted_code = \"def flag_df_ssl(df):\\n\\n    # Flag kicked out bad cases (want more of these)\\n    if df[\\\"target\\\"] == 1 and df[\\\"prediction_ssl\\\"] == 1:\\n        return \\\"CB\\\"\\n\\n    # Flag kicked out good cases (want less of these)\\n    elif df[\\\"target\\\"] == 1 and df[\\\"prediction_ssl\\\"] == 0:\\n        return \\\"IB\\\"\\n\\n    # Flag kicked in good cases (want more of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_ssl\\\"] == 0:\\n        return \\\"CG\\\"\\n\\n    # Flag kicked in bad cases (want less of these)\\n    elif df[\\\"target\\\"] == 0 and df[\\\"prediction_ssl\\\"] == 1:\\n        return \\\"IG\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flag_df_ssl(df):\n",
    "\n",
    "    # Flag kicked out bad cases (want more of these)\n",
    "    if df[\"target\"] == 1 and df[\"prediction_ssl\"] == 1:\n",
    "        return \"CB\"\n",
    "\n",
    "    # Flag kicked out good cases (want less of these)\n",
    "    elif df[\"target\"] == 1 and df[\"prediction_ssl\"] == 0:\n",
    "        return \"IB\"\n",
    "\n",
    "    # Flag kicked in good cases (want more of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_ssl\"] == 0:\n",
    "        return \"CG\"\n",
    "\n",
    "    # Flag kicked in bad cases (want less of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_ssl\"] == 1:\n",
    "        return \"IG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 133;\n",
       "                var nbb_unformatted_code = \"# def kickout_ssl(df):\\n\\n#     # Counts of kickout bad and kickout good\\n#     counts = df[\\\"Flag\\\"].value_counts()\\n#     if \\\"KB\\\" in df.values:\\n#         kb = counts.KB  # want more of these\\n#     else:\\n#         kb = 0\\n#     if \\\"KG\\\" in df.values:\\n#         kg = counts.KG  # want less of these\\n#     else:\\n#         kg = 0\\n\\n#     if \\\"IG\\\" in df.values:\\n#         ig = counts.IG  # want more of these\\n#     else:\\n#         ig = 0\\n\\n#     if \\\"IB\\\" in df.values:\\n#         ib = counts.IB  # want less of these\\n#     else:\\n#         ib = 0\\n\\n#     # Counts of number of actual bad cases\\n#     sb = df[df[\\\"target\\\"] == 1].shape[0]\\n#     sg = df[df[\\\"target\\\"] == 0].shape[0]\\n\\n#     # Target\\n#     counts_target = df[\\\"target\\\"].value_counts()\\n#     total_bads = counts_target[0]\\n#     total_goods = counts_target[1]\\n\\n#     total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n#     total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n#     pb = total_bads / (total_bads + total_goods)\\n#     pg = total_goods / (total_bads + total_goods)\\n\\n#     # Calculate kickout metric\\n#     kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * 0.5)\\n#     kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg ** 0.5)\\n#     weighted_total = kickout + kickin\\n\\n#     return print(\\n#         \\\"Kickout + Kickin Metric:\\\",\\n#         round(weighted_total, 3),\\n#     )\";\n",
       "                var nbb_formatted_code = \"# def kickout_ssl(df):\\n\\n#     # Counts of kickout bad and kickout good\\n#     counts = df[\\\"Flag\\\"].value_counts()\\n#     if \\\"KB\\\" in df.values:\\n#         kb = counts.KB  # want more of these\\n#     else:\\n#         kb = 0\\n#     if \\\"KG\\\" in df.values:\\n#         kg = counts.KG  # want less of these\\n#     else:\\n#         kg = 0\\n\\n#     if \\\"IG\\\" in df.values:\\n#         ig = counts.IG  # want more of these\\n#     else:\\n#         ig = 0\\n\\n#     if \\\"IB\\\" in df.values:\\n#         ib = counts.IB  # want less of these\\n#     else:\\n#         ib = 0\\n\\n#     # Counts of number of actual bad cases\\n#     sb = df[df[\\\"target\\\"] == 1].shape[0]\\n#     sg = df[df[\\\"target\\\"] == 0].shape[0]\\n\\n#     # Target\\n#     counts_target = df[\\\"target\\\"].value_counts()\\n#     total_bads = counts_target[0]\\n#     total_goods = counts_target[1]\\n\\n#     total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n#     total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n#     pb = total_bads / (total_bads + total_goods)\\n#     pg = total_goods / (total_bads + total_goods)\\n\\n#     # Calculate kickout metric\\n#     kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * 0.5)\\n#     kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg ** 0.5)\\n#     weighted_total = kickout + kickin\\n\\n#     return print(\\n#         \\\"Kickout + Kickin Metric:\\\",\\n#         round(weighted_total, 3),\\n#     )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 133;\n",
       "                var nbb_unformatted_code = \"# def kickout_ssl(df):\\n\\n#     # Counts of kickout bad and kickout good\\n#     counts = df[\\\"Flag\\\"].value_counts()\\n#     if \\\"KB\\\" in df.values:\\n#         kb = counts.KB  # want more of these\\n#     else:\\n#         kb = 0\\n#     if \\\"KG\\\" in df.values:\\n#         kg = counts.KG  # want less of these\\n#     else:\\n#         kg = 0\\n\\n#     if \\\"IG\\\" in df.values:\\n#         ig = counts.IG  # want more of these\\n#     else:\\n#         ig = 0\\n\\n#     if \\\"IB\\\" in df.values:\\n#         ib = counts.IB  # want less of these\\n#     else:\\n#         ib = 0\\n\\n#     # Counts of number of actual bad cases\\n#     sb = df[df[\\\"target\\\"] == 1].shape[0]\\n#     sg = df[df[\\\"target\\\"] == 0].shape[0]\\n\\n#     # Target\\n#     counts_target = df[\\\"target\\\"].value_counts()\\n#     total_bads = counts_target[0]\\n#     total_goods = counts_target[1]\\n\\n#     total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n#     total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n#     pb = total_bads / (total_bads + total_goods)\\n#     pg = total_goods / (total_bads + total_goods)\\n\\n#     # Calculate kickout metric\\n#     kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * 0.5)\\n#     kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg ** 0.5)\\n#     weighted_total = kickout + kickin\\n\\n#     return print(\\n#         \\\"Kickout + Kickin Metric:\\\",\\n#         round(weighted_total, 3),\\n#     )\";\n",
       "                var nbb_formatted_code = \"# def kickout_ssl(df):\\n\\n#     # Counts of kickout bad and kickout good\\n#     counts = df[\\\"Flag\\\"].value_counts()\\n#     if \\\"KB\\\" in df.values:\\n#         kb = counts.KB  # want more of these\\n#     else:\\n#         kb = 0\\n#     if \\\"KG\\\" in df.values:\\n#         kg = counts.KG  # want less of these\\n#     else:\\n#         kg = 0\\n\\n#     if \\\"IG\\\" in df.values:\\n#         ig = counts.IG  # want more of these\\n#     else:\\n#         ig = 0\\n\\n#     if \\\"IB\\\" in df.values:\\n#         ib = counts.IB  # want less of these\\n#     else:\\n#         ib = 0\\n\\n#     # Counts of number of actual bad cases\\n#     sb = df[df[\\\"target\\\"] == 1].shape[0]\\n#     sg = df[df[\\\"target\\\"] == 0].shape[0]\\n\\n#     # Target\\n#     counts_target = df[\\\"target\\\"].value_counts()\\n#     total_bads = counts_target[0]\\n#     total_goods = counts_target[1]\\n\\n#     total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n#     total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n#     pb = total_bads / (total_bads + total_goods)\\n#     pg = total_goods / (total_bads + total_goods)\\n\\n#     # Calculate kickout metric\\n#     kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * 0.5)\\n#     kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg ** 0.5)\\n#     weighted_total = kickout + kickin\\n\\n#     return print(\\n#         \\\"Kickout + Kickin Metric:\\\",\\n#         round(weighted_total, 3),\\n#     )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def kickout_ssl(df):\n",
    "\n",
    "#     # Counts of kickout bad and kickout good\n",
    "#     counts = df[\"Flag\"].value_counts()\n",
    "#     if \"KB\" in df.values:\n",
    "#         kb = counts.KB  # want more of these\n",
    "#     else:\n",
    "#         kb = 0\n",
    "#     if \"KG\" in df.values:\n",
    "#         kg = counts.KG  # want less of these\n",
    "#     else:\n",
    "#         kg = 0\n",
    "\n",
    "#     if \"IG\" in df.values:\n",
    "#         ig = counts.IG  # want more of these\n",
    "#     else:\n",
    "#         ig = 0\n",
    "\n",
    "#     if \"IB\" in df.values:\n",
    "#         ib = counts.IB  # want less of these\n",
    "#     else:\n",
    "#         ib = 0\n",
    "\n",
    "#     # Counts of number of actual bad cases\n",
    "#     sb = df[df[\"target\"] == 1].shape[0]\n",
    "#     sg = df[df[\"target\"] == 0].shape[0]\n",
    "\n",
    "#     # Target\n",
    "#     counts_target = df[\"target\"].value_counts()\n",
    "#     total_bads = counts_target[0]\n",
    "#     total_goods = counts_target[1]\n",
    "\n",
    "#     total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "#     total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "#     pb = total_bads / (total_bads + total_goods)\n",
    "#     pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "#     # Calculate kickout metric\n",
    "#     kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * 0.5)\n",
    "#     kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg ** 0.5)\n",
    "#     weighted_total = kickout + kickin\n",
    "\n",
    "#     return print(\n",
    "#         \"Kickout + Kickin Metric:\",\n",
    "#         round(weighted_total, 3),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 134;\n",
       "                var nbb_unformatted_code = \"def kickout_ssl(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_formatted_code = \"def kickout_ssl(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 134;\n",
       "                var nbb_unformatted_code = \"def kickout_ssl(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_formatted_code = \"def kickout_ssl(df):\\n\\n    # Counts of kickout bad and kickout good\\n    counts = df[\\\"Flag\\\"].value_counts()\\n    if \\\"CB\\\" in df.values:\\n        cb = counts.CB  # want more of these\\n    else:\\n        cb = 0\\n    if \\\"IB\\\" in df.values:\\n        ib = counts.IB  # want less of these\\n    else:\\n        ib = 0\\n\\n    if \\\"CG\\\" in df.values:\\n        cg = counts.CG  # want more of these\\n    else:\\n        cg = 0\\n\\n    if \\\"IG\\\" in df.values:\\n        ig = counts.IG  # want less of these\\n    else:\\n        ig = 0\\n\\n    # Target\\n    total_bads = df[df[\\\"target\\\"] == 1].shape[0]\\n    total_goods = df[df[\\\"target\\\"] == 0].shape[0]\\n    pb = total_bads / (total_bads + total_goods)\\n    pg = total_goods / (total_bads + total_goods)\\n\\n    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\\n    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\\n    weighted_total = kickout + kickin\\n    return print(\\n        \\\"Kickout + Kickin Metric:\\\",\\n        round(weighted_total, 3),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def kickout_ssl(df):\n",
    "\n",
    "    # Counts of kickout bad and kickout good\n",
    "    counts = df[\"Flag\"].value_counts()\n",
    "    if \"CB\" in df.values:\n",
    "        cb = counts.CB  # want more of these\n",
    "    else:\n",
    "        cb = 0\n",
    "    if \"IB\" in df.values:\n",
    "        ib = counts.IB  # want less of these\n",
    "    else:\n",
    "        ib = 0\n",
    "\n",
    "    if \"CG\" in df.values:\n",
    "        cg = counts.CG  # want more of these\n",
    "    else:\n",
    "        cg = 0\n",
    "\n",
    "    if \"IG\" in df.values:\n",
    "        ig = counts.IG  # want less of these\n",
    "    else:\n",
    "        ig = 0\n",
    "\n",
    "    # Target\n",
    "    total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "    total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "    pb = total_bads / (total_bads + total_goods)\n",
    "    pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\n",
    "    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\n",
    "    weighted_total = kickout + kickin\n",
    "    return print(\n",
    "        \"Kickout + Kickin Metric:\",\n",
    "        round(weighted_total, 3),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSL Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 135;\n",
       "                var nbb_unformatted_code = \"# Train\\ntrain_new_model = ssl_prep(\\n    X_train,\\n    y_train,\\n    r_dev_mod,\\n)\\n# Test\\ntest_new_model = ssl_prep(\\n    X_test_3,\\n    y_test,\\n    r_test_mod,\\n)\\nX_ssl, y_ssl = ssl_split(train_new_model, \\\"unlabel\\\")\";\n",
       "                var nbb_formatted_code = \"# Train\\ntrain_new_model = ssl_prep(\\n    X_train,\\n    y_train,\\n    r_dev_mod,\\n)\\n# Test\\ntest_new_model = ssl_prep(\\n    X_test_3,\\n    y_test,\\n    r_test_mod,\\n)\\nX_ssl, y_ssl = ssl_split(train_new_model, \\\"unlabel\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 135;\n",
       "                var nbb_unformatted_code = \"# Train\\ntrain_new_model = ssl_prep(\\n    X_train,\\n    y_train,\\n    r_dev_mod,\\n)\\n# Test\\ntest_new_model = ssl_prep(\\n    X_test_3,\\n    y_test,\\n    r_test_mod,\\n)\\nX_ssl, y_ssl = ssl_split(train_new_model, \\\"unlabel\\\")\";\n",
       "                var nbb_formatted_code = \"# Train\\ntrain_new_model = ssl_prep(\\n    X_train,\\n    y_train,\\n    r_dev_mod,\\n)\\n# Test\\ntest_new_model = ssl_prep(\\n    X_test_3,\\n    y_test,\\n    r_test_mod,\\n)\\nX_ssl, y_ssl = ssl_split(train_new_model, \\\"unlabel\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "train_new_model = ssl_prep(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    r_dev_mod,\n",
    ")\n",
    "# Test\n",
    "test_new_model = ssl_prep(\n",
    "    X_test_3,\n",
    "    y_test,\n",
    "    r_test_mod,\n",
    ")\n",
    "X_ssl, y_ssl = ssl_split(train_new_model, \"unlabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSL Models & Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Accepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 136;\n",
       "                var nbb_unformatted_code = \"self_training = ssl_model_selftraining(X_ssl, y_ssl, SelfTrainingClassifier)\\nx = ssl_predictions_oth(self_training, X_test_3)\";\n",
       "                var nbb_formatted_code = \"self_training = ssl_model_selftraining(X_ssl, y_ssl, SelfTrainingClassifier)\\nx = ssl_predictions_oth(self_training, X_test_3)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 136;\n",
       "                var nbb_unformatted_code = \"self_training = ssl_model_selftraining(X_ssl, y_ssl, SelfTrainingClassifier)\\nx = ssl_predictions_oth(self_training, X_test_3)\";\n",
       "                var nbb_formatted_code = \"self_training = ssl_model_selftraining(X_ssl, y_ssl, SelfTrainingClassifier)\\nx = ssl_predictions_oth(self_training, X_test_3)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "self_training = ssl_model_selftraining(X_ssl, y_ssl, SelfTrainingClassifier)\n",
    "x = ssl_predictions_oth(self_training, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kickout + Kickin Metric: 0.829\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 137;\n",
       "                var nbb_unformatted_code = \"# x[\\\"Flag\\\"] = x.apply(flag_df, axis=1)\\n# kickout(x)\\nx[\\\"Flag\\\"] = x.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x)\";\n",
       "                var nbb_formatted_code = \"# x[\\\"Flag\\\"] = x.apply(flag_df, axis=1)\\n# kickout(x)\\nx[\\\"Flag\\\"] = x.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 137;\n",
       "                var nbb_unformatted_code = \"# x[\\\"Flag\\\"] = x.apply(flag_df, axis=1)\\n# kickout(x)\\nx[\\\"Flag\\\"] = x.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x)\";\n",
       "                var nbb_formatted_code = \"# x[\\\"Flag\\\"] = x.apply(flag_df, axis=1)\\n# kickout(x)\\nx[\\\"Flag\\\"] = x.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x[\"Flag\"] = x.apply(flag_df, axis=1)\n",
    "# kickout(x)\n",
    "x[\"Flag\"] = x.apply(flag_df_ssl, axis=1)\n",
    "kickout_ssl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_spreading = ssl_model_label(X_ssl, y_ssl, LabelSpreading)\n",
    "x_ls = ssl_predictions_oth(label_spreading, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_ls[\"Flag\"] = x_ls.apply(flag_df, axis=1)\n",
    "# kickout(x_ls)\n",
    "x_ls[\"Flag\"] = x_ls.apply(flag_df_ssl, axis=1)\n",
    "kickout_ssl(x_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_propagation = ssl_model_label(X_ssl, y_ssl, LabelPropagation)\n",
    "x_lp = ssl_predictions_oth(label_propagation, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_lp[\"Flag\"] = x_lp.apply(flag_df_new, axis=1)\n",
    "# kickout_new(x_lp_new)\n",
    "x_lp[\"Flag\"] = x_lp.apply(flag_df_ssl, axis=1)\n",
    "kickout_ssl(x_lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Rejects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kickout + Kickin Metric: 0.735\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 138;\n",
       "                var nbb_unformatted_code = \"x_rej_st = ssl_predictions_oth_rej(self_training, dfr_test_with_label_X)\\n# x_rej_st[\\\"Flag\\\"] = x_rej_st.apply(flag_df, axis=1)\\n# kickout(x_rej_st)\\nx_rej_st[\\\"Flag\\\"] = x_rej_st.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x_rej_st)\";\n",
       "                var nbb_formatted_code = \"x_rej_st = ssl_predictions_oth_rej(self_training, dfr_test_with_label_X)\\n# x_rej_st[\\\"Flag\\\"] = x_rej_st.apply(flag_df, axis=1)\\n# kickout(x_rej_st)\\nx_rej_st[\\\"Flag\\\"] = x_rej_st.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x_rej_st)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 138;\n",
       "                var nbb_unformatted_code = \"x_rej_st = ssl_predictions_oth_rej(self_training, dfr_test_with_label_X)\\n# x_rej_st[\\\"Flag\\\"] = x_rej_st.apply(flag_df, axis=1)\\n# kickout(x_rej_st)\\nx_rej_st[\\\"Flag\\\"] = x_rej_st.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x_rej_st)\";\n",
       "                var nbb_formatted_code = \"x_rej_st = ssl_predictions_oth_rej(self_training, dfr_test_with_label_X)\\n# x_rej_st[\\\"Flag\\\"] = x_rej_st.apply(flag_df, axis=1)\\n# kickout(x_rej_st)\\nx_rej_st[\\\"Flag\\\"] = x_rej_st.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x_rej_st)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_rej_st = ssl_predictions_oth_rej(self_training, dfr_test_with_label_X)\n",
    "# x_rej_st[\"Flag\"] = x_rej_st.apply(flag_df, axis=1)\n",
    "# kickout(x_rej_st)\n",
    "x_rej_st[\"Flag\"] = x_rej_st.apply(flag_df_ssl, axis=1)\n",
    "kickout_ssl(x_rej_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_spreading' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-9de830ec082f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_rej_ls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mssl_predictions_oth_rej\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_spreading\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfr_test_with_label_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# x_rej_ls[\"Flag\"] = x_rej_ls.apply(flag_df, axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# kickout(x_rej_ls)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_rej_ls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Flag\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_rej_ls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag_df_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mkickout_ssl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_rej_ls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_spreading' is not defined"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 139;\n",
       "                var nbb_unformatted_code = \"x_rej_ls = ssl_predictions_oth_rej(label_spreading, dfr_test_with_label_X)\\n# x_rej_ls[\\\"Flag\\\"] = x_rej_ls.apply(flag_df, axis=1)\\n# kickout(x_rej_ls)\\nx_rej_ls[\\\"Flag\\\"] = x_rej_ls.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x_rej_ls)\";\n",
       "                var nbb_formatted_code = \"x_rej_ls = ssl_predictions_oth_rej(label_spreading, dfr_test_with_label_X)\\n# x_rej_ls[\\\"Flag\\\"] = x_rej_ls.apply(flag_df, axis=1)\\n# kickout(x_rej_ls)\\nx_rej_ls[\\\"Flag\\\"] = x_rej_ls.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x_rej_ls)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 139;\n",
       "                var nbb_unformatted_code = \"x_rej_ls = ssl_predictions_oth_rej(label_spreading, dfr_test_with_label_X)\\n# x_rej_ls[\\\"Flag\\\"] = x_rej_ls.apply(flag_df, axis=1)\\n# kickout(x_rej_ls)\\nx_rej_ls[\\\"Flag\\\"] = x_rej_ls.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x_rej_ls)\";\n",
       "                var nbb_formatted_code = \"x_rej_ls = ssl_predictions_oth_rej(label_spreading, dfr_test_with_label_X)\\n# x_rej_ls[\\\"Flag\\\"] = x_rej_ls.apply(flag_df, axis=1)\\n# kickout(x_rej_ls)\\nx_rej_ls[\\\"Flag\\\"] = x_rej_ls.apply(flag_df_ssl, axis=1)\\nkickout_ssl(x_rej_ls)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_rej_ls = ssl_predictions_oth_rej(label_spreading, dfr_test_with_label_X)\n",
    "# x_rej_ls[\"Flag\"] = x_rej_ls.apply(flag_df, axis=1)\n",
    "# kickout(x_rej_ls)\n",
    "x_rej_ls[\"Flag\"] = x_rej_ls.apply(flag_df_ssl, axis=1)\n",
    "kickout_ssl(x_rej_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rej_lp = ssl_predictions_oth_rej(label_propagation, dfr_test_with_label_X)\n",
    "# x_rej_lp[\"Flag\"] = x_rej_st.apply(flag_df, axis=1)\n",
    "# kickout(x_rej_lp)\n",
    "x_rej_lp[\"Flag\"] = x_rej_st.apply(flag_df_ssl, axis=1)\n",
    "kickout_ssl(x_rej_lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 162;\n",
       "                var nbb_unformatted_code = \"def regression_std(regressor, X):\\n    _, std = regressor.predict(X, return_std=True)\\n    return np.argmax(std)\";\n",
       "                var nbb_formatted_code = \"def regression_std(regressor, X):\\n    _, std = regressor.predict(X, return_std=True)\\n    return np.argmax(std)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 162;\n",
       "                var nbb_unformatted_code = \"def regression_std(regressor, X):\\n    _, std = regressor.predict(X, return_std=True)\\n    return np.argmax(std)\";\n",
       "                var nbb_formatted_code = \"def regression_std(regressor, X):\\n    _, std = regressor.predict(X, return_std=True)\\n    return np.argmax(std)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def regression_std(regressor, X):\n",
    "    _, std = regressor.predict(X, return_std=True)\n",
    "    return np.argmax(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 163;\n",
       "                var nbb_unformatted_code = \"def active_learning(X_train, y_train, r_dev_mod):\\n    n_initial = len(X_train)\\n    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\\n    X_training, y_training = X_train.iloc[initial_idx], y_train.iloc[initial_idx]\\n\\n    learner = ActiveLearner(\\n        estimator=LGBMClassifier(),\\n        query_strategy=uncertainty_sampling,\\n        X_training=X_training,\\n        y_training=y_training,\\n    )\\n    query_idx, query_inst = learner.query(r_dev_mod)\\n    # active learning\\n    n_queries = int(0.4 * len(r_dev_mod))\\n    for idx in range(n_queries):\\n        query_idx, query_instance = learner.query(X_train)\\n        learner.teach(X_train.iloc[query_idx], y_train.iloc[query_idx])\\n    return learner\";\n",
       "                var nbb_formatted_code = \"def active_learning(X_train, y_train, r_dev_mod):\\n    n_initial = len(X_train)\\n    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\\n    X_training, y_training = X_train.iloc[initial_idx], y_train.iloc[initial_idx]\\n\\n    learner = ActiveLearner(\\n        estimator=LGBMClassifier(),\\n        query_strategy=uncertainty_sampling,\\n        X_training=X_training,\\n        y_training=y_training,\\n    )\\n    query_idx, query_inst = learner.query(r_dev_mod)\\n    # active learning\\n    n_queries = int(0.4 * len(r_dev_mod))\\n    for idx in range(n_queries):\\n        query_idx, query_instance = learner.query(X_train)\\n        learner.teach(X_train.iloc[query_idx], y_train.iloc[query_idx])\\n    return learner\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 163;\n",
       "                var nbb_unformatted_code = \"def active_learning(X_train, y_train, r_dev_mod):\\n    n_initial = len(X_train)\\n    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\\n    X_training, y_training = X_train.iloc[initial_idx], y_train.iloc[initial_idx]\\n\\n    learner = ActiveLearner(\\n        estimator=LGBMClassifier(),\\n        query_strategy=uncertainty_sampling,\\n        X_training=X_training,\\n        y_training=y_training,\\n    )\\n    query_idx, query_inst = learner.query(r_dev_mod)\\n    # active learning\\n    n_queries = int(0.4 * len(r_dev_mod))\\n    for idx in range(n_queries):\\n        query_idx, query_instance = learner.query(X_train)\\n        learner.teach(X_train.iloc[query_idx], y_train.iloc[query_idx])\\n    return learner\";\n",
       "                var nbb_formatted_code = \"def active_learning(X_train, y_train, r_dev_mod):\\n    n_initial = len(X_train)\\n    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\\n    X_training, y_training = X_train.iloc[initial_idx], y_train.iloc[initial_idx]\\n\\n    learner = ActiveLearner(\\n        estimator=LGBMClassifier(),\\n        query_strategy=uncertainty_sampling,\\n        X_training=X_training,\\n        y_training=y_training,\\n    )\\n    query_idx, query_inst = learner.query(r_dev_mod)\\n    # active learning\\n    n_queries = int(0.4 * len(r_dev_mod))\\n    for idx in range(n_queries):\\n        query_idx, query_instance = learner.query(X_train)\\n        learner.teach(X_train.iloc[query_idx], y_train.iloc[query_idx])\\n    return learner\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def active_learning(X_train, y_train, r_dev_mod):\n",
    "    n_initial = len(X_train)\n",
    "    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n",
    "    X_training, y_training = X_train.iloc[initial_idx], y_train.iloc[initial_idx]\n",
    "\n",
    "    learner = ActiveLearner(\n",
    "        estimator=LGBMClassifier(),\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training=X_training,\n",
    "        y_training=y_training,\n",
    "    )\n",
    "    query_idx, query_inst = learner.query(r_dev_mod)\n",
    "    # active learning\n",
    "    n_queries = int(0.4 * len(r_dev_mod))\n",
    "    for idx in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_train)\n",
    "        learner.teach(X_train.iloc[query_idx], y_train.iloc[query_idx])\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 164;\n",
       "                var nbb_unformatted_code = \"def ssl_predictions_oth2(ssl, X_test):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n\\n    y_pred = ssl.predict_proba(X_test)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_afterRI\\\"],\\n        index=X_test.index.copy(),\\n    )\\n    a1 = pred_test1[[\\\"id\\\", \\\"target\\\", \\\"prediction_beforeRI\\\"]]  # hard-coded for now\\n    a2 = pred_test[[\\\"prediction_afterRI\\\"]]  # hard-coded for now\\n\\n    # Merge a1 and a2\\n    a1_a2_inner = pd.merge(\\n        a1,\\n        a2,\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff 75percentile of the distribution\\n    q1 = a1_a2_inner[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = a1_a2_inner[\\\"prediction_afterRI\\\"].quantile(q=1 - conservative_dr)\\n\\n    a1_a2_inner[\\\"prediction_beforeRI_binary\\\"] = a1_a2_inner[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    a1_a2_inner[\\\"prediction_afterRI_binary\\\"] = a1_a2_inner[\\\"prediction_afterRI\\\"].apply(\\n        lambda x: 0 if (x < q2) else 1\\n    )\\n\\n    # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", a1_a2_inner, \\\"target\\\", \\\"prediction_beforeRI\\\")\\n    #     log_loss_fun(\\\"After\\\", a1_a2_inner, \\\"target\\\", \\\"prediction_afterRI\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(a1_a2_inner, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(a1_a2_inner, \\\"after RI\\\", \\\"prediction_afterRI_binary\\\")\\n    return a1_a2_inner\";\n",
       "                var nbb_formatted_code = \"def ssl_predictions_oth2(ssl, X_test):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n\\n    y_pred = ssl.predict_proba(X_test)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_afterRI\\\"],\\n        index=X_test.index.copy(),\\n    )\\n    a1 = pred_test1[[\\\"id\\\", \\\"target\\\", \\\"prediction_beforeRI\\\"]]  # hard-coded for now\\n    a2 = pred_test[[\\\"prediction_afterRI\\\"]]  # hard-coded for now\\n\\n    # Merge a1 and a2\\n    a1_a2_inner = pd.merge(\\n        a1,\\n        a2,\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff 75percentile of the distribution\\n    q1 = a1_a2_inner[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = a1_a2_inner[\\\"prediction_afterRI\\\"].quantile(q=1 - conservative_dr)\\n\\n    a1_a2_inner[\\\"prediction_beforeRI_binary\\\"] = a1_a2_inner[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    a1_a2_inner[\\\"prediction_afterRI_binary\\\"] = a1_a2_inner[\\\"prediction_afterRI\\\"].apply(\\n        lambda x: 0 if (x < q2) else 1\\n    )\\n\\n    # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", a1_a2_inner, \\\"target\\\", \\\"prediction_beforeRI\\\")\\n    #     log_loss_fun(\\\"After\\\", a1_a2_inner, \\\"target\\\", \\\"prediction_afterRI\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(a1_a2_inner, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(a1_a2_inner, \\\"after RI\\\", \\\"prediction_afterRI_binary\\\")\\n    return a1_a2_inner\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 164;\n",
       "                var nbb_unformatted_code = \"def ssl_predictions_oth2(ssl, X_test):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n\\n    y_pred = ssl.predict_proba(X_test)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_afterRI\\\"],\\n        index=X_test.index.copy(),\\n    )\\n    a1 = pred_test1[[\\\"id\\\", \\\"target\\\", \\\"prediction_beforeRI\\\"]]  # hard-coded for now\\n    a2 = pred_test[[\\\"prediction_afterRI\\\"]]  # hard-coded for now\\n\\n    # Merge a1 and a2\\n    a1_a2_inner = pd.merge(\\n        a1,\\n        a2,\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff 75percentile of the distribution\\n    q1 = a1_a2_inner[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = a1_a2_inner[\\\"prediction_afterRI\\\"].quantile(q=1 - conservative_dr)\\n\\n    a1_a2_inner[\\\"prediction_beforeRI_binary\\\"] = a1_a2_inner[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    a1_a2_inner[\\\"prediction_afterRI_binary\\\"] = a1_a2_inner[\\\"prediction_afterRI\\\"].apply(\\n        lambda x: 0 if (x < q2) else 1\\n    )\\n\\n    # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", a1_a2_inner, \\\"target\\\", \\\"prediction_beforeRI\\\")\\n    #     log_loss_fun(\\\"After\\\", a1_a2_inner, \\\"target\\\", \\\"prediction_afterRI\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(a1_a2_inner, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(a1_a2_inner, \\\"after RI\\\", \\\"prediction_afterRI_binary\\\")\\n    return a1_a2_inner\";\n",
       "                var nbb_formatted_code = \"def ssl_predictions_oth2(ssl, X_test):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n\\n    ssl : trained semi-supervised learning model\\n    X_test : testing data of accepted and rejected population for predictions\\n\\n    Return\\n    ------\\n\\n    Predictions before RI (binary)\\n    Predictions after RI (binary)\\n\\n    \\\"\\\"\\\"\\n    # Make Predictions\\n\\n    y_pred = ssl.predict_proba(X_test)[:, 1]\\n\\n    # Convert y_pred array to pandas dataframe\\n    pred_test = pd.DataFrame(\\n        data=y_pred,\\n        columns=[\\\"prediction_afterRI\\\"],\\n        index=X_test.index.copy(),\\n    )\\n    a1 = pred_test1[[\\\"id\\\", \\\"target\\\", \\\"prediction_beforeRI\\\"]]  # hard-coded for now\\n    a2 = pred_test[[\\\"prediction_afterRI\\\"]]  # hard-coded for now\\n\\n    # Merge a1 and a2\\n    a1_a2_inner = pd.merge(\\n        a1,\\n        a2,\\n        how=\\\"inner\\\",\\n        left_index=True,\\n        right_index=True,\\n    )\\n\\n    # Make binary predictions based on cutoff 75percentile of the distribution\\n    q1 = a1_a2_inner[\\\"prediction_beforeRI\\\"].quantile(q=1 - conservative_dr)\\n    q2 = a1_a2_inner[\\\"prediction_afterRI\\\"].quantile(q=1 - conservative_dr)\\n\\n    a1_a2_inner[\\\"prediction_beforeRI_binary\\\"] = a1_a2_inner[\\n        \\\"prediction_beforeRI\\\"\\n    ].apply(lambda x: 0 if (x < q1) else 1)\\n    a1_a2_inner[\\\"prediction_afterRI_binary\\\"] = a1_a2_inner[\\\"prediction_afterRI\\\"].apply(\\n        lambda x: 0 if (x < q2) else 1\\n    )\\n\\n    # Log Loss\\n    #     log_loss_fun(\\\"Before\\\", a1_a2_inner, \\\"target\\\", \\\"prediction_beforeRI\\\")\\n    #     log_loss_fun(\\\"After\\\", a1_a2_inner, \\\"target\\\", \\\"prediction_afterRI\\\")\\n\\n    #     # Numbers of accurately classified and misclassified cases\\n    #     print_results(a1_a2_inner, \\\"before RI\\\", \\\"prediction_beforeRI_binary\\\")\\n    #     print_results(a1_a2_inner, \\\"after RI\\\", \\\"prediction_afterRI_binary\\\")\\n    return a1_a2_inner\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ssl_predictions_oth2(ssl, X_test):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ssl : trained semi-supervised learning model\n",
    "    X_test : testing data of accepted and rejected population for predictions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    Predictions before RI (binary)\n",
    "    Predictions after RI (binary)\n",
    "\n",
    "    \"\"\"\n",
    "    # Make Predictions\n",
    "\n",
    "    y_pred = ssl.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Convert y_pred array to pandas dataframe\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        columns=[\"prediction_afterRI\"],\n",
    "        index=X_test.index.copy(),\n",
    "    )\n",
    "    a1 = pred_test1[[\"id\", \"target\", \"prediction_beforeRI\"]]  # hard-coded for now\n",
    "    a2 = pred_test[[\"prediction_afterRI\"]]  # hard-coded for now\n",
    "\n",
    "    # Merge a1 and a2\n",
    "    a1_a2_inner = pd.merge(\n",
    "        a1,\n",
    "        a2,\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff 75percentile of the distribution\n",
    "    q1 = a1_a2_inner[\"prediction_beforeRI\"].quantile(q=1 - conservative_dr)\n",
    "    q2 = a1_a2_inner[\"prediction_afterRI\"].quantile(q=1 - conservative_dr)\n",
    "\n",
    "    a1_a2_inner[\"prediction_beforeRI_binary\"] = a1_a2_inner[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(lambda x: 0 if (x < q1) else 1)\n",
    "    a1_a2_inner[\"prediction_afterRI_binary\"] = a1_a2_inner[\"prediction_afterRI\"].apply(\n",
    "        lambda x: 0 if (x < q2) else 1\n",
    "    )\n",
    "\n",
    "    # Log Loss\n",
    "    #     log_loss_fun(\"Before\", a1_a2_inner, \"target\", \"prediction_beforeRI\")\n",
    "    #     log_loss_fun(\"After\", a1_a2_inner, \"target\", \"prediction_afterRI\")\n",
    "\n",
    "    #     # Numbers of accurately classified and misclassified cases\n",
    "    #     print_results(a1_a2_inner, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    #     print_results(a1_a2_inner, \"after RI\", \"prediction_afterRI_binary\")\n",
    "    return a1_a2_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 189;\n",
       "                var nbb_unformatted_code = \"regressor_20 = active_learning(X_train, y_train, r_dev_mod)\\nx_al_20 = ssl_predictions_oth(regressor_20, X_test_3)\";\n",
       "                var nbb_formatted_code = \"regressor_20 = active_learning(X_train, y_train, r_dev_mod)\\nx_al_20 = ssl_predictions_oth(regressor_20, X_test_3)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 189;\n",
       "                var nbb_unformatted_code = \"regressor_20 = active_learning(X_train, y_train, r_dev_mod)\\nx_al_20 = ssl_predictions_oth(regressor_20, X_test_3)\";\n",
       "                var nbb_formatted_code = \"regressor_20 = active_learning(X_train, y_train, r_dev_mod)\\nx_al_20 = ssl_predictions_oth(regressor_20, X_test_3)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor_20 = active_learning(X_train, y_train, r_dev_mod)\n",
    "x_al_20 = ssl_predictions_oth(regressor_20, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kickout + Kickin Metric: 0.826\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 190;\n",
       "                var nbb_unformatted_code = \"x_al_20[\\\"Flag\\\"] = x_al_20.apply(flag_df_ssl, axis=1)\\nkickout_al_20 = kickout_ssl(x_al_20)\";\n",
       "                var nbb_formatted_code = \"x_al_20[\\\"Flag\\\"] = x_al_20.apply(flag_df_ssl, axis=1)\\nkickout_al_20 = kickout_ssl(x_al_20)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 190;\n",
       "                var nbb_unformatted_code = \"x_al_20[\\\"Flag\\\"] = x_al_20.apply(flag_df_ssl, axis=1)\\nkickout_al_20 = kickout_ssl(x_al_20)\";\n",
       "                var nbb_formatted_code = \"x_al_20[\\\"Flag\\\"] = x_al_20.apply(flag_df_ssl, axis=1)\\nkickout_al_20 = kickout_ssl(x_al_20)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_al_20[\"Flag\"] = x_al_20.apply(flag_df_ssl, axis=1)\n",
    "kickout_al_20 = kickout_ssl(x_al_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 167;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nfrom scipy import stats\\nfrom scipy.special import logsumexp\\nfrom sklearn.mixture import GaussianMixture\\nfrom matplotlib import pyplot as plt\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nfrom scipy import stats\\nfrom scipy.special import logsumexp\\nfrom sklearn.mixture import GaussianMixture\\nfrom matplotlib import pyplot as plt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 167;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nfrom scipy import stats\\nfrom scipy.special import logsumexp\\nfrom sklearn.mixture import GaussianMixture\\nfrom matplotlib import pyplot as plt\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nfrom scipy import stats\\nfrom scipy.special import logsumexp\\nfrom sklearn.mixture import GaussianMixture\\nfrom matplotlib import pyplot as plt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 168;\n",
       "                var nbb_unformatted_code = \"def get_avg_log_likelihood(x, params):\\n    loglikelihood, _ = e_step(x, params)\\n    return np.mean(loglikelihood)\\ndef learn_params(x_labeled, y_labeled):\\n    n = x_labeled.shape[0]\\n    phi = x_labeled[y_labeled == 1].shape[0] / n\\n    mu0 = np.sum(x_labeled[y_labeled == 0], axis=0) / x_labeled[y_labeled == 0].shape[0]\\n    mu1 = np.sum(x_labeled[y_labeled == 1], axis=0) / x_labeled[y_labeled == 1].shape[0]\\n    sigma0 = np.cov(x_labeled[y_labeled == 0].T, bias=True)\\n    sigma1 = np.cov(x_labeled[y_labeled == 1].T, bias=True)\\n    return {\\\"phi\\\": phi, \\\"mu0\\\": mu0, \\\"mu1\\\": mu1, \\\"sigma0\\\": sigma0, \\\"sigma1\\\": sigma1}\\n\\ndef e_step(x, params):\\n    np.log(\\n        [\\n            stats.multivariate_normal(params[\\\"mu0\\\"], params[\\\"sigma0\\\"]).pdf(x),\\n            stats.multivariate_normal(params[\\\"mu1\\\"], params[\\\"sigma1\\\"]).pdf(x),\\n        ]\\n    )\\n    log_p_y_x = (\\n        np.log([1 - params[\\\"phi\\\"], params[\\\"phi\\\"]])[np.newaxis, ...]\\n        + np.log(\\n            [\\n                stats.multivariate_normal(params[\\\"mu0\\\"], params[\\\"sigma0\\\"]).pdf(x),\\n                stats.multivariate_normal(params[\\\"mu1\\\"], params[\\\"sigma1\\\"]).pdf(x),\\n            ]\\n        ).T\\n    )\\n    log_p_y_x_norm = logsumexp(log_p_y_x, axis=1)\\n    return log_p_y_x_norm, np.exp(log_p_y_x - log_p_y_x_norm[..., np.newaxis])\\n\\n\\ndef m_step(x, params):\\n    total_count = x.shape[0]\\n    _, heuristics = e_step(x, params)\\n    heuristic0 = heuristics[:, 0]\\n    heuristic1 = heuristics[:, 1]\\n    sum_heuristic1 = np.sum(heuristic1)\\n    sum_heuristic0 = np.sum(heuristic0)\\n    phi = sum_heuristic1 / total_count\\n    mu0 = (heuristic0[..., np.newaxis].T.dot(x) / sum_heuristic0).flatten()\\n    mu1 = (heuristic1[..., np.newaxis].T.dot(x) / sum_heuristic1).flatten()\\n    diff0 = x - mu0\\n    sigma0 = diff0.T.dot(diff0 * heuristic0[..., np.newaxis]) / sum_heuristic0\\n    diff1 = x - mu1\\n    sigma1 = diff1.T.dot(diff1 * heuristic1[..., np.newaxis]) / sum_heuristic1\\n    params = {\\\"phi\\\": phi, \\\"mu0\\\": mu0, \\\"mu1\\\": mu1, \\\"sigma0\\\": sigma0, \\\"sigma1\\\": sigma1}\\n    return params\\n\\ndef run_em(x, params):\\n    avg_loglikelihoods = []\\n    while True:\\n        avg_loglikelihood = get_avg_log_likelihood(x, params)\\n        avg_loglikelihoods.append(avg_loglikelihood)\\n        if (\\n            len(avg_loglikelihoods) > 2\\n            and abs(avg_loglikelihoods[-1] - avg_loglikelihoods[-2]) < 0.0001\\n        ):\\n            break\\n        params = m_step(x_unlabeled, params)\\n    print(\\n        \\\"\\\\tphi: %s\\\\n\\\\tmu_0: %s\\\\n\\\\tmu_1: %s\\\\n\\\\tsigma_0: %s\\\\n\\\\tsigma_1: %s\\\"\\n        % (\\n            params[\\\"phi\\\"],\\n            params[\\\"mu0\\\"],\\n            params[\\\"mu1\\\"],\\n            params[\\\"sigma0\\\"],\\n            params[\\\"sigma1\\\"],\\n        )\\n    )\\n    _, posterior = e_step(x_unlabeled, params)\\n    forecasts = np.argmax(posterior, axis=1)\\n    return forecasts, posterior, avg_loglikelihoods\";\n",
       "                var nbb_formatted_code = \"def get_avg_log_likelihood(x, params):\\n    loglikelihood, _ = e_step(x, params)\\n    return np.mean(loglikelihood)\\n\\n\\ndef learn_params(x_labeled, y_labeled):\\n    n = x_labeled.shape[0]\\n    phi = x_labeled[y_labeled == 1].shape[0] / n\\n    mu0 = np.sum(x_labeled[y_labeled == 0], axis=0) / x_labeled[y_labeled == 0].shape[0]\\n    mu1 = np.sum(x_labeled[y_labeled == 1], axis=0) / x_labeled[y_labeled == 1].shape[0]\\n    sigma0 = np.cov(x_labeled[y_labeled == 0].T, bias=True)\\n    sigma1 = np.cov(x_labeled[y_labeled == 1].T, bias=True)\\n    return {\\\"phi\\\": phi, \\\"mu0\\\": mu0, \\\"mu1\\\": mu1, \\\"sigma0\\\": sigma0, \\\"sigma1\\\": sigma1}\\n\\n\\ndef e_step(x, params):\\n    np.log(\\n        [\\n            stats.multivariate_normal(params[\\\"mu0\\\"], params[\\\"sigma0\\\"]).pdf(x),\\n            stats.multivariate_normal(params[\\\"mu1\\\"], params[\\\"sigma1\\\"]).pdf(x),\\n        ]\\n    )\\n    log_p_y_x = (\\n        np.log([1 - params[\\\"phi\\\"], params[\\\"phi\\\"]])[np.newaxis, ...]\\n        + np.log(\\n            [\\n                stats.multivariate_normal(params[\\\"mu0\\\"], params[\\\"sigma0\\\"]).pdf(x),\\n                stats.multivariate_normal(params[\\\"mu1\\\"], params[\\\"sigma1\\\"]).pdf(x),\\n            ]\\n        ).T\\n    )\\n    log_p_y_x_norm = logsumexp(log_p_y_x, axis=1)\\n    return log_p_y_x_norm, np.exp(log_p_y_x - log_p_y_x_norm[..., np.newaxis])\\n\\n\\ndef m_step(x, params):\\n    total_count = x.shape[0]\\n    _, heuristics = e_step(x, params)\\n    heuristic0 = heuristics[:, 0]\\n    heuristic1 = heuristics[:, 1]\\n    sum_heuristic1 = np.sum(heuristic1)\\n    sum_heuristic0 = np.sum(heuristic0)\\n    phi = sum_heuristic1 / total_count\\n    mu0 = (heuristic0[..., np.newaxis].T.dot(x) / sum_heuristic0).flatten()\\n    mu1 = (heuristic1[..., np.newaxis].T.dot(x) / sum_heuristic1).flatten()\\n    diff0 = x - mu0\\n    sigma0 = diff0.T.dot(diff0 * heuristic0[..., np.newaxis]) / sum_heuristic0\\n    diff1 = x - mu1\\n    sigma1 = diff1.T.dot(diff1 * heuristic1[..., np.newaxis]) / sum_heuristic1\\n    params = {\\\"phi\\\": phi, \\\"mu0\\\": mu0, \\\"mu1\\\": mu1, \\\"sigma0\\\": sigma0, \\\"sigma1\\\": sigma1}\\n    return params\\n\\n\\ndef run_em(x, params):\\n    avg_loglikelihoods = []\\n    while True:\\n        avg_loglikelihood = get_avg_log_likelihood(x, params)\\n        avg_loglikelihoods.append(avg_loglikelihood)\\n        if (\\n            len(avg_loglikelihoods) > 2\\n            and abs(avg_loglikelihoods[-1] - avg_loglikelihoods[-2]) < 0.0001\\n        ):\\n            break\\n        params = m_step(x_unlabeled, params)\\n    print(\\n        \\\"\\\\tphi: %s\\\\n\\\\tmu_0: %s\\\\n\\\\tmu_1: %s\\\\n\\\\tsigma_0: %s\\\\n\\\\tsigma_1: %s\\\"\\n        % (\\n            params[\\\"phi\\\"],\\n            params[\\\"mu0\\\"],\\n            params[\\\"mu1\\\"],\\n            params[\\\"sigma0\\\"],\\n            params[\\\"sigma1\\\"],\\n        )\\n    )\\n    _, posterior = e_step(x_unlabeled, params)\\n    forecasts = np.argmax(posterior, axis=1)\\n    return forecasts, posterior, avg_loglikelihoods\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 168;\n",
       "                var nbb_unformatted_code = \"def get_avg_log_likelihood(x, params):\\n    loglikelihood, _ = e_step(x, params)\\n    return np.mean(loglikelihood)\\ndef learn_params(x_labeled, y_labeled):\\n    n = x_labeled.shape[0]\\n    phi = x_labeled[y_labeled == 1].shape[0] / n\\n    mu0 = np.sum(x_labeled[y_labeled == 0], axis=0) / x_labeled[y_labeled == 0].shape[0]\\n    mu1 = np.sum(x_labeled[y_labeled == 1], axis=0) / x_labeled[y_labeled == 1].shape[0]\\n    sigma0 = np.cov(x_labeled[y_labeled == 0].T, bias=True)\\n    sigma1 = np.cov(x_labeled[y_labeled == 1].T, bias=True)\\n    return {\\\"phi\\\": phi, \\\"mu0\\\": mu0, \\\"mu1\\\": mu1, \\\"sigma0\\\": sigma0, \\\"sigma1\\\": sigma1}\\n\\ndef e_step(x, params):\\n    np.log(\\n        [\\n            stats.multivariate_normal(params[\\\"mu0\\\"], params[\\\"sigma0\\\"]).pdf(x),\\n            stats.multivariate_normal(params[\\\"mu1\\\"], params[\\\"sigma1\\\"]).pdf(x),\\n        ]\\n    )\\n    log_p_y_x = (\\n        np.log([1 - params[\\\"phi\\\"], params[\\\"phi\\\"]])[np.newaxis, ...]\\n        + np.log(\\n            [\\n                stats.multivariate_normal(params[\\\"mu0\\\"], params[\\\"sigma0\\\"]).pdf(x),\\n                stats.multivariate_normal(params[\\\"mu1\\\"], params[\\\"sigma1\\\"]).pdf(x),\\n            ]\\n        ).T\\n    )\\n    log_p_y_x_norm = logsumexp(log_p_y_x, axis=1)\\n    return log_p_y_x_norm, np.exp(log_p_y_x - log_p_y_x_norm[..., np.newaxis])\\n\\n\\ndef m_step(x, params):\\n    total_count = x.shape[0]\\n    _, heuristics = e_step(x, params)\\n    heuristic0 = heuristics[:, 0]\\n    heuristic1 = heuristics[:, 1]\\n    sum_heuristic1 = np.sum(heuristic1)\\n    sum_heuristic0 = np.sum(heuristic0)\\n    phi = sum_heuristic1 / total_count\\n    mu0 = (heuristic0[..., np.newaxis].T.dot(x) / sum_heuristic0).flatten()\\n    mu1 = (heuristic1[..., np.newaxis].T.dot(x) / sum_heuristic1).flatten()\\n    diff0 = x - mu0\\n    sigma0 = diff0.T.dot(diff0 * heuristic0[..., np.newaxis]) / sum_heuristic0\\n    diff1 = x - mu1\\n    sigma1 = diff1.T.dot(diff1 * heuristic1[..., np.newaxis]) / sum_heuristic1\\n    params = {\\\"phi\\\": phi, \\\"mu0\\\": mu0, \\\"mu1\\\": mu1, \\\"sigma0\\\": sigma0, \\\"sigma1\\\": sigma1}\\n    return params\\n\\ndef run_em(x, params):\\n    avg_loglikelihoods = []\\n    while True:\\n        avg_loglikelihood = get_avg_log_likelihood(x, params)\\n        avg_loglikelihoods.append(avg_loglikelihood)\\n        if (\\n            len(avg_loglikelihoods) > 2\\n            and abs(avg_loglikelihoods[-1] - avg_loglikelihoods[-2]) < 0.0001\\n        ):\\n            break\\n        params = m_step(x_unlabeled, params)\\n    print(\\n        \\\"\\\\tphi: %s\\\\n\\\\tmu_0: %s\\\\n\\\\tmu_1: %s\\\\n\\\\tsigma_0: %s\\\\n\\\\tsigma_1: %s\\\"\\n        % (\\n            params[\\\"phi\\\"],\\n            params[\\\"mu0\\\"],\\n            params[\\\"mu1\\\"],\\n            params[\\\"sigma0\\\"],\\n            params[\\\"sigma1\\\"],\\n        )\\n    )\\n    _, posterior = e_step(x_unlabeled, params)\\n    forecasts = np.argmax(posterior, axis=1)\\n    return forecasts, posterior, avg_loglikelihoods\";\n",
       "                var nbb_formatted_code = \"def get_avg_log_likelihood(x, params):\\n    loglikelihood, _ = e_step(x, params)\\n    return np.mean(loglikelihood)\\n\\n\\ndef learn_params(x_labeled, y_labeled):\\n    n = x_labeled.shape[0]\\n    phi = x_labeled[y_labeled == 1].shape[0] / n\\n    mu0 = np.sum(x_labeled[y_labeled == 0], axis=0) / x_labeled[y_labeled == 0].shape[0]\\n    mu1 = np.sum(x_labeled[y_labeled == 1], axis=0) / x_labeled[y_labeled == 1].shape[0]\\n    sigma0 = np.cov(x_labeled[y_labeled == 0].T, bias=True)\\n    sigma1 = np.cov(x_labeled[y_labeled == 1].T, bias=True)\\n    return {\\\"phi\\\": phi, \\\"mu0\\\": mu0, \\\"mu1\\\": mu1, \\\"sigma0\\\": sigma0, \\\"sigma1\\\": sigma1}\\n\\n\\ndef e_step(x, params):\\n    np.log(\\n        [\\n            stats.multivariate_normal(params[\\\"mu0\\\"], params[\\\"sigma0\\\"]).pdf(x),\\n            stats.multivariate_normal(params[\\\"mu1\\\"], params[\\\"sigma1\\\"]).pdf(x),\\n        ]\\n    )\\n    log_p_y_x = (\\n        np.log([1 - params[\\\"phi\\\"], params[\\\"phi\\\"]])[np.newaxis, ...]\\n        + np.log(\\n            [\\n                stats.multivariate_normal(params[\\\"mu0\\\"], params[\\\"sigma0\\\"]).pdf(x),\\n                stats.multivariate_normal(params[\\\"mu1\\\"], params[\\\"sigma1\\\"]).pdf(x),\\n            ]\\n        ).T\\n    )\\n    log_p_y_x_norm = logsumexp(log_p_y_x, axis=1)\\n    return log_p_y_x_norm, np.exp(log_p_y_x - log_p_y_x_norm[..., np.newaxis])\\n\\n\\ndef m_step(x, params):\\n    total_count = x.shape[0]\\n    _, heuristics = e_step(x, params)\\n    heuristic0 = heuristics[:, 0]\\n    heuristic1 = heuristics[:, 1]\\n    sum_heuristic1 = np.sum(heuristic1)\\n    sum_heuristic0 = np.sum(heuristic0)\\n    phi = sum_heuristic1 / total_count\\n    mu0 = (heuristic0[..., np.newaxis].T.dot(x) / sum_heuristic0).flatten()\\n    mu1 = (heuristic1[..., np.newaxis].T.dot(x) / sum_heuristic1).flatten()\\n    diff0 = x - mu0\\n    sigma0 = diff0.T.dot(diff0 * heuristic0[..., np.newaxis]) / sum_heuristic0\\n    diff1 = x - mu1\\n    sigma1 = diff1.T.dot(diff1 * heuristic1[..., np.newaxis]) / sum_heuristic1\\n    params = {\\\"phi\\\": phi, \\\"mu0\\\": mu0, \\\"mu1\\\": mu1, \\\"sigma0\\\": sigma0, \\\"sigma1\\\": sigma1}\\n    return params\\n\\n\\ndef run_em(x, params):\\n    avg_loglikelihoods = []\\n    while True:\\n        avg_loglikelihood = get_avg_log_likelihood(x, params)\\n        avg_loglikelihoods.append(avg_loglikelihood)\\n        if (\\n            len(avg_loglikelihoods) > 2\\n            and abs(avg_loglikelihoods[-1] - avg_loglikelihoods[-2]) < 0.0001\\n        ):\\n            break\\n        params = m_step(x_unlabeled, params)\\n    print(\\n        \\\"\\\\tphi: %s\\\\n\\\\tmu_0: %s\\\\n\\\\tmu_1: %s\\\\n\\\\tsigma_0: %s\\\\n\\\\tsigma_1: %s\\\"\\n        % (\\n            params[\\\"phi\\\"],\\n            params[\\\"mu0\\\"],\\n            params[\\\"mu1\\\"],\\n            params[\\\"sigma0\\\"],\\n            params[\\\"sigma1\\\"],\\n        )\\n    )\\n    _, posterior = e_step(x_unlabeled, params)\\n    forecasts = np.argmax(posterior, axis=1)\\n    return forecasts, posterior, avg_loglikelihoods\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_avg_log_likelihood(x, params):\n",
    "    loglikelihood, _ = e_step(x, params)\n",
    "    return np.mean(loglikelihood)\n",
    "def learn_params(x_labeled, y_labeled):\n",
    "    n = x_labeled.shape[0]\n",
    "    phi = x_labeled[y_labeled == 1].shape[0] / n\n",
    "    mu0 = np.sum(x_labeled[y_labeled == 0], axis=0) / x_labeled[y_labeled == 0].shape[0]\n",
    "    mu1 = np.sum(x_labeled[y_labeled == 1], axis=0) / x_labeled[y_labeled == 1].shape[0]\n",
    "    sigma0 = np.cov(x_labeled[y_labeled == 0].T, bias=True)\n",
    "    sigma1 = np.cov(x_labeled[y_labeled == 1].T, bias=True)\n",
    "    return {\"phi\": phi, \"mu0\": mu0, \"mu1\": mu1, \"sigma0\": sigma0, \"sigma1\": sigma1}\n",
    "\n",
    "def e_step(x, params):\n",
    "    np.log(\n",
    "        [\n",
    "            stats.multivariate_normal(params[\"mu0\"], params[\"sigma0\"]).pdf(x),\n",
    "            stats.multivariate_normal(params[\"mu1\"], params[\"sigma1\"]).pdf(x),\n",
    "        ]\n",
    "    )\n",
    "    log_p_y_x = (\n",
    "        np.log([1 - params[\"phi\"], params[\"phi\"]])[np.newaxis, ...]\n",
    "        + np.log(\n",
    "            [\n",
    "                stats.multivariate_normal(params[\"mu0\"], params[\"sigma0\"]).pdf(x),\n",
    "                stats.multivariate_normal(params[\"mu1\"], params[\"sigma1\"]).pdf(x),\n",
    "            ]\n",
    "        ).T\n",
    "    )\n",
    "    log_p_y_x_norm = logsumexp(log_p_y_x, axis=1)\n",
    "    return log_p_y_x_norm, np.exp(log_p_y_x - log_p_y_x_norm[..., np.newaxis])\n",
    "\n",
    "\n",
    "def m_step(x, params):\n",
    "    total_count = x.shape[0]\n",
    "    _, heuristics = e_step(x, params)\n",
    "    heuristic0 = heuristics[:, 0]\n",
    "    heuristic1 = heuristics[:, 1]\n",
    "    sum_heuristic1 = np.sum(heuristic1)\n",
    "    sum_heuristic0 = np.sum(heuristic0)\n",
    "    phi = sum_heuristic1 / total_count\n",
    "    mu0 = (heuristic0[..., np.newaxis].T.dot(x) / sum_heuristic0).flatten()\n",
    "    mu1 = (heuristic1[..., np.newaxis].T.dot(x) / sum_heuristic1).flatten()\n",
    "    diff0 = x - mu0\n",
    "    sigma0 = diff0.T.dot(diff0 * heuristic0[..., np.newaxis]) / sum_heuristic0\n",
    "    diff1 = x - mu1\n",
    "    sigma1 = diff1.T.dot(diff1 * heuristic1[..., np.newaxis]) / sum_heuristic1\n",
    "    params = {\"phi\": phi, \"mu0\": mu0, \"mu1\": mu1, \"sigma0\": sigma0, \"sigma1\": sigma1}\n",
    "    return params\n",
    "\n",
    "def run_em(x, params):\n",
    "    avg_loglikelihoods = []\n",
    "    while True:\n",
    "        avg_loglikelihood = get_avg_log_likelihood(x, params)\n",
    "        avg_loglikelihoods.append(avg_loglikelihood)\n",
    "        if (\n",
    "            len(avg_loglikelihoods) > 2\n",
    "            and abs(avg_loglikelihoods[-1] - avg_loglikelihoods[-2]) < 0.0001\n",
    "        ):\n",
    "            break\n",
    "        params = m_step(x_unlabeled, params)\n",
    "    print(\n",
    "        \"\\tphi: %s\\n\\tmu_0: %s\\n\\tmu_1: %s\\n\\tsigma_0: %s\\n\\tsigma_1: %s\"\n",
    "        % (\n",
    "            params[\"phi\"],\n",
    "            params[\"mu0\"],\n",
    "            params[\"mu1\"],\n",
    "            params[\"sigma0\"],\n",
    "            params[\"sigma1\"],\n",
    "        )\n",
    "    )\n",
    "    _, posterior = e_step(x_unlabeled, params)\n",
    "    forecasts = np.argmax(posterior, axis=1)\n",
    "    return forecasts, posterior, avg_loglikelihoods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tphi: 0.5822773496004533\n",
      "\tmu_0: [0.10314666 0.11459452 0.41736188 1.47411802]\n",
      "\tmu_1: [-0.09443411 -0.03986393  0.13549624  0.84421364]\n",
      "\tsigma_0: [[ 0.8779045  -0.02618267 -0.13045205 -0.04521762]\n",
      " [-0.02618267  1.10424681  0.10548973 -0.06813863]\n",
      " [-0.13045205  0.10548973  0.89065026 -0.24506134]\n",
      " [-0.04521762 -0.06813863 -0.24506134  0.36812073]]\n",
      "\tsigma_1: [[ 0.95110702 -0.08242479  0.01173042 -0.0201646 ]\n",
      " [-0.08242479  1.02092943 -0.09364236  0.01164521]\n",
      " [ 0.01173042 -0.09364236  1.06044446 -0.21156565]\n",
      " [-0.0201646   0.01164521 -0.21156565  0.12714576]]\n",
      "total steps:  42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'semi-supervised log likelihoods')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 178;\n",
       "                var nbb_unformatted_code = \"x_unlabeled = r_test_mod[significant_columns].values\\nx_labeled = X_train[significant_columns].values\\ny_labeled = y_train[\\\"target\\\"].values\\nlearned_params = learn_params(x_labeled, y_labeled)\\n(\\n    semisupervised_forecasts,\\n    semisupervised_posterior,\\n    semisupervised_loglikelihoods,\\n) = run_em(x_unlabeled, learned_params)\\nprint(\\\"total steps: \\\", len(semisupervised_loglikelihoods))\\nplt.plot(semisupervised_loglikelihoods)\\nplt.title(\\\"semi-supervised log likelihoods\\\")\\n# plt.savefig(\\\"semi-supervised.png\\\")\";\n",
       "                var nbb_formatted_code = \"x_unlabeled = r_test_mod[significant_columns].values\\nx_labeled = X_train[significant_columns].values\\ny_labeled = y_train[\\\"target\\\"].values\\nlearned_params = learn_params(x_labeled, y_labeled)\\n(\\n    semisupervised_forecasts,\\n    semisupervised_posterior,\\n    semisupervised_loglikelihoods,\\n) = run_em(x_unlabeled, learned_params)\\nprint(\\\"total steps: \\\", len(semisupervised_loglikelihoods))\\nplt.plot(semisupervised_loglikelihoods)\\nplt.title(\\\"semi-supervised log likelihoods\\\")\\n# plt.savefig(\\\"semi-supervised.png\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 178;\n",
       "                var nbb_unformatted_code = \"x_unlabeled = r_test_mod[significant_columns].values\\nx_labeled = X_train[significant_columns].values\\ny_labeled = y_train[\\\"target\\\"].values\\nlearned_params = learn_params(x_labeled, y_labeled)\\n(\\n    semisupervised_forecasts,\\n    semisupervised_posterior,\\n    semisupervised_loglikelihoods,\\n) = run_em(x_unlabeled, learned_params)\\nprint(\\\"total steps: \\\", len(semisupervised_loglikelihoods))\\nplt.plot(semisupervised_loglikelihoods)\\nplt.title(\\\"semi-supervised log likelihoods\\\")\\n# plt.savefig(\\\"semi-supervised.png\\\")\";\n",
       "                var nbb_formatted_code = \"x_unlabeled = r_test_mod[significant_columns].values\\nx_labeled = X_train[significant_columns].values\\ny_labeled = y_train[\\\"target\\\"].values\\nlearned_params = learn_params(x_labeled, y_labeled)\\n(\\n    semisupervised_forecasts,\\n    semisupervised_posterior,\\n    semisupervised_loglikelihoods,\\n) = run_em(x_unlabeled, learned_params)\\nprint(\\\"total steps: \\\", len(semisupervised_loglikelihoods))\\nplt.plot(semisupervised_loglikelihoods)\\nplt.title(\\\"semi-supervised log likelihoods\\\")\\n# plt.savefig(\\\"semi-supervised.png\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_unlabeled = r_test_mod[significant_columns].values\n",
    "x_labeled = X_train[significant_columns].values\n",
    "y_labeled = y_train[\"target\"].values\n",
    "learned_params = learn_params(x_labeled, y_labeled)\n",
    "(\n",
    "    semisupervised_forecasts,\n",
    "    semisupervised_posterior,\n",
    "    semisupervised_loglikelihoods,\n",
    ") = run_em(x_unlabeled, learned_params)\n",
    "print(\"total steps: \", len(semisupervised_loglikelihoods))\n",
    "plt.plot(semisupervised_loglikelihoods)\n",
    "plt.title(\"semi-supervised log likelihoods\")\n",
    "# plt.savefig(\"semi-supervised.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 186;\n",
       "                var nbb_unformatted_code = \"y_pred = semisupervised_posterior[:, 1]\\n\\n# Convert y_pred array to pandas dataframe\\npred_test = pd.DataFrame(\\n    data=y_pred,\\n    columns=[\\\"prediction_ssl_cont\\\"],\\n    index=dfr_test_with_label_X.index.copy(),\\n)\\n\\n# Set quantile\\nq = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\npred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n    lambda x: 0 if (x < q) else 1\\n)\\n\\n# Merge Baseline and SSL prediction\\npred_test_final2 = pd.merge(\\n    outcome_b[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n    pred_test[[\\\"prediction_ssl\\\"]],\\n    how=\\\"inner\\\",\\n    left_index=True,\\n    right_index=True,\\n)\";\n",
       "                var nbb_formatted_code = \"y_pred = semisupervised_posterior[:, 1]\\n\\n# Convert y_pred array to pandas dataframe\\npred_test = pd.DataFrame(\\n    data=y_pred,\\n    columns=[\\\"prediction_ssl_cont\\\"],\\n    index=dfr_test_with_label_X.index.copy(),\\n)\\n\\n# Set quantile\\nq = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\npred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n    lambda x: 0 if (x < q) else 1\\n)\\n\\n# Merge Baseline and SSL prediction\\npred_test_final2 = pd.merge(\\n    outcome_b[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n    pred_test[[\\\"prediction_ssl\\\"]],\\n    how=\\\"inner\\\",\\n    left_index=True,\\n    right_index=True,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 186;\n",
       "                var nbb_unformatted_code = \"y_pred = semisupervised_posterior[:, 1]\\n\\n# Convert y_pred array to pandas dataframe\\npred_test = pd.DataFrame(\\n    data=y_pred,\\n    columns=[\\\"prediction_ssl_cont\\\"],\\n    index=dfr_test_with_label_X.index.copy(),\\n)\\n\\n# Set quantile\\nq = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\npred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n    lambda x: 0 if (x < q) else 1\\n)\\n\\n# Merge Baseline and SSL prediction\\npred_test_final2 = pd.merge(\\n    outcome_b[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n    pred_test[[\\\"prediction_ssl\\\"]],\\n    how=\\\"inner\\\",\\n    left_index=True,\\n    right_index=True,\\n)\";\n",
       "                var nbb_formatted_code = \"y_pred = semisupervised_posterior[:, 1]\\n\\n# Convert y_pred array to pandas dataframe\\npred_test = pd.DataFrame(\\n    data=y_pred,\\n    columns=[\\\"prediction_ssl_cont\\\"],\\n    index=dfr_test_with_label_X.index.copy(),\\n)\\n\\n# Set quantile\\nq = pred_test[\\\"prediction_ssl_cont\\\"].quantile(q=1 - conservative_dr)\\npred_test[\\\"prediction_ssl\\\"] = pred_test[\\\"prediction_ssl_cont\\\"].apply(\\n    lambda x: 0 if (x < q) else 1\\n)\\n\\n# Merge Baseline and SSL prediction\\npred_test_final2 = pd.merge(\\n    outcome_b[[\\\"target\\\", \\\"prediction_baseline\\\", \\\"prediction_beforeRI_binary\\\"]],\\n    pred_test[[\\\"prediction_ssl\\\"]],\\n    how=\\\"inner\\\",\\n    left_index=True,\\n    right_index=True,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = semisupervised_posterior[:, 1]\n",
    "\n",
    "# Convert y_pred array to pandas dataframe\n",
    "pred_test = pd.DataFrame(\n",
    "    data=y_pred,\n",
    "    columns=[\"prediction_ssl_cont\"],\n",
    "    index=dfr_test_with_label_X.index.copy(),\n",
    ")\n",
    "\n",
    "# Set quantile\n",
    "q = pred_test[\"prediction_ssl_cont\"].quantile(q=1 - conservative_dr)\n",
    "pred_test[\"prediction_ssl\"] = pred_test[\"prediction_ssl_cont\"].apply(\n",
    "    lambda x: 0 if (x < q) else 1\n",
    ")\n",
    "\n",
    "# Merge Baseline and SSL prediction\n",
    "pred_test_final2 = pd.merge(\n",
    "    outcome_b[[\"target\", \"prediction_baseline\", \"prediction_beforeRI_binary\"]],\n",
    "    pred_test[[\"prediction_ssl\"]],\n",
    "    how=\"inner\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>prediction_baseline</th>\n",
       "      <th>prediction_beforeRI_binary</th>\n",
       "      <th>prediction_ssl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18096</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10838</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15233</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18772</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12694</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17324</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17428</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target  prediction_baseline  prediction_beforeRI_binary  prediction_ssl\n",
       "18096       1                    0                           0               1\n",
       "10838       0                    0                           0               0\n",
       "15233       1                    1                           1               1\n",
       "15997       0                    0                           0               0\n",
       "18772       0                    0                           0               0\n",
       "...       ...                  ...                         ...             ...\n",
       "12694       0                    0                           0               1\n",
       "17324       1                    0                           0               0\n",
       "17428       0                    0                           0               0\n",
       "17995       0                    0                           0               0\n",
       "2982        1                    1                           1               0\n",
       "\n",
       "[1200 rows x 4 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 187;\n",
       "                var nbb_unformatted_code = \"pred_test_final2\";\n",
       "                var nbb_formatted_code = \"pred_test_final2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 187;\n",
       "                var nbb_unformatted_code = \"pred_test_final2\";\n",
       "                var nbb_formatted_code = \"pred_test_final2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_test_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kickout + Kickin Metric: -0.052\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 188;\n",
       "                var nbb_unformatted_code = \"pred_test_final2[\\\"Flag\\\"] = pred_test_final2.apply(flag_df_ssl, axis=1)\\nkickout_ssl(pred_test_final2)\";\n",
       "                var nbb_formatted_code = \"pred_test_final2[\\\"Flag\\\"] = pred_test_final2.apply(flag_df_ssl, axis=1)\\nkickout_ssl(pred_test_final2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 188;\n",
       "                var nbb_unformatted_code = \"pred_test_final2[\\\"Flag\\\"] = pred_test_final2.apply(flag_df_ssl, axis=1)\\nkickout_ssl(pred_test_final2)\";\n",
       "                var nbb_formatted_code = \"pred_test_final2[\\\"Flag\\\"] = pred_test_final2.apply(flag_df_ssl, axis=1)\\nkickout_ssl(pred_test_final2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_test_final2[\"Flag\"] = pred_test_final2.apply(flag_df_ssl, axis=1)\n",
    "kickout_ssl(pred_test_final2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
