{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    r\"C:\\Users\\Asus\\Desktop\\Repo\\MasterThesis_RI\\Python-Real-World-Machine-Learning\\Module 2\\Chapter 5\",\n",
    ")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Basic Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sampling\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "\n",
    "# Modelling\n",
    "# Classification\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Semi-Supervised Learning\n",
    "from sklearn.semi_supervised import (\n",
    "    LabelPropagation,\n",
    "    LabelSpreading,\n",
    "    SelfTrainingClassifier,\n",
    ")\n",
    "from modAL.models import ActiveLearner\n",
    "\n",
    "# Chapter 5\n",
    "from SelfLearning import SelfLearningModel\n",
    "from scikitWQDA import WQDA\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    log_loss,\n",
    ")\n",
    "\n",
    "# Ensembling\n",
    "from sklearn.ensemble import *\n",
    "\n",
    "# Balancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Binning\n",
    "import woeBinningPandas\n",
    "\n",
    "# Create Unique ID\n",
    "import uuid\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, accepted_flag, target, train_ratio):\n",
    "    \"\"\"\n",
    "    The goal of this function is to load the original dataset, split it into accepts and rejects,\n",
    "    add ids, which can later be used for merging. For the rejects to further perform train / test split\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : name of the original dataset in quotation marks, csv format\n",
    "    accepted_flag: name of the accepted flag; Binary: 1 if accepted, 0 if rejected\n",
    "    target : name of the target column\n",
    "    train_ratio : percentage used for training; Continuous (0,1)\n",
    "    Return\n",
    "    ------\n",
    "    a : accepted data\n",
    "    r : rejected data\n",
    "    r_dev : rejected trainining data\n",
    "    r_test : rejected testing data\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_csv(\"C:/Users/Asus/Desktop/Repo/MasterThesis_RI/Data_09_05/\" + df)\n",
    "\n",
    "    # Accepted\n",
    "\n",
    "    ## Create separate dataset with accepts\n",
    "    dfa = data[data[accepted_flag] == 1]\n",
    "    dfa = dfa.drop([accepted_flag], axis=1)\n",
    "    ## Rename target variable as \"target\"\n",
    "    dfa = dfa.rename(columns={target: \"target\"})\n",
    "    ## Add id to the dataset, which can later be used for merging\n",
    "    dfa[\"id\"] = dfa.index.to_series().map(lambda x: uuid.uuid4())\n",
    "\n",
    "    # Rejected\n",
    "\n",
    "    ## Create separate dataset with accepts\n",
    "    dfr = data[data[accepted_flag] == 0]\n",
    "    dfr = dfr.drop([accepted_flag], axis=1)\n",
    "    ## Add id to the dataset, which can later be used for merging\n",
    "    dfr[\"id\"] = dfr.index.to_series().map(lambda x: uuid.uuid4())\n",
    "    ## Train/Test Split (without labels)\n",
    "    ### Shuffle the dataset\n",
    "    shuffle_df = dfr.sample(frac=1, random_state=42)\n",
    "    ### Define a size for the train set\n",
    "    train_size = int(train_ratio * len(shuffle_df))\n",
    "    ### Split the dataset\n",
    "    dfr_dev = shuffle_df[:train_size]\n",
    "    dfr_test = shuffle_df[train_size:]\n",
    "    ## Unlabel the rejects (i.e. drop the target)\n",
    "    dfr_dev_with_label = dfr_dev\n",
    "    dfr_test_with_label = dfr_test\n",
    "    dfr_dev2 = dfr_dev_with_label.drop([target], axis=1)\n",
    "    dfr_test2 = dfr_test_with_label.drop([target], axis=1)\n",
    "\n",
    "    return dfr_dev_with_label, dfr_test_with_label, dfa, dfr, dfr_dev2, dfr_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_rejects_without_id(r_dev, r_test, r_dev_mod, r_test_mod):\n",
    "    # Create rejects datasets with the modelling columns only (for a dataset with 8 features)\n",
    "    r_dev_mod = r_dev.iloc[:, :9]\n",
    "    r_test_mod = r_test.iloc[:, :9]\n",
    "    return r_dev_mod, r_test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_rejects_with_id(r_dev, r_test, r_dev_mod_id, r_test_mod_id):\n",
    "    # Create rejects datasets with the modelling columns + id\n",
    "    r_dev_mod_id = r_dev.iloc[:, :10]\n",
    "    r_test_mod_id = r_test.iloc[:, :10]\n",
    "    return r_dev_mod_id, r_test_mod_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_y(data):\n",
    "    \"\"\"\n",
    "    Undersample the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : accepts, dataframe\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    X_res : undersampled data\n",
    "    y_res : undersampled labels\n",
    "\n",
    "    \"\"\"\n",
    "    # Create X and y\n",
    "    X = data.loc[:, data.columns != \"target\"]\n",
    "    y = data.loc[:, data.columns == \"target\"]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sample\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : data\n",
    "    y : labels\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    X_train : training modelling fields\n",
    "    X_test : test modelling fields\n",
    "    y_train : training labels\n",
    "    y_test : testing labels\n",
    "\n",
    "    \"\"\"\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_res, y_res, test_size=0.2, random_state=7\n",
    "    )\n",
    "    columns = X_train.columns\n",
    "\n",
    "    # Columns\n",
    "    X_train = pd.DataFrame(data=X_train, columns=columns)\n",
    "    y_train = pd.DataFrame(data=y_train, columns=[\"target\"])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(X_train, y_train, X_test):\n",
    "    #logreg = LogisticRegression(fit_intercept=True, penalty=\"none\")\n",
    "    logreg = LGBMClassifier()\n",
    "    logreg.fit(X_train, y_train.values.ravel())\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    return logreg, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation_forest(X_train, r_dev_mod, r_test_mod):\n",
    "    \"\"\"\n",
    "    The goal of this function is to filter the outliers from the rejected sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: accepts training data; Dataframe\n",
    "    r_dev_mod: rejects modelling data prior outlier treatment; Dataframe\n",
    "    r_test_mod: rejects testinf data prior outlier treatment; Dataframe\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    r_dev_mod: rejects modelling data post outlier treatment; Dataframe\n",
    "    r_test_mod: rejects training data prior outlier treatment; Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Build Isolation forest model\n",
    "    isf = IsolationForest(\n",
    "        n_estimators=50, max_samples=\"auto\", contamination=float(0.02), max_features=1.0\n",
    "    )\n",
    "    isf.fit(X_train)\n",
    "    rej_isf = isf.predict(r_dev_mod)\n",
    "    # Add scores and anomaly columns to rejected train\n",
    "    r_dev_mod[\"scores\"] = isf.decision_function(r_dev_mod)\n",
    "    r_dev_mod[\"anomaly\"] = isf.predict(\n",
    "        r_dev_mod[[\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\"]]\n",
    "    )\n",
    "    # Print number of non-outliers and outliers\n",
    "#     print(\n",
    "#         \"Rejected Train. Number of non-outliers is:\", np.sum(r_dev_mod[\"anomaly\"] == 1)\n",
    "#     )\n",
    "#     print(\"Rejected Train. Number of outliers is:\", np.sum(r_dev_mod[\"anomaly\"] == -1))\n",
    "    # Drop all outliers\n",
    "    r_dev_mod = r_dev_mod[r_dev_mod.anomaly != -1]\n",
    "    # Delete columns related to the outliers\n",
    "    r_dev_mod = r_dev_mod[[\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\"]]\n",
    "\n",
    "    # Add scores and anomaly columns to rejected test\n",
    "    r_test_mod[\"scores\"] = isf.decision_function(r_test_mod)\n",
    "    r_test_mod[\"anomaly\"] = isf.predict(\n",
    "        r_test_mod[[\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\"]]\n",
    "    )\n",
    "    # Print number of non-outliers and outliers\n",
    "#     print(\n",
    "#         \"Rejected Test. Number of non-outliers is:\", np.sum(r_test_mod[\"anomaly\"] == 1)\n",
    "#     )\n",
    "#     print(\"Rejected Test. Number of outliers is:\", np.sum(r_test_mod[\"anomaly\"] == -1))\n",
    "    # Drop all outliers\n",
    "    r_test_mod = r_test_mod[r_test_mod.anomaly != -1]\n",
    "    # Delete columns related to the outliers\n",
    "    r_test_mod = r_test_mod[\n",
    "        [\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\"]\n",
    "    ]\n",
    "\n",
    "    return r_dev_mod, r_test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(y_test, X_test, X_test_3, model):\n",
    "    # Test set with labels\n",
    "    test_labels = pd.merge(\n",
    "        y_test,\n",
    "        X_test,\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    # Predictions on testset\n",
    "    test_pred = model.predict_proba(X_test_3)[:, 1]\n",
    "    test_pred2 = pd.DataFrame(data=test_pred, columns=[\"prediction\"])\n",
    "    test_pred2[\"count\"] = test_pred2.groupby(\"prediction\")[\"prediction\"].transform(\n",
    "        \"count\"\n",
    "    )\n",
    "    test_pred2.groupby([\"prediction\"]).count()\n",
    "    test_pred2.describe()\n",
    "\n",
    "    # Join predictions with test new\n",
    "    pred_test_kgb = pd.DataFrame(\n",
    "        data=test_pred, columns=[\"prediction_beforeRI\"], index=y_test.index.copy()\n",
    "    )\n",
    "    pred_test1 = pd.merge(\n",
    "        test_labels,\n",
    "        pred_test_kgb[[\"prediction_beforeRI\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    return pred_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metrics(pred_label, true_label, model):\n",
    "    \"\"\"\"\"\n",
    "    pred_label = predicted label of the model\n",
    "    true_label = true label\n",
    "    model = model name\n",
    "    \"\"\" \"\"\n",
    "\n",
    "    # F1 score\n",
    "    f1_stat = f1_score(pred_label, true_label, average=\"weighted\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(pred_label, true_label, labels=model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    fig = disp.plot()\n",
    "\n",
    "    return print(\"F1_stat \", model, \"is: \", f1_stat, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions1(model):\n",
    "    # Join predictions with train new\n",
    "    pred = model.predict_proba(r_dev_mod)[:, 1]\n",
    "    pred2 = pd.DataFrame(\n",
    "        data=pred,\n",
    "        columns=[\"prediction2\"],\n",
    "        index=r_dev_mod.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Set cut-off\n",
    "    q1 = pred2[\"prediction2\"].quantile(q=1 - conservative_dr)\n",
    "    pred2[\"prediction_beforeRI\"] = pred2[\"prediction2\"].apply(\n",
    "        lambda x: 0 if (x < q1) else 1\n",
    "    )\n",
    "    outcome = pd.merge(\n",
    "        r_dev_mod_id,\n",
    "        pred2[[\"prediction_beforeRI\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    # pred_test1.dropna(subset=[\"prediction_beforeRI\"], inplace=True)\n",
    "    outcome = outcome[[\"id\", \"prediction_beforeRI\"]]\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(data, state, prediction):\n",
    "    print(\n",
    "        \"The number of accurately classified cases \",\n",
    "        state,\n",
    "        \" is: \",\n",
    "        data[\n",
    "            (data.target == 1) & (data[prediction] == 1)\n",
    "            | (data.target == 0) & (data[prediction] == 0)\n",
    "        ].shape[0],\n",
    "    )\n",
    "    print(\n",
    "        \"The number of misclassified cases \",\n",
    "        state,\n",
    "        \" is: \",\n",
    "        data[\n",
    "            (data.target == 1) & (data[prediction] == 0)\n",
    "            | (data.target == 0) & (data[prediction] == 1)\n",
    "        ].shape[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_fun(\n",
    "    category,\n",
    "    data,\n",
    "    y_true,\n",
    "    y_pred,\n",
    "):\n",
    "    print(category, \" :\", log_loss(data[y_true], data[y_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(ri_data):  # ri1_train, ri2_train, etc..\n",
    "\n",
    "    # TRAIN NEW\n",
    "    # Join labels to train set\n",
    "    # Accepts\n",
    "    train_accepts = pd.merge(\n",
    "        X_train,\n",
    "        y_train[[\"target\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    train_accepts[\"Flag1\"] = \"Accept\"\n",
    "\n",
    "    # Rejects\n",
    "    train_rejects = pd.merge(\n",
    "        r_dev_mod,\n",
    "        ri_data[[\"prediction_beforeRI\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Align naming\n",
    "    train_rejects.rename(columns={\"prediction_beforeRI\": \"target\"}, inplace=True)\n",
    "\n",
    "    # Create X and y\n",
    "    X_res_rej, y_res_rej = create_X_y(train_rejects)\n",
    "\n",
    "    #     # Sample a matching number of observations from the accepts as the size of rejects\n",
    "    #     # Shuffle the dataset\n",
    "    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\n",
    "    #     # Define a size for the train set\n",
    "    #     train_size = int(0.25 * len(shuffle_df))\n",
    "    #     train_accepts = shuffle_df[:train_size]\n",
    "    #     print(train_accepts.shape)\n",
    "\n",
    "    # Concatenate Train Accepts and Train Rejects\n",
    "    train_new = pd.concat([train_accepts, train_rejects])\n",
    "\n",
    "    # Flag\n",
    "    train_new[\"Flag\"] = train_new[\"Flag1\"].apply(\n",
    "        lambda x: \"Accept\" if x == \"Accept\" else \"Reject\"\n",
    "    )\n",
    "    train_new = train_new.drop(columns=[\"Flag1\"])\n",
    "\n",
    "    # Retrain KGB Model\n",
    "\n",
    "    # Split\n",
    "    X_new = train_new[significant_columns]\n",
    "    y_new = train_new[\"target\"]\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "        X_new, y_new, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Keep only columns for modelling\n",
    "    os_data_X_2_new = X_train_new[significant_columns]\n",
    "    X_test_2_new = X_test_new[significant_columns]\n",
    "\n",
    "    # Build Logistic regression\n",
    "    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\"none\")\n",
    "    # logreg2 = GradientBoostingClassifier(criterion=\"mse\")\n",
    "    # logreg2 = RandomForestClassifier()\n",
    "    logreg2 = LGBMClassifier()\n",
    "    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\n",
    "\n",
    "    # Predictions\n",
    "    pred_test2 = pred(y_test, X_test, X_test_3, logreg2)\n",
    "    pred_test2.rename(\n",
    "        columns={\"prediction_beforeRI\": \"prediction_baseline\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    # Merge original and baseline predictions\n",
    "    pred_test_final = pd.merge(\n",
    "        pred_test1[[\"target\", \"prediction_beforeRI\"]],\n",
    "        pred_test2[[\"prediction_baseline\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff 50percentile of the distribution\n",
    "    q1 = pred_test_final[\"prediction_beforeRI\"].quantile(q=1 - conservative_dr)\n",
    "    q2 = pred_test_final[\"prediction_baseline\"].quantile(q=1 - conservative_dr)\n",
    "    #     print(q1)\n",
    "    #     print(q2)\n",
    "    pred_test_final[\"prediction_beforeRI_binary\"] = pred_test_final[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(lambda x: 0 if (x < q1) else 1)\n",
    "    pred_test_final[\"prediction_baseline\"] = pred_test_final[\n",
    "        \"prediction_baseline\"\n",
    "    ].apply(lambda x: 0 if (x < q2) else 1)\n",
    "\n",
    "    #     # Log Loss\n",
    "    #     log_loss_fun(\"Before\", pred_test_final, \"target\", \"prediction_beforeRI_binary\")\n",
    "    #     log_loss_fun(\"After\", pred_test_final, \"target\", \"prediction_baseline\")\n",
    "\n",
    "    #     # Numbers of accurately classified and misclassified cases\n",
    "    #     print_results(pred_test_final, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    #     print_results(pred_test_final, \"with baseline\", \"prediction_baseline\")\n",
    "    return pred_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_rejects(ri_data):  # ri1_train, ri2_train, etc..\n",
    "\n",
    "    # TRAIN NEW\n",
    "    # Join labels to train set\n",
    "    # Accepts\n",
    "    train_accepts = pd.merge(\n",
    "        X_train,\n",
    "        y_train[[\"target\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    train_accepts[\"Flag1\"] = \"Accept\"\n",
    "\n",
    "    # Rejects\n",
    "    train_rejects = pd.merge(\n",
    "        r_dev_mod,\n",
    "        ri_data[[\"prediction_beforeRI\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Align naming\n",
    "    train_rejects.rename(columns={\"prediction_beforeRI\": \"target\"}, inplace=True)\n",
    "\n",
    "    # Create X and y for rejects\n",
    "    X_res_rej, y_res_rej = create_X_y(train_rejects)\n",
    "\n",
    "    #     # Sample a matching number of observations from the accepts as the size of rejetcs\n",
    "    #     ## Shuffle the dataset\n",
    "    #     shuffle_df = train_accepts.sample(frac=1, random_state=42)\n",
    "    #     ## Define a size for the train set\n",
    "    #     train_size = int(0.03 * len(shuffle_df))\n",
    "    #     train_accepts = shuffle_df[:train_size]\n",
    "    #     print(train_accepts.shape)\n",
    "\n",
    "    # Concatenate Train Accepts and Train Rejects\n",
    "    train_new = pd.concat([train_accepts, train_rejects])\n",
    "\n",
    "    # Flag\n",
    "    train_new[\"Flag\"] = train_new[\"Flag1\"].apply(\n",
    "        lambda x: \"Accept\" if x == \"Accept\" else \"Reject\"\n",
    "    )\n",
    "    train_new = train_new.drop(columns=[\"Flag1\"])\n",
    "\n",
    "    # Retrain KGB Model\n",
    "\n",
    "    # Split\n",
    "    X_new = train_new[significant_columns]\n",
    "    y_new = train_new[\"target\"]\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "        X_new, y_new, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Keep only columns for modelling\n",
    "    os_data_X_2_new = X_train_new[significant_columns]\n",
    "    X_test_2_new = X_test_new[significant_columns]\n",
    "\n",
    "    # Build Logistic regression\n",
    "    # logreg2 = LogisticRegression(fit_intercept=False, penalty=\"none\")\n",
    "    # logreg2 = GradientBoostingClassifier(criterion=\"mse\")\n",
    "    # logreg2 = RandomForestClassifier()\n",
    "    logreg2 = LGBMClassifier()\n",
    "    logreg2.fit(os_data_X_2_new, y_train_new.values.ravel())\n",
    "\n",
    "    # Predictions\n",
    "    pred_rej2 = pred(\n",
    "        dfr_test_with_label_y, dfr_test_with_label_X, dfr_test_with_label_X, logreg2\n",
    "    )\n",
    "    pred_rej2.rename(\n",
    "        columns={\"prediction_beforeRI\": \"prediction_baseline\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    # Merge original and baseline predictions\n",
    "    pred_test_final = pd.merge(\n",
    "        pred_rej1[[\"target\", \"prediction_beforeRI\"]],\n",
    "        pred_rej2[[\"prediction_baseline\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff: median of the distribution\n",
    "    q1 = pred_test_final[\"prediction_beforeRI\"].quantile(q=1 - conservative_dr)\n",
    "    q2 = pred_test_final[\"prediction_baseline\"].quantile(q=1 - conservative_dr)\n",
    "    #     print(q1)\n",
    "    #     print(q2)\n",
    "    pred_test_final[\"prediction_beforeRI_binary\"] = pred_test_final[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(lambda x: 0 if (x < q1) else 1)\n",
    "    pred_test_final[\"prediction_baseline\"] = pred_test_final[\n",
    "        \"prediction_baseline\"\n",
    "    ].apply(lambda x: 0 if (x < q2) else 1)\n",
    "\n",
    "    #     # Log Loss\n",
    "    #     log_loss_fun(\"Before\", pred_test_final, \"target\", \"prediction_beforeRI_binary\")\n",
    "    #     log_loss_fun(\"After\", pred_test_final, \"target\", \"prediction_baseline\")\n",
    "\n",
    "    #     # Numbers of accurately classified and misclassified cases\n",
    "    #     print_results(pred_test_final, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    #     print_results(pred_test_final, \"with baseline\", \"prediction_baseline\")\n",
    "    return pred_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_prep(X_accept, y_accept, X_reject):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X_train_acc : training data of accepted population\n",
    "    y_train_acc: training lables of accepted population\n",
    "    X_train_rej: training data of rejected population\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df : data of accepted and rejected population\n",
    "\n",
    "    \"\"\"\n",
    "    # Merge explanatory and target in accepts\n",
    "    accepts = pd.merge(\n",
    "        X_accept, y_accept, how=\"left\", left_index=True, right_index=True\n",
    "    )\n",
    "    # Create accept flag\n",
    "    accepts[\"Flag1\"] = \"Accept\"\n",
    "\n",
    "    # Sample a matching number of observations from the rejects as the size of accepts\n",
    "    ## Shuffle the dataset\n",
    "    shuffle_df = X_reject.sample(frac=1)\n",
    "    ## Define a size for the train set\n",
    "    train_size = int(0.25 * len(shuffle_df))\n",
    "    train_rejects = shuffle_df[:train_size]\n",
    "\n",
    "    # Merge accepts and rejects\n",
    "    df = pd.concat([accepts, train_rejects])\n",
    "\n",
    "    # If accepted use accept label, if rejected use -1 (default value for unlabelled entries) - hard-coded for now\n",
    "    conditions = [\n",
    "        (df[\"Flag1\"] == \"Accept\") & (df[\"target\"] == 1),\n",
    "        (df[\"Flag1\"] == \"Accept\") & (df[\"target\"] == 0),\n",
    "    ]\n",
    "    choices = [1, 0]\n",
    "\n",
    "    # New target is called unlabel\n",
    "    df[\"unlabel\"] = np.select(conditions, choices, -1)\n",
    "\n",
    "    # Select columns for modelling - hard-coded for now - can be moved outside of the function\n",
    "    df = df[[\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\", \"unlabel\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_split(df, target):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : dataframe of accepted and rejected population, including data and labels\n",
    "    target: string name of the target column, should be passed in quotation marks (e.g. \"target\")\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    X_train: training data of accepted and rejected population, ready to be fed into the semi-supervised model\n",
    "    y_train: training labels of accepted and rejected population, ready to be fed into the semi-supervised model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = df.loc[:, df.columns != target]\n",
    "    y_train = df.loc[:, df.columns == target]\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_model_selftraining(X_train, y_train, model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X_train : training data of accepted and rejected population\n",
    "    y_train : training lables of accepted population (0,1) and rejected population (-1)\n",
    "    model : semi-supervised learning model from sklearn (Self-Training Classifier)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    ssl: trained semi-supervised learning model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit SSL moodel\n",
    "    #base = SVC(probability=True, gamma=\"auto\")\n",
    "    #base = LogisticRegression(fit_intercept=True, penalty=\"none\")\n",
    "    base = LGBMClassifier()\n",
    "    model = model(base)\n",
    "    labels = np.copy(y_train)\n",
    "    data = np.copy(X_train)\n",
    "    ssl = model.fit(data, labels)\n",
    "    return ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_model_label(X_train, y_train, model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X_train : training data of accepted and rejected population\n",
    "    y_train : training lables of accepted population (0,1) and rejected population (-1)\n",
    "    model : semi-supervised learning model from sklearn (Label Propagation, Label Spreading)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    ssl: trained semi-supervised learning model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit SSL moodel\n",
    "    model = model()\n",
    "    labels = np.copy(y_train)\n",
    "    data = np.copy(X_train)\n",
    "    ssl = model.fit(data, labels)\n",
    "    return ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_predictions_ds(X_test, estimators):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ssl : trained semi-supervised learning model\n",
    "    X_test : testing data of accepted and rejected population for predictions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    Predictions before RI (binary)\n",
    "    Predictions after RI (binary)\n",
    "\n",
    "    \"\"\"\n",
    "    # Make Predictions\n",
    "    y_pred = combine_using_Dempster_Schafer(X_test, estimators)\n",
    "\n",
    "    # Convert y_pred array to pandas dataframe\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        columns=[\"prediction_afterRI\"],\n",
    "        index=X_test.index.copy(),\n",
    "    )\n",
    "    a1 = pred_test1[[\"id\", \"target\", \"prediction_beforeRI\"]]  # hard-coded for now\n",
    "    a2 = pred_test[[\"prediction_afterRI\"]]  # hard-coded for now\n",
    "\n",
    "    # Merge a1 and a2\n",
    "    a1_a2_inner = pd.merge(\n",
    "        a1,\n",
    "        a2,\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff 75percentile of the distribution\n",
    "    q1 = a1_a2_inner[\"prediction_beforeRI\"].quantile(q=0.25)\n",
    "    q2 = a1_a2_inner[\"prediction_afterRI\"].quantile(q=0.25)\n",
    "\n",
    "    a1_a2_inner[\"prediction_beforeRI_binary\"] = a1_a2_inner[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(lambda x: 0 if (x < q1) else 1)\n",
    "    a1_a2_inner[\"prediction_afterRI_binary\"] = a1_a2_inner[\"prediction_afterRI\"].apply(\n",
    "        lambda x: 0 if (x < q2) else 1\n",
    "    )\n",
    "\n",
    "    # Log Loss\n",
    "    #log_loss_fun(\"Before\", a1_a2_inner, \"target\", \"prediction_beforeRI\")\n",
    "    #log_loss_fun(\"After\", a1_a2_inner, \"target\", \"prediction_afterRI\")\n",
    "\n",
    "    # Numbers of accurately classified and misclassified cases\n",
    "    #print_results(a1_a2_inner, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    #print_results(a1_a2_inner, \"after RI\", \"prediction_afterRI_binary\")\n",
    "    return a1_a2_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_predictions_oth(ssl, X_test):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ssl : trained semi-supervised learning model\n",
    "    X_test : testing data of accepted and rejected population for predictions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    Predictions before RI (binary)\n",
    "    Predictions after RI (binary)\n",
    "\n",
    "    \"\"\"\n",
    "    # Make Predictions\n",
    "    y_pred = ssl.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Convert y_pred array to pandas dataframe\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        columns=[\"prediction_ssl_cont\"],\n",
    "        index=X_test.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Set quantile\n",
    "    q = pred_test[\"prediction_ssl_cont\"].quantile(q=1 - conservative_dr)\n",
    "    pred_test[\"prediction_ssl\"] = pred_test[\"prediction_ssl_cont\"].apply(\n",
    "        lambda x: 0 if (x < q) else 1\n",
    "    )\n",
    "\n",
    "    # Merge Baseline and SSL prediction\n",
    "    pred_test_final2 = pd.merge(\n",
    "        outcome_a[[\"target\", \"prediction_baseline\", \"prediction_beforeRI_binary\"]],\n",
    "        pred_test[[\"prediction_ssl\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "#     # Log Loss\n",
    "#     log_loss_fun(\"Before\", pred_test_final2, \"target\", \"prediction_beforeRI_binary\")\n",
    "#     log_loss_fun(\"After\", pred_test_final2, \"target\", \"prediction_ssl\")\n",
    "\n",
    "#     # Numbers of accurately classified and misclassified cases\n",
    "#     print_results(pred_test_final2, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "#     print_results(pred_test_final2, \"after RI\", \"prediction_ssl\")\n",
    "    return pred_test_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_predictions_oth_rej(ssl, dfr_test_with_label_X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ssl : trained semi-supervised learning model\n",
    "    X_test : testing data of accepted and rejected population for predictions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    Predictions before RI (binary)\n",
    "    Predictions after RI (binary)\n",
    "\n",
    "    \"\"\"\n",
    "    # Make Predictions\n",
    "\n",
    "    y_pred = ssl.predict_proba(dfr_test_with_label_X)[:, 1]\n",
    "\n",
    "    # Convert y_pred array to pandas dataframe\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        columns=[\"prediction_ssl_cont\"],\n",
    "        index=dfr_test_with_label_X.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Set quantile\n",
    "    q = pred_test[\"prediction_ssl_cont\"].quantile(q=1 - conservative_dr)\n",
    "    pred_test[\"prediction_ssl\"] = pred_test[\"prediction_ssl_cont\"].apply(\n",
    "        lambda x: 0 if (x < q) else 1\n",
    "    )\n",
    "\n",
    "    # Merge Baseline and SSL prediction\n",
    "    pred_test_final2 = pd.merge(\n",
    "        outcome_b[[\"target\", \"prediction_baseline\", \"prediction_beforeRI_binary\"]],\n",
    "        pred_test[[\"prediction_ssl\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "#     # Log Loss\n",
    "#     log_loss_fun(\"Before\", pred_test_final2, \"target\", \"prediction_beforeRI_binary\")\n",
    "#     log_loss_fun(\"After\", pred_test_final2, \"target\", \"prediction_ssl\")\n",
    "\n",
    "#     # Numbers of accurately classified and misclassified cases\n",
    "#     print_results(pred_test_final2, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "#     print_results(pred_test_final2, \"after RI\", \"prediction_ssl\")\n",
    "    return pred_test_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_predictions_oth2(ssl, X_test):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ssl : trained semi-supervised learning model\n",
    "    X_test : testing data of accepted and rejected population for predictions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    Predictions before RI (binary)\n",
    "    Predictions after RI (binary)\n",
    "\n",
    "    \"\"\"\n",
    "    # Make Predictions\n",
    "\n",
    "    y_pred = ssl.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Convert y_pred array to pandas dataframe\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        columns=[\"prediction_afterRI\"],\n",
    "        index=X_test.index.copy(),\n",
    "    )\n",
    "    a1 = pred_test1[[\"id\", \"target\", \"prediction_beforeRI\"]]  # hard-coded for now\n",
    "    a2 = pred_test[[\"prediction_afterRI\"]]  # hard-coded for now\n",
    "\n",
    "    # Merge a1 and a2\n",
    "    a1_a2_inner = pd.merge(\n",
    "        a1,\n",
    "        a2,\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff 75percentile of the distribution\n",
    "    q1 = a1_a2_inner[\"prediction_beforeRI\"].quantile(q=0.25)\n",
    "    q2 = a1_a2_inner[\"prediction_afterRI\"].quantile(q=0.25)\n",
    "\n",
    "    a1_a2_inner[\"prediction_beforeRI_binary\"] = a1_a2_inner[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(lambda x: 0 if (x < q1) else 1)\n",
    "    a1_a2_inner[\"prediction_afterRI_binary\"] = a1_a2_inner[\"prediction_afterRI\"].apply(\n",
    "        lambda x: 0 if (x < q2) else 1\n",
    "    )\n",
    "\n",
    "    # Log Loss\n",
    "#     log_loss_fun(\"Before\", a1_a2_inner, \"target\", \"prediction_beforeRI\")\n",
    "#     log_loss_fun(\"After\", a1_a2_inner, \"target\", \"prediction_afterRI\")\n",
    "\n",
    "#     # Numbers of accurately classified and misclassified cases\n",
    "#     print_results(a1_a2_inner, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "#     print_results(a1_a2_inner, \"after RI\", \"prediction_afterRI_binary\")\n",
    "    return a1_a2_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_predictions_al(al, X_test):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ssl : trained semi-supervised learning model\n",
    "    X_test : testing data of accepted and rejected population for predictions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    Predictions before RI (binary)\n",
    "    Predictions after RI (binary)\n",
    "\n",
    "    \"\"\"\n",
    "    # Make Predictions\n",
    "\n",
    "    y_pred = regressor.predict(X_test_3)\n",
    "\n",
    "    # Convert y_pred array to pandas dataframe\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        columns=[\"prediction_afterRI\"],\n",
    "        index=X_test.index.copy(),\n",
    "    )\n",
    "    a1 = pred_test1[[\"id\", \"target\", \"prediction_beforeRI\"]]  # hard-coded for now\n",
    "    a2 = pred_test[[\"prediction_afterRI\"]]  # hard-coded for now\n",
    "\n",
    "    # Merge a1 and a2\n",
    "    a1_a2_inner = pd.merge(\n",
    "        a1,\n",
    "        a2,\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff 75percentile of the distribution\n",
    "    q1 = a1_a2_inner[\"prediction_beforeRI\"].quantile(q=0.25)\n",
    "    q2 = a1_a2_inner[\"prediction_afterRI\"].quantile(q=0.25)\n",
    "\n",
    "    a1_a2_inner[\"prediction_beforeRI_binary\"] = a1_a2_inner[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(lambda x: 0 if (x < q1) else 1)\n",
    "    a1_a2_inner[\"prediction_afterRI_binary\"] = a1_a2_inner[\"prediction_afterRI\"].apply(\n",
    "        lambda x: 0 if (x < q2) else 1\n",
    "    )\n",
    "\n",
    "    # Log Loss\n",
    "    log_loss_fun(\"Before\", a1_a2_inner, \"target\", \"prediction_beforeRI\")\n",
    "    log_loss_fun(\"After\", a1_a2_inner, \"target\", \"prediction_afterRI\")\n",
    "\n",
    "    # Numbers of accurately classified and misclassified cases\n",
    "    print_results(a1_a2_inner, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    print_results(a1_a2_inner, \"after RI\", \"prediction_afterRI_binary\")\n",
    "    return a1_a2_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_std(regressor, X):\n",
    "    _, std = regressor.predict(X, return_std=True)\n",
    "    return np.argmax(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning(n, X_train, y_train, r_dev_mod):\n",
    "    n_initial = n\n",
    "    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n",
    "    X_training, y_training = X_train.iloc[initial_idx], y_train.iloc[initial_idx]\n",
    "\n",
    "    learner = ActiveLearner(\n",
    "        estimator=LGBMClassifier(),\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training=X_training,\n",
    "        y_training=y_training,\n",
    "    )\n",
    "    query_idx, query_inst = learner.query(r_dev_mod)\n",
    "    # active learning\n",
    "    n_queries = int(0.2 * len(r_dev_mod))\n",
    "    for idx in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_train)\n",
    "        learner.teach(X_train.iloc[query_idx], y_train.iloc[query_idx])\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning2(X_train, y_train, r_dev_mod, fraction):\n",
    "    n_initial = len(X_train)\n",
    "    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n",
    "    X_training, y_training = X_train.iloc[initial_idx], y_train.iloc[initial_idx]\n",
    "\n",
    "    learner = ActiveLearner(\n",
    "        estimator=LGBMClassifier(),\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training=X_training,\n",
    "        y_training=y_training,\n",
    "    )\n",
    "    query_idx, query_inst = learner.query(r_dev_mod)\n",
    "    # active learning\n",
    "    n_queries = int(fraction * len(r_dev_mod))\n",
    "    for idx in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_train)\n",
    "        learner.teach(X_train.iloc[query_idx], y_train.iloc[query_idx])\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Learning with Self-Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning3(X_train, y_train, r_dev_mod, fraction):\n",
    "    n_initial = len(X_train)\n",
    "    initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n",
    "    X_training, y_training = X_train.iloc[initial_idx], y_train.iloc[initial_idx]\n",
    "\n",
    "    learner = ActiveLearner(\n",
    "        estimator=SelfTrainingClassifier(\n",
    "            base_estimator=LGBMClassifier()\n",
    "        ),\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training=X_training,\n",
    "        y_training=y_training,\n",
    "    )\n",
    "    query_idx, query_inst = learner.query(r_dev_mod)\n",
    "    # active learning\n",
    "    n_queries = int(fraction * len(r_dev_mod))\n",
    "    for idx in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_train)\n",
    "        learner.teach(X_train.iloc[query_idx], y_train.iloc[query_idx])\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kickout measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flag_df(df):\n",
    "\n",
    "#     # Flag kicked out bad cases (want more of these)\n",
    "#     if (\n",
    "#         df[\"target\"] == 1\n",
    "#         and df[\"prediction_beforeRI_binary\"] == 0\n",
    "#         and df[\"prediction_ssl\"] == 1\n",
    "#     ):\n",
    "#         return \"KB\"\n",
    "\n",
    "#     # Flag kicked out good cases (want less of these)\n",
    "#     elif (\n",
    "#         df[\"target\"] == 0\n",
    "#         and df[\"prediction_beforeRI_binary\"] == 0\n",
    "#         and df[\"prediction_ssl\"] == 1\n",
    "#     ):\n",
    "#         return \"KG\"\n",
    "\n",
    "#     # Flag kicked in good cases (want more of these)\n",
    "#     elif (\n",
    "#         df[\"target\"] == 0\n",
    "#         and df[\"prediction_beforeRI_binary\"] == 1\n",
    "#         and df[\"prediction_ssl\"] == 0\n",
    "#     ):\n",
    "#         return \"IG\"\n",
    "\n",
    "#     # Flag kicked in bad cases (want less of these)\n",
    "#     elif (\n",
    "#         df[\"target\"] == 1\n",
    "#         and df[\"prediction_beforeRI_binary\"] == 1\n",
    "#         and df[\"prediction_ssl\"] == 0\n",
    "#     ):\n",
    "#         return \"IB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kickout(df):\n",
    "\n",
    "#     # Counts of kickout bad and kickout good\n",
    "#     counts = df[\"Flag\"].value_counts()\n",
    "#     if \"KB\" in df.values:\n",
    "#         kb = counts.KB  # want more of these\n",
    "#     else:\n",
    "#         kb = 0\n",
    "#     if \"KG\" in df.values:\n",
    "#         kg = counts.KG  # want less of these\n",
    "#     else:\n",
    "#         kg = 0\n",
    "\n",
    "#     if \"IG\" in df.values:\n",
    "#         ig = counts.IG  # want more of these\n",
    "#     else:\n",
    "#         ig = 0\n",
    "\n",
    "#     if \"IB\" in df.values:\n",
    "#         ib = counts.IB  # want less of these\n",
    "#     else:\n",
    "#         ib = 0\n",
    "\n",
    "#     # Counts of number of actual bad cases\n",
    "#     sb = df[df[\"target\"] == 1].shape[0]\n",
    "#     sg = df[df[\"target\"] == 0].shape[0]\n",
    "\n",
    "#     # Target\n",
    "#     counts_target = df[\"target\"].value_counts()\n",
    "#     total_bads = counts_target[0]\n",
    "#     total_goods = counts_target[1]\n",
    "\n",
    "#     total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "#     total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "#     pb = total_bads / (total_bads + total_goods)\n",
    "#     pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "#     # Calculate kickout metric\n",
    "#     kickout = (((kb / pb) - (kg / (1 - pb))) / sb) * (pb * pb)\n",
    "#     kickin = (((ig / pg) - (ib / (1 - pg))) / sg) * (pg * pg)\n",
    "#     weighted_total = kickout + kickin\n",
    "\n",
    "#     return weighted_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_df_beforeRI(df):\n",
    "\n",
    "    # Flag kicked out bad cases (want more of these)\n",
    "    if df[\"target\"] == 1 and df[\"prediction_beforeRI_binary\"] == 1:\n",
    "        return \"CB\"\n",
    "\n",
    "    # Flag kicked out good cases (want less of these)\n",
    "    elif df[\"target\"] == 1 and df[\"prediction_beforeRI_binary\"] == 0:\n",
    "        return \"IB\"\n",
    "\n",
    "    # Flag kicked in good cases (want more of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_beforeRI_binary\"] == 0:\n",
    "        return \"CG\"\n",
    "\n",
    "    # Flag kicked in bad cases (want less of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_beforeRI_binary\"] == 1:\n",
    "        return \"IG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kickout_beforeRI(df):\n",
    "\n",
    "    # Counts of kickout bad and kickout good\n",
    "    counts = df[\"Flag\"].value_counts()\n",
    "    if \"CB\" in df.values:\n",
    "        cb = counts.CB  # want more of these\n",
    "    else:\n",
    "        cb = 0\n",
    "    if \"IB\" in df.values:\n",
    "        ib = counts.IB  # want less of these\n",
    "    else:\n",
    "        ib = 0\n",
    "\n",
    "    if \"CG\" in df.values:\n",
    "        cg = counts.CG  # want more of these\n",
    "    else:\n",
    "        cg = 0\n",
    "\n",
    "    if \"IG\" in df.values:\n",
    "        ig = counts.IG  # want less of these\n",
    "    else:\n",
    "        ig = 0\n",
    "\n",
    "    # Target\n",
    "    total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "    total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "    pb = total_bads / (total_bads + total_goods)\n",
    "    pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\n",
    "    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\n",
    "    weighted_total = kickout + kickin\n",
    "    return weighted_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_df_baseline(df):\n",
    "\n",
    "    # Flag kicked out bad cases (want more of these)\n",
    "    if df[\"target\"] == 1 and df[\"prediction_baseline\"] == 1:\n",
    "        return \"CB\"\n",
    "\n",
    "    # Flag kicked out good cases (want less of these)\n",
    "    elif df[\"target\"] == 1 and df[\"prediction_baseline\"] == 0:\n",
    "        return \"IB\"\n",
    "\n",
    "    # Flag kicked in good cases (want more of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_baseline\"] == 0:\n",
    "        return \"CG\"\n",
    "\n",
    "    # Flag kicked in bad cases (want less of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_baseline\"] == 1:\n",
    "        return \"IG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kickout_baseline(df):\n",
    "\n",
    "    # Counts of kickout bad and kickout good\n",
    "    counts = df[\"Flag\"].value_counts()\n",
    "    if \"CB\" in df.values:\n",
    "        cb = counts.CB  # want more of these\n",
    "    else:\n",
    "        cb = 0\n",
    "    if \"IB\" in df.values:\n",
    "        ib = counts.IB  # want less of these\n",
    "    else:\n",
    "        ib = 0\n",
    "\n",
    "    if \"CG\" in df.values:\n",
    "        cg = counts.CG  # want more of these\n",
    "    else:\n",
    "        cg = 0\n",
    "\n",
    "    if \"IG\" in df.values:\n",
    "        ig = counts.IG  # want less of these\n",
    "    else:\n",
    "        ig = 0\n",
    "\n",
    "    # Target\n",
    "    total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "    total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "    pb = total_bads / (total_bads + total_goods)\n",
    "    pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\n",
    "    kickin = (((cg / pg) - (ig /  pg)) / total_goods) * (pg ** 2)\n",
    "    weighted_total = kickout + kickin\n",
    "    return weighted_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_df_ssl(df):\n",
    "\n",
    "    # Flag kicked out bad cases (want more of these)\n",
    "    if df[\"target\"] == 1 and df[\"prediction_ssl\"] == 1:\n",
    "        return \"CB\"\n",
    "\n",
    "    # Flag kicked out good cases (want less of these)\n",
    "    elif df[\"target\"] == 1 and df[\"prediction_ssl\"] == 0:\n",
    "        return \"IB\"\n",
    "\n",
    "    # Flag kicked in good cases (want more of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_ssl\"] == 0:\n",
    "        return \"CG\"\n",
    "\n",
    "    # Flag kicked in bad cases (want less of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_ssl\"] == 1:\n",
    "        return \"IG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kickout_ssl(df):\n",
    "\n",
    "    # Counts of kickout bad and kickout good\n",
    "    counts = df[\"Flag\"].value_counts()\n",
    "    if \"CB\" in df.values:\n",
    "        cb = counts.CB  # want more of these\n",
    "    else:\n",
    "        cb = 0\n",
    "    if \"IB\" in df.values:\n",
    "        ib = counts.IB  # want less of these\n",
    "    else:\n",
    "        ib = 0\n",
    "\n",
    "    if \"CG\" in df.values:\n",
    "        cg = counts.CG  # want more of these\n",
    "    else:\n",
    "        cg = 0\n",
    "\n",
    "    if \"IG\" in df.values:\n",
    "        ig = counts.IG  # want less of these\n",
    "    else:\n",
    "        ig = 0\n",
    "\n",
    "    # Target\n",
    "    total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "    total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "    pb = total_bads / (total_bads + total_goods)\n",
    "    pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "    kickout = (((cb / pb) - (ib /  pb)) / total_bads) * (pb ** 2)\n",
    "    kickin = (((cg / pg) - (ig /  pg)) / total_goods) * (pg ** 2)\n",
    "    weighted_total = kickout + kickin\n",
    "    return weighted_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
