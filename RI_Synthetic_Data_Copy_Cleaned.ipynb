{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    r\"C:\\Users\\Asus\\Desktop\\Repo\\MasterThesis_RI\\Python-Real-World-Machine-Learning\\Module 2\\Chapter 5\",\n",
    ")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Basic Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelling\n",
    "# Classification\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Semi-Supervised Learning\n",
    "from sklearn.semi_supervised import (\n",
    "    LabelPropagation,\n",
    "    LabelSpreading,\n",
    "    SelfTrainingClassifier,\n",
    ")\n",
    "\n",
    "# Chapter 5\n",
    "from SelfLearning import SelfLearningModel\n",
    "from scikitWQDA import WQDA\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "# Ensembling\n",
    "from sklearn.ensemble import *\n",
    "\n",
    "# Balancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Binning\n",
    "import woeBinningPandas\n",
    "\n",
    "# Create Unique ID\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, accepted_flag, target, train_ratio):\n",
    "    \"\"\"\n",
    "    The goal of this function is to load the original dataset, split it into accepts and rejects,\n",
    "    add ids, which can later be used for merging. For the rejects to further perform train / test split for the rejects\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : name of the original dataset in quotation marks, csv format\n",
    "    accepted_flag: name of the accepted flag; Binary: 1 if accepted, 0 if rejected\n",
    "    target : name of the target column\n",
    "    train_ratio : percentage used for training; Continuous (0,1)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a : accepted data\n",
    "    r : rejected data\n",
    "    r_dev : rejected trainining data\n",
    "    r_test : rejected testing data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(df)\n",
    "\n",
    "    # Accepted\n",
    "\n",
    "    ## Create separate dataset with accepts\n",
    "    dfa = data[data[accepted_flag] == 1]\n",
    "    dfa = dfa.drop([accepted_flag], axis=1)\n",
    "    ## Rename target variable as \"target\"\n",
    "    dfa = dfa.rename(columns={target: \"target\"})\n",
    "    ## Add id to the dataset, which can later be used for merging\n",
    "    dfa[\"id\"] = dfa.index.to_series().map(lambda x: uuid.uuid4())\n",
    "\n",
    "    # Rejected\n",
    "\n",
    "    ## Create separate dataset with accepts\n",
    "    dfr = data[data[accepted_flag] == 0]\n",
    "    dfr = dfr.drop([accepted_flag], axis=1)\n",
    "    ## Add id to the dataset, which can later be used for merging\n",
    "    dfr[\"id\"] = dfr.index.to_series().map(lambda x: uuid.uuid4())\n",
    "    ## Train/Test Split (without labels)\n",
    "    ### Shuffle the dataset\n",
    "    shuffle_df = dfr.sample(frac=1)\n",
    "    ### Define a size for the train set\n",
    "    train_size = int(train_ratio * len(shuffle_df))\n",
    "    ### Split the dataset\n",
    "    dfr_dev = shuffle_df[:train_size]\n",
    "    dfr_test = shuffle_df[train_size:]\n",
    "    ## Unlabel the rejects (i.e. drop the target)\n",
    "    dfr_dev2 = dfr_dev\n",
    "    dfr_test2 = dfr_test\n",
    "    dfr_dev2 = dfr_dev2.drop([target], axis=1)\n",
    "    dfr_test2 = dfr_test2.drop([target], axis=1)\n",
    "\n",
    "    return dfa, dfr, dfr_dev2, dfr_dev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, r, r_dev, r_test = data_preprocessing(\"model_ds.csv\", \"is_accepted\", \"y\", 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.shape)  # accepted\n",
    "print(r.shape)  # rejected\n",
    "print(r_dev.shape)  # rejected train\n",
    "print(r_test.shape)  # rejected test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rejects datasets with the modelling columns only (for a dataset with 8 features)\n",
    "r_dev_mod = r_dev.iloc[:, :9]\n",
    "r_test_mod = r_test.iloc[:, :9]\n",
    "# Create rejects datasets with the modelling columns + id\n",
    "r_dev_mod_id = r_dev.iloc[:, :10]\n",
    "r_dev_mod_id = r_test.iloc[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "model_ds = pd.read_csv(\"model_ds.csv\")\n",
    "\n",
    "# Accepted\n",
    "df3 = model_ds[model_ds[\"is_accepted\"] == 1]\n",
    "df3 = df3.drop([\"is_accepted\"], axis=1)\n",
    "df3 = df3.rename(columns={\"y\": \"target\"})\n",
    "df3[\"id\"] = df3.index.to_series().map(lambda x: uuid.uuid4())\n",
    "# Rejected\n",
    "dfr = model_ds[model_ds[\"is_accepted\"] == 0]\n",
    "dfr = dfr.drop([\"is_accepted\"], axis=1)\n",
    "\n",
    "# Create ids for rejects\n",
    "dfr[\"id\"] = dfr.index.to_series().map(lambda x: uuid.uuid4())\n",
    "\n",
    "# Train/Test Split (without labels)\n",
    "# Shuffle the dataset\n",
    "shuffle_df = dfr.sample(frac=1)\n",
    "\n",
    "# Define a size for your train set\n",
    "train_size = int(0.7 * len(shuffle_df))\n",
    "\n",
    "# Split your dataset\n",
    "dfr_dev3 = shuffle_df[:train_size]\n",
    "dfr_test3 = shuffle_df[train_size:]\n",
    "\n",
    "# Drop y\n",
    "dfr_dev2 = dfr_dev3\n",
    "dfr_test2 = dfr_test3\n",
    "dfr_dev3 = dfr_dev3.drop([\"y\"], axis=1)\n",
    "dfr_test3 = dfr_test3.drop([\"y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_accepts(accepted_data):\n",
    "    \"\"\"\n",
    "    Perform Undersampling and Split the data into training and testing sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    accepted_data : accepts, dataframe\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    os_data_X_2 : undersampled training modelling fields\n",
    "    X_test_2 : undersampled test modelling fields\n",
    "    y_train : undersampled training labels\n",
    "    y_test : undersampled testing labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create X and y\n",
    "    X = accepted_data.loc[:, accepted_data.columns != \"target\"]\n",
    "    y = accepted_data.loc[:, accepted_data.columns == \"target\"]\n",
    "\n",
    "    # Train Test Split and Balance data\n",
    "    os = RandomUnderSampler(sampling_strategy=0.5, random_state=7)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=7\n",
    "    )\n",
    "    columns = X_train.columns\n",
    "\n",
    "    # Train\n",
    "    os_data_X, os_data_y = os.fit_sample(X_train, y_train)\n",
    "    os_data_X = pd.DataFrame(data=os_data_X, columns=columns)\n",
    "    os_data_y = pd.DataFrame(data=os_data_y, columns=[\"target\"])\n",
    "\n",
    "    # Match names with Lending Club Data\n",
    "    os_data_X_2 = os_data_X\n",
    "    X_test_2 = X_test\n",
    "    return os_data_X_2, X_test_2, os_data_y, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_data_X_2, X_test_2, os_data_y, y_test = split_accepts(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os_data_X_2.shape)\n",
    "print(X_test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection of columns below is subject to iteration based on the modelling outcomes from the logistic regression, i.e. significance (p-values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_columns = [\n",
    "    \"known_col_0\",\n",
    "    \"known_col_1\",\n",
    "    \"known_col_3\",\n",
    "    \"known_col_4\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_data_X_2 = os_data_X_2[significant_columns]\n",
    "X_test_3 = X_test_2[significant_columns]\n",
    "r_dev_mod = r_dev_mod[significant_columns]\n",
    "r_test_mod = r_test_mod[significant_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build Logistic regression\n",
    "# Statmodels\n",
    "X_in = sm.add_constant(os_data_X_2.astype(float))\n",
    "logit_model = sm.Logit(os_data_y, X_in)\n",
    "result3 = logit_model.fit()\n",
    "print(result3.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "logreg = LogisticRegression(fit_intercept=True, penalty=\"none\")\n",
    "logreg.fit(os_data_X_2, os_data_y.values.ravel())\n",
    "y_pred = logreg.predict(X_test_3)\n",
    "print(\"Accuracy score Logistic Regression:\", logreg.score(X_test_3, y_test))\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test_3))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test_3)[:, 1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"Log_ROC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set with labels\n",
    "test_labels = pd.merge(\n",
    "    y_test,\n",
    "    X_test_2,\n",
    "    how=\"inner\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on testset\n",
    "test_pred = logreg.predict_proba(X_test_3)[:, 1]\n",
    "test_pred2 = pd.DataFrame(data=test_pred, columns=[\"prediction\"])\n",
    "test_pred2[\"count\"] = test_pred2.groupby(\"prediction\")[\"prediction\"].transform(\"count\")\n",
    "test_pred2.groupby([\"prediction\"]).count()\n",
    "test_pred2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join predictions with test new\n",
    "pred_test_kgb = pd.DataFrame(\n",
    "    data=test_pred, columns=[\"prediction_beforeRI\"], index=y_test.index.copy()\n",
    ")\n",
    "pred_test1 = pd.merge(\n",
    "    test_labels,\n",
    "    pred_test_kgb[[\"prediction_beforeRI\"]],\n",
    "    how=\"inner\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "# pred_test1.dropna(subset=[\"prediction_beforeRI\"], inplace=True)\n",
    "pred_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test1.shape  # Expected nr. of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metrics(pred_label, true_label, model):\n",
    "    \"\"\"\"\"\n",
    "    pred_label = predicted label of the model\n",
    "    true_label = true label\n",
    "    model = model name\n",
    "    \"\"\" \"\"\n",
    "\n",
    "    # F1 score\n",
    "    f1_stat = f1_score(pred_label, true_label, average=\"weighted\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(pred_label, true_label, labels=model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    fig = disp.plot()\n",
    "\n",
    "    return print(\"F1_stat \", model, \"is: \", f1_stat, fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rf.fit(os_data_X_2, os_data_y.values.ravel())\n",
    "y_pred = rf.predict(X_test_3)\n",
    "all_metrics(y_pred, y_test, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(os_data_X_2, os_data_y.values.ravel())\n",
    "y_pred = dt.predict(X_test_3)\n",
    "all_metrics(y_pred, y_test, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC()\n",
    "svm.fit(os_data_X_2, os_data_y.values.ravel())\n",
    "y_pred = svm.predict(X_test_3)\n",
    "all_metrics(y_pred, y_test, svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.LGBMClassifier()\n",
    "lgbm.fit(os_data_X_2, os_data_y.values.ravel())\n",
    "y_pred = lgbm.predict(X_test_3)\n",
    "all_metrics(y_pred, y_test, lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions1 uses predict_proba; predictions 2 uses predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions1(model, treshold):\n",
    "    # Join predictions with train new\n",
    "    pred = model.predict_proba(r_dev_mod)[:, 1]\n",
    "    pred2 = pd.DataFrame(\n",
    "        data=pred,\n",
    "        columns=[\"prediction2\"],\n",
    "        index=r_dev_mod_id.index.copy(),\n",
    "    )\n",
    "    pred2[\"prediction_beforeRI\"] = pred2[\"prediction2\"].apply(\n",
    "        lambda x: 0 if (x < treshold) else 1\n",
    "    )\n",
    "    outcome = pd.merge(\n",
    "        r_dev_mod_id,\n",
    "        pred2[[\"prediction_beforeRI\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    # pred_test1.dropna(subset=[\"prediction_beforeRI\"], inplace=True)\n",
    "    outcome = outcome[[\"id\", \"prediction_beforeRI\"]]\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions2(model, treshold):\n",
    "    # Join predictions with train new\n",
    "    pred = model.predict(r_dev_mod)\n",
    "    pred2 = pd.DataFrame(\n",
    "        data=pred,\n",
    "        columns=[\"prediction2\"],\n",
    "        index=r_dev_mod_id.index.copy(),\n",
    "    )\n",
    "    pred2[\"prediction_beforeRI\"] = pred2[\"prediction2\"].apply(\n",
    "        lambda x: 0 if (x < treshold) else 1\n",
    "    )\n",
    "    outcome = pd.merge(\n",
    "        r_dev_mod_id,\n",
    "        pred2[[\"prediction_beforeRI\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    # pred_test1.dropna(subset=[\"prediction_beforeRI\"], inplace=True)\n",
    "    outcome = outcome[[\"id\", \"prediction_beforeRI\"]]\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri1_train = predictions1(logreg, 0.1)  # Logistic Regression\n",
    "ri2_train = predictions1(dt, 0.1)  # Decision Tree\n",
    "ri3_train = predictions1(rf, 0.1)  # Random Forest\n",
    "ri4_train = predictions2(svm, 0.1)  # SVM\n",
    "ri5_train = predictions2(lgbm, 0.1)  # Light GBM ranking 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(data, state, prediction):\n",
    "    print(\n",
    "        \"The number of accurately classified cases \",\n",
    "        state,\n",
    "        \" is: \",\n",
    "        data[\n",
    "            (data.target == 1) & (data[prediction] == 1)\n",
    "            | (data.target == 0) & (data[prediction] == 0)\n",
    "        ].shape[0],\n",
    "    )\n",
    "    print(\n",
    "        \"The number of misclassified cases \",\n",
    "        state,\n",
    "        \" is: \",\n",
    "        data[\n",
    "            (data.target == 1) & (data[prediction] == 0)\n",
    "            | (data.target == 0) & (data[prediction] == 1)\n",
    "        ].shape[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(ri_data):  # ri1_train, ri2_train, etc..\n",
    "\n",
    "    # TRAIN NEW\n",
    "    # Join labels to train set\n",
    "    # Accepts\n",
    "    train_accepts = pd.merge(\n",
    "        os_data_X_2,\n",
    "        os_data_y[[\"target\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    train_accepts[\"Flag1\"] = \"Accept\"\n",
    "\n",
    "    # Rejects\n",
    "    train_rejects = pd.merge(\n",
    "        r_dev_mod,\n",
    "        ri_data[[\"prediction_beforeRI\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Drop Null values and align naming\n",
    "    train_accepts.dropna(subset=[\"target\"], inplace=True)\n",
    "    train_rejects.rename(columns={\"prediction_beforeRI\": \"target\"}, inplace=True)\n",
    "\n",
    "    # Concatenate Train Accepts and Train Rejects\n",
    "    train_new = pd.concat([train_accepts, train_rejects])\n",
    "\n",
    "    # Flag\n",
    "    train_new[\"Flag\"] = train_new[\"Flag1\"].apply(\n",
    "        lambda x: \"Accept\" if x == \"Accept\" else \"Reject\"\n",
    "    )\n",
    "    train_new = train_new.drop(columns=[\"Flag1\"])\n",
    "\n",
    "    # Retrain KGB Model\n",
    "\n",
    "    # Split\n",
    "    X_new = train_new.loc[:, train_new.columns != \"target\"]\n",
    "    X_new = X_new.loc[:, X_new.columns != \"Flag\"]\n",
    "    y_new = train_new.loc[:, train_new.columns == \"target\"]\n",
    "    X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "        X_new, y_new, test_size=0.3, random_state=7\n",
    "    )\n",
    "\n",
    "    # Keep only columns for modelling\n",
    "    os_data_X_2_new = X_train_new[significant_columns]\n",
    "    X_test_2_new = X_test_new[significant_columns]\n",
    "\n",
    "    # Build Logistic regression\n",
    "    logreg = LogisticRegression(fit_intercept=False, penalty=\"none\")\n",
    "    logreg.fit(os_data_X_2_new, y_train_new.values.ravel())\n",
    "\n",
    "    # TEST NEW\n",
    "    # Join labels to test set\n",
    "    # Accepts\n",
    "    test_accepts = pd.merge(\n",
    "        X_test_2, y_test[[\"target\"]], how=\"left\", left_index=True, right_index=True\n",
    "    )\n",
    "\n",
    "    test_accepts[\"Flag1\"] = \"Accept\"\n",
    "\n",
    "    # Rejects\n",
    "    test_rejects = pd.merge(\n",
    "        r_dev_mod,\n",
    "        ri_data[[\"prediction_beforeRI\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Drop Null values and align naming\n",
    "    test_accepts.dropna(subset=[\"target\"], inplace=True)\n",
    "    test_rejects.rename(columns={\"prediction_beforeRI\": \"target\"}, inplace=True)\n",
    "    test_rejects.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # Concatenate Test Accepts and Test Rejects\n",
    "    test_new = pd.concat([test_accepts, test_rejects])\n",
    "\n",
    "    # Flag\n",
    "    test_new[\"Flag\"] = test_new[\"Flag1\"].apply(\n",
    "        lambda x: \"Accept\" if x == \"Accept\" else \"Reject\"\n",
    "    )\n",
    "\n",
    "    test_new = test_new.drop(columns=[\"Flag1\"])\n",
    "\n",
    "    test_new = test_new.reset_index(drop=True)\n",
    "\n",
    "    X_test_new = test_new.loc[:, test_new.columns != \"target\"]\n",
    "    X_test_new2 = X_test_new.loc[:, X_test_new.columns != \"Flag\"]\n",
    "    X_test_new4 = X_test_new2.loc[:, X_test_new2.columns != \"id\"]\n",
    "\n",
    "    X_test_new5 = X_test_new4[significant_columns]\n",
    "\n",
    "    y_test_new = test_new.loc[:, test_new.columns == \"target\"]\n",
    "\n",
    "    # PREDICTIONS\n",
    "    # Join predictions with test new\n",
    "    test_new_pred = logreg.predict_proba(X_test_new5)[:, 1]\n",
    "    pred_test_df = pd.DataFrame(\n",
    "        data=test_new_pred, columns=[\"prediction_afterRI\"], index=test_new.index.copy()\n",
    "    )\n",
    "    pred_test = pd.merge(\n",
    "        test_new,\n",
    "        pred_test_df[[\"prediction_afterRI\"]],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    # pred_test1.dropna(subset=[\"prediction_beforeRI\"], inplace=True)\n",
    "    pred_test\n",
    "\n",
    "    a1 = pred_test1[[\"id\", \"target\", \"prediction_beforeRI\"]]\n",
    "    a2 = pred_test[[\"id\", \"Flag\", \"prediction_afterRI\"]]\n",
    "\n",
    "    # Join (outer to get full sample)\n",
    "    a1_a2_outer = pd.merge(a1, a2, how=\"outer\", on=\"id\")\n",
    "    # Join (inner to get only accepts and be able to compare)\n",
    "    a1_a2_inner = pd.merge(a1, a2, how=\"inner\", on=\"id\")\n",
    "    a1_a2_inner\n",
    "\n",
    "    # Make binary predictions based on cutoff 0.3\n",
    "    a1_a2_inner[\"prediction_beforeRI_binary\"] = a1_a2_inner[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(lambda x: 0 if (x < 0.3) else 1)\n",
    "    a1_a2_inner[\"prediction_afterRI_binary\"] = a1_a2_inner[\"prediction_afterRI\"].apply(\n",
    "        lambda x: 0 if (x < 0.3) else 1\n",
    "    )\n",
    "\n",
    "    print_results(a1_a2_inner, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    print_results(a1_a2_inner, \"after RI\", \"prediction_afterRI_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(ri1_train)  # Simple Augmentation - Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(ri2_train)  # Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(ri3_train)  # Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(ri4_train)  # SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(ri5_train)  # LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on existing literature semi-supervised learning are chosen for RI. The method is suitable for the problem because the labels of the accepted population (good/bad) are known and the labels of the rejected population are unknown. Without ignoring the inherent bias between accepts and rejects, semi-supervised methods use both labelled and unlabelled data during fit. <br> **1. Data preparation:** the goal is to create initial dataframe, which contains the known training data and training labels of the accepts and the known training data of the rejects. The training labels of the rejects are unknown, and are therefore labelled with a default value of -1. <br> **2. Train/Test Split** The resulting dataset is again split into explanatory variables and target in order to fit the model. <br> **3. Fit model:** The semi-supervised model is fit <br> **4. Predictions:** Predictions are made using the known testing data and testing labels of the accepts <br> **5. Evaluation:**: The results of the model before and after reject inference are compared  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_prep(X_accept, y_accept, X_reject):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X_train_acc : training data of accepted population\n",
    "    y_train_acc: training lables of accepted population\n",
    "    X_train_rej: training data of rejected population\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df : data of accepted and rejected population\n",
    "\n",
    "    \"\"\"\n",
    "    # Merge explanatory and target in accepts\n",
    "    accepts = pd.merge(\n",
    "        X_accept, y_accept, how=\"left\", left_index=True, right_index=True\n",
    "    )\n",
    "    # Create accept flag\n",
    "    accepts[\"Flag1\"] = \"Accept\"\n",
    "\n",
    "    # Merge accepts and rejects\n",
    "    df = pd.concat([accepts, X_reject])\n",
    "\n",
    "    # If accepted use accept label, if rejected use -1 (default value for unlabelled entries) - hard-coded for now\n",
    "    conditions = [\n",
    "        (df[\"Flag1\"] == \"Accept\") & (df[\"target\"] == 1),\n",
    "        (df[\"Flag1\"] == \"Accept\") & (df[\"target\"] == 0),\n",
    "    ]\n",
    "    choices = [1, 0]\n",
    "\n",
    "    # New target is called unlabel\n",
    "    df[\"unlabel\"] = np.select(conditions, choices, -1)\n",
    "\n",
    "    # Select columns for modelling - hard-coded for now - can be moved outside of the function\n",
    "    df = df[[\"known_col_0\", \"known_col_1\", \"known_col_3\", \"known_col_4\", \"unlabel\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_split(df, target):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : dataframe of accepted and rejected population, including data and labels\n",
    "    target: string name of the target column, should be passed in quotation marks (e.g. \"target\")\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    X_train: training data of accepted and rejected population, ready to be fed into the semi-supervised model\n",
    "    y_train: training labels of accepted and rejected population, ready to be fed into the semi-supervised model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = df.loc[:, df.columns != target]\n",
    "    y_train = df.loc[:, df.columns == target]\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_model_selftraining(X_train, y_train, model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X_train : training data of accepted and rejected population\n",
    "    y_train : training lables of accepted population (0,1) and rejected population (-1)\n",
    "    model : semi-supervised learning model from sklearn (Self-Training Classifier)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    ssl: trained semi-supervised learning model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit SSL moodel\n",
    "    base = SVC(probability=True, gamma=\"auto\")\n",
    "    model = model(base)\n",
    "    labels = np.copy(y_train)\n",
    "    data = np.copy(X_train)\n",
    "    ssl = model.fit(data, labels)\n",
    "    return ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_model_label(X_train, y_train, model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X_train : training data of accepted and rejected population\n",
    "    y_train : training lables of accepted population (0,1) and rejected population (-1)\n",
    "    model : semi-supervised learning model from sklearn (Label Propagation, Label Spreading)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    ssl: trained semi-supervised learning model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit SSL moodel\n",
    "    model = model()\n",
    "    labels = np.copy(y_train)\n",
    "    data = np.copy(X_train)\n",
    "    ssl = model.fit(data, labels)\n",
    "    return ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_predictions(ssl, X_test):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ssl : trained semi-supervised learning model\n",
    "    X_test : testing data of accepted and rejected population for predictions\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    Predictions before RI (binary)\n",
    "    Predictions after RI (binary)\n",
    "\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = ssl.predict(X_test)\n",
    "    # Convert y_pred array to pandas dataframe\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        columns=[\"prediction_afterRI\"],\n",
    "        index=X_test.index.copy(),\n",
    "    )\n",
    "    a1 = pred_test1[[\"id\", \"target\", \"prediction_beforeRI\"]]  # hard-coded for now\n",
    "    a2 = pred_test[[\"prediction_afterRI\"]]  # hard-coded for now\n",
    "\n",
    "    # Merge a1 and a2\n",
    "    a1_a2_inner = pd.merge(\n",
    "        a1,\n",
    "        a2,\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make final prediction\n",
    "    a1_a2_inner[\"prediction_beforeRI_binary\"] = a1_a2_inner[\n",
    "        \"prediction_beforeRI\"\n",
    "    ].apply(\n",
    "        lambda x: 0 if (x < 0.3) else 1\n",
    "    )  # hard-coded to 0.3 for now\n",
    "    a1_a2_inner[\"prediction_afterRI_binary\"] = a1_a2_inner[\"prediction_afterRI\"].apply(\n",
    "        lambda x: 0 if (x < 0.3) else 1  # hard-coded to 0.3 for now\n",
    "    )\n",
    "\n",
    "    print_results(a1_a2_inner, \"before RI\", \"prediction_beforeRI_binary\")\n",
    "    print_results(a1_a2_inner, \"after RI\", \"prediction_afterRI_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSL Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_new_model = ssl_prep(\n",
    "    os_data_X_2,\n",
    "    os_data_y,\n",
    "    r_dev_mod,\n",
    ")\n",
    "# Test\n",
    "test_new_model = ssl_prep(\n",
    "    X_test_3,\n",
    "    y_test,\n",
    "    r_test_mod,\n",
    ")\n",
    "X_ssl, y_ssl = ssl_split(train_new_model, \"unlabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSL Models & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_training = ssl_model_selftraining(X_ssl, y_ssl, SelfTrainingClassifier)\n",
    "ssl_predictions(self_training, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_spreading = ssl_model_label(X_ssl, y_ssl, LabelSpreading)\n",
    "ssl_predictions(label_spreading, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_propagation = ssl_model_label(X_ssl, y_ssl, LabelPropagation)\n",
    "ssl_predictions(label_propagation, X_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    (\"lp\", label_propagation),\n",
    "    (\"lsp\", label_spreading),\n",
    "    (\"st\", self_training),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling Models and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Classifier\n",
    "final_estimator = GradientBoostingClassifier(\n",
    "    n_estimators=5, subsample=0.5, min_samples_leaf=25, max_features=1, random_state=42\n",
    ")\n",
    "sc = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n",
    "# Hard Voting Classifier\n",
    "hvc = VotingClassifier(estimators=estimators, voting=\"hard\")\n",
    "# Soft Voting Classifier\n",
    "soft_vc = VotingClassifier(estimators=estimators, voting=\"soft\", weights=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_classifier = sc.fit(X_ssl, y_ssl)\n",
    "ssl_predictions(stacking_classifier, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_voting_classifier = hvc.fit(X_ssl, y_ssl)\n",
    "ssl_predictions(hard_voting_classifier, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_voting_classifier = hvc.fit(X_ssl, y_ssl)\n",
    "ssl_predictions(soft_voting_classifier, X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
