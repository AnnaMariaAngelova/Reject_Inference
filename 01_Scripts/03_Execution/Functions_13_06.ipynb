{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Basic Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelling\n",
    "# Classification\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# from sklearn import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import f1_score, log_loss, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, accepted_flag, target, train_ratio):\n",
    "    \"\"\"\n",
    "    The goal of this function is to load the original dataset, split it into accepts and rejects,\n",
    "    add ids, which can later be used for merging. For the rejects to further perform train / test split\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : name of the original dataset in quotation marks, csv format\n",
    "    accepted_flag: name of the accepted flag; Binary: 1 if accepted, 0 if rejected\n",
    "    target : name of the target column\n",
    "    train_ratio : percentage used for training; Continuous (0,1)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a : accepted data\n",
    "    r : rejected data\n",
    "    r_dev : rejected trainining data without label\n",
    "    r_test : rejected testing data without label\n",
    "    dfr_dev_with_label: rejected training data with label\n",
    "    dft_test_with_label: rejected training data with label\n",
    "\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(\"C:/Users/Asus/Desktop/Repo/MasterThesis_RI/Data_13_06/\" + df)\n",
    "\n",
    "    # Accepted\n",
    "\n",
    "    ## Create separate dataset with accepts\n",
    "    dfa = data[data[accepted_flag] == 1]\n",
    "    dfa = dfa.drop([accepted_flag], axis=1)\n",
    "    ## Rename target variable as \"target\"\n",
    "    dfa = dfa.rename(columns={target: \"target\"})\n",
    "    ## Add id to the dataset, which can later be used for merging\n",
    "    # dfa[\"id\"] = dfa.index.to_series().map(lambda x: uuid.uuid4())\n",
    "\n",
    "    # Rejected\n",
    "\n",
    "    ## Create separate dataset with accepts\n",
    "    dfr = data[data[accepted_flag] == 0]\n",
    "    dfr = dfr.drop([accepted_flag], axis=1)\n",
    "    ## Add id to the dataset, which can later be used for merging\n",
    "    #     dfr[\"id\"] = dfr.index.to_series().map(lambda x: uuid.uuid4())\n",
    "    ## Train/Test Split (without labels)\n",
    "    ### Shuffle the dataset\n",
    "    shuffle_df = dfr.sample(frac=1, random_state=42)\n",
    "    ### Define a size for the train set\n",
    "    train_size = int(train_ratio * len(shuffle_df))\n",
    "    ### Split the dataset\n",
    "    dfr_dev = shuffle_df[:train_size]\n",
    "    dfr_test = shuffle_df[train_size:]\n",
    "    ## Save a copy of the rejected data with label\n",
    "    dfr_dev_with_label = dfr_dev\n",
    "    dfr_test_with_label = dfr_test\n",
    "    ## Unlabel the rejects (i.e. drop the target) and save a copy of the rejeted data without label\n",
    "    dfr_dev2 = dfr_dev_with_label.drop([target], axis=1)\n",
    "    dfr_test2 = dfr_test_with_label.drop([target], axis=1)\n",
    "    # Rename target variable\n",
    "    dfr_dev_with_label = dfr_dev_with_label.rename(columns={target: \"target\"})\n",
    "    dfr_test_with_label = dfr_test_with_label.rename(columns={target: \"target\"})\n",
    "\n",
    "    return dfr_dev_with_label, dfr_test_with_label, dfa, dfr, dfr_dev2, dfr_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_y(data):\n",
    "    \"\"\"\n",
    "    Undersample the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Dataframe\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    X_res : undersampled data; Dataframe\n",
    "    y_res : undersampled labels; Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    # Create X and y\n",
    "    X = data.loc[:, data.columns != \"target\"]\n",
    "    y = data.loc[:, data.columns == \"target\"]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssl():\n",
    "    #Make copies of the dataframes\n",
    "    X_train_iter = X_train.copy()\n",
    "    y_train_iter = y_train.copy()\n",
    "    r_dev_iter = r_dev.copy()\n",
    "    \n",
    "    f1_scores = []\n",
    "    log_losses = []\n",
    "    mccs = []\n",
    "    iterations = []\n",
    "\n",
    "    iteration = 0\n",
    "    while (1 - 2 * dr) * len(r_dev) < len(r_dev_iter):\n",
    "        iteration = iteration + 1\n",
    "        print(\"Iteration Nr {}\".format(iteration))\n",
    "        print(len(r_dev_iter))\n",
    "        # Build logistic regression\n",
    "        # KGB1 = LogisticRegression(fit_intercept=False, penalty=\"none\").fit(X_train_iter, y_train_iter)\n",
    "        KGB_new = RandomForestClassifier(random_state=42).fit(X_train_iter, y_train_iter)\n",
    "        # KGB1 = LinearRegression().fit(X_train_iter, y_train_iter)\n",
    "        # KGB1 = LGBMClassifier().fit(X_train_iter, y_train_iter)\n",
    "        # KGB1 = DecisionTreeClassifier().fit(X_train_iter, y_train_iter)\n",
    "\n",
    "        # Scores\n",
    "        # mcc = f1_score(y_test, KGB1.predict(X_test), average=\"weighted\")\n",
    "        # mccs.append(mcc)\n",
    "\n",
    "        #     logloss = log_loss(y_test, KGB1.predict(X_test), eps=1e-15)\n",
    "        #     log_losses.append(logloss)\n",
    "\n",
    "        #     mcc = roc_auc_score(y_test, KGB1.predict(X_test))\n",
    "        #     mccs.append(mcc)\n",
    "\n",
    "        mcc = matthews_corrcoef(y_test, KGB_new.predict(X_test))\n",
    "        mccs.append(mcc)\n",
    "\n",
    "        print(\"MCC: \", mcc)\n",
    "\n",
    "        # Make predictions on the rejected data\n",
    "        pred = KGB_new.predict_proba(r_dev_iter)[:, 1]\n",
    "        # pred = KGB1.predict(r_dev_iter)\n",
    "        pred = pd.DataFrame(\n",
    "            data=pred,\n",
    "            columns=[\"target\"],\n",
    "            index=r_dev_iter.index.copy(),\n",
    "        )\n",
    "\n",
    "        # Choose the most certain predictions\n",
    "        lq = pred[\"target\"].quantile(q=0.05)\n",
    "        uq = pred[\"target\"].quantile(q=0.95)\n",
    "        pred[\"certain\"] = pred[\"target\"].apply(lambda x: 1 if (x < lq or x > uq) else 0)\n",
    "        # pred[\"certain\"] = pred[\"target\"].apply(lambda x: 1 if (x > uq) else 0)\n",
    "\n",
    "        # If PD is high, apply default status\n",
    "        pred[\"target\"] = pred[\"target\"].apply(lambda x: 1 if (x > uq) else 0)\n",
    "        # pred[\"target\"] = pred[\"target\"].apply(lambda x: 1 if (x > np.random.uniform()) else 0)\n",
    "\n",
    "        # If PD is low, be conservative and apply non-default status only to some examples\n",
    "        #     pred[\"target\"] = pred[\"target\"].apply(\n",
    "        #         lambda x: 0 if (x < np.random.uniform()) else 1\n",
    "        #     )\n",
    "\n",
    "        # Pick only the certain predictions and concatenate them to the dev set\n",
    "        # Y TRAIN\n",
    "        certain = pred[pred[\"certain\"] == 1]\n",
    "        certain2 = certain[\"target\"].to_frame()\n",
    "        y_train_iter = pd.concat((y_train_iter, certain2))\n",
    "\n",
    "        # Get significant columns of the rejects based on index\n",
    "        certain_features = pd.merge(\n",
    "            certain[\"target\"],\n",
    "            r_dev_iter[significant_columns],\n",
    "            how=\"inner\",\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "\n",
    "        # X TRAIN\n",
    "        certain_features = certain_features.loc[:, certain_features.columns != \"target\"]\n",
    "        X_train_iter = pd.concat((X_train_iter, certain_features))\n",
    "\n",
    "        # Remove certain columns from rejected data\n",
    "        rows = certain_features.index\n",
    "        r_dev_iter = r_dev_iter.drop(rows, axis=\"index\")\n",
    "    return KGB_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iter():\n",
    "    X_train_iter1 = X_train.copy()\n",
    "    y_train_iter1 = y_train.copy()\n",
    "    r_dev_iter1 = r_dev.copy()\n",
    "    \n",
    "    f1_scores = []\n",
    "    mccs = []\n",
    "    iterations = []\n",
    "    # log_losses = []\n",
    "\n",
    "    for iteration in range(1, 2):  # Change to how many iterrations you like\n",
    "        print(\"Iteration Nr {}\".format(iteration))\n",
    "        # Build logistic regression\n",
    "        # KGB1 = LogisticRegression(fit_intercept=False, penalty=\"none\").fit(X_train_iter1, y_train_iter1)\n",
    "        # KGB1 = LinearRegression().fit(X_train_iter1, y_train_iter1)\n",
    "        KGB1 = RandomForestClassifier(random_state=42).fit(X_train_iter1, y_train_iter1)\n",
    "        # KGB1 = DecisionTreeClassifier().fit(X_train_iter, y_train_iter)\n",
    "\n",
    "        # Scores\n",
    "        #     f1_stat = f1_score(y_test, KGB1.predict(X_test), average=\"weighted\")\n",
    "        #     f1_scores.append(f1_stat)\n",
    "        #     print(\"F1: \", f1_stat)\n",
    "\n",
    "        #     logloss = log_loss(y_test, KGB1.predict(X_test), eps=1e-15)\n",
    "        #     log_losses.append(logloss)\n",
    "\n",
    "        #     mcc = roc_auc_score(y_test, KGB1.predict(X_test))\n",
    "        #     mccs.append(mcc)\n",
    "\n",
    "        mcc = matthews_corrcoef(y_test, KGB1.predict(X_test))\n",
    "        mccs.append(mcc)\n",
    "\n",
    "        print(\"MCC: \", mcc)\n",
    "\n",
    "        # Make predictions on the rejected data\n",
    "        pred = KGB1.predict_proba(r_dev_iter1)[:, 1]\n",
    "        # pred = KGB1.predict(r_dev_iter1)\n",
    "        pred = pd.DataFrame(\n",
    "            data=pred,\n",
    "            columns=[\"target\"],\n",
    "            index=r_dev_iter1.index.copy(),\n",
    "        )\n",
    "\n",
    "        # Choose the most certain predictions\n",
    "        lq = pred[\"target\"].quantile(q=0.05)\n",
    "        uq = pred[\"target\"].quantile(q=0.95)\n",
    "        # pred[\"certain\"] = pred[\"target\"].apply(lambda x: 1 if (x < lq or x > uq) else 0)\n",
    "        pred[\"certain\"] = pred[\"target\"].apply(lambda x: 1 if (x > uq) else 0)\n",
    "\n",
    "        # If PD is high, apply default status\n",
    "        pred[\"target\"] = pred[\"target\"].apply(lambda x: 1 if (x > uq) else 0)\n",
    "        # pred[\"target\"] = pred[\"target\"].apply(lambda x: 1 if (x > np.random.uniform()) else 0)\n",
    "\n",
    "        # If PD is low, be conservative and apply non-default status only to some examples\n",
    "        #     pred[\"target\"] = pred[\"target\"].apply(\n",
    "        #         lambda x: 0 if (x < np.random.uniform()) else 1\n",
    "        #     )\n",
    "\n",
    "        # Pick only the certain predictions and concatenate them to the dev set\n",
    "        # Y TRAIN\n",
    "        certain = pred[pred[\"certain\"] == 1]\n",
    "        certain2 = certain[\"target\"].to_frame()\n",
    "        y_train_iter1 = pd.concat((y_train_iter1, certain2))\n",
    "\n",
    "        # Get significant columns of the rejects based on index\n",
    "        certain_features = pd.merge(\n",
    "            certain[\"target\"],\n",
    "            r_dev_iter1[significant_columns],\n",
    "            how=\"inner\",\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "\n",
    "        # X TRAIN\n",
    "        certain_features = certain_features.loc[:, certain_features.columns != \"target\"]\n",
    "        X_train_iter1 = pd.concat((X_train_iter1, certain_features))\n",
    "\n",
    "        # Remove certain columns from rejected data\n",
    "        rows = certain_features.index\n",
    "        r_dev_iter1 = r_dev_iter1.drop(rows, axis=\"index\")\n",
    "        return KGB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rejects(model, r_dev):\n",
    "    # Make predictions on the Train Rejects\n",
    "    pred_test = model.predict_proba(r_dev)[:, 1]\n",
    "    # pred_test = model.predict(r_dev)\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=pred_test,\n",
    "        columns=[\"pred\"],\n",
    "        index=r_dev.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff DR\n",
    "    q1 = pred_test[\"pred\"].quantile(q=1 - conservative_dr)\n",
    "    pred_test[\"target\"] = pred_test[\"pred\"].apply(lambda x: 0 if (x < q1) else 1)\n",
    "    pred_test = pred_test[\"target\"].to_frame()\n",
    "\n",
    "    # Add new rows to df\n",
    "    y_train_new = pd.concat((y_train, pred_test))\n",
    "    X_train_new = pd.concat((X_train, r_dev))\n",
    "\n",
    "    # Fit new model\n",
    "    # KGB_baseline_new = LogisticRegression(fit_intercept=False, penalty=\"none\").fit(X_train_new, y_train_new)\n",
    "    # KGB_baseline_new = LinearRegression().fit(X_train_new, y_train_new)\n",
    "    KGB_baseline_new = RandomForestClassifier(random_state=42).fit(\n",
    "        X_train_new, y_train_new\n",
    "    )\n",
    "    # KGB_baseline_new = DecisionTreeClassifier().fit(X_train_iter, y_train_iter)\n",
    "    return KGB_baseline_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_accepts(model, X_test):\n",
    "    pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    # pred_test = model.predict(X_test)\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=pred_test,\n",
    "        columns=[\"pred\"],\n",
    "        index=X_test.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Merge with Target\n",
    "    pred_test2 = pd.merge(\n",
    "        pred_test[\"pred\"],\n",
    "        y_test[\"target\"],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff 50percentile of the distribution\n",
    "    q1 = pred_test2[\"pred\"].quantile(q=1 - conservative_dr)\n",
    "    pred_test2[\"prediction_baseline\"] = pred_test2[\"pred\"].apply(\n",
    "        lambda x: 0 if (x < q1) else 1\n",
    "    )\n",
    "    return pred_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_rejects(model, r_test):\n",
    "    pred_test = model.predict_proba(r_test)[:, 1]\n",
    "    # pred_test = model.predict(r_test)\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=pred_test,\n",
    "        columns=[\"pred\"],\n",
    "        index=r_test.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Merge with Target\n",
    "    pred_test2 = pd.merge(\n",
    "        pred_test[\"pred\"],\n",
    "        dfr_test_with_label[\"target\"],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff 50percentile of the distribution\n",
    "    q1 = pred_test2[\"pred\"].quantile(q=1 - conservative_dr)\n",
    "    pred_test2[\"prediction_baseline\"] = pred_test2[\"pred\"].apply(\n",
    "        lambda x: 0 if (x < q1) else 1\n",
    "    )\n",
    "    return pred_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_combined(model, X_test, r_test):\n",
    "    # Attach target to X_test and r_test\n",
    "\n",
    "    r_test_target = pd.merge(\n",
    "        r_test,\n",
    "        dfr_test_with_label[\"target\"],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    X_test_target = pd.merge(\n",
    "        X_test,\n",
    "        y_test[\"target\"],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Concatenate labels\n",
    "    ta_tr_labels = pd.concat([X_test_target, r_test_target], axis=0)\n",
    "\n",
    "    # Concatenate Test Accepts and Test Rejects\n",
    "    ta_tr = pd.concat([X_test, r_test], axis=0)\n",
    "\n",
    "    pred_test = model.predict_proba(ta_tr)[:, 1]\n",
    "    # pred_test = model.predict(r_test)\n",
    "    pred_test = pd.DataFrame(\n",
    "        data=pred_test,\n",
    "        columns=[\"pred\"],\n",
    "        index=ta_tr.index.copy(),\n",
    "    )\n",
    "\n",
    "    # Merge with Target\n",
    "    pred_test2 = pd.merge(\n",
    "        pred_test[\"pred\"],\n",
    "        ta_tr_labels[\"target\"],\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Make binary predictions based on cutoff 50percentile of the distribution\n",
    "    q1 = pred_test2[\"pred\"].quantile(q=1 - conservative_dr)\n",
    "    pred_test2[\"prediction_baseline\"] = pred_test2[\"pred\"].apply(\n",
    "        lambda x: 0 if (x < q1) else 1\n",
    "    )\n",
    "    return pred_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_df_baseline(df):\n",
    "\n",
    "    # Flag kicked out bad cases (want more of these)\n",
    "    if df[\"target\"] == 1 and df[\"prediction_baseline\"] == 1:\n",
    "        return \"CB\"\n",
    "\n",
    "    # Flag kicked out good cases (want less of these)\n",
    "    elif df[\"target\"] == 1 and df[\"prediction_baseline\"] == 0:\n",
    "        return \"IB\"\n",
    "\n",
    "    # Flag kicked in good cases (want more of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_baseline\"] == 0:\n",
    "        return \"CG\"\n",
    "\n",
    "    # Flag kicked in bad cases (want less of these)\n",
    "    elif df[\"target\"] == 0 and df[\"prediction_baseline\"] == 1:\n",
    "        return \"IG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kickout_baseline(df):\n",
    "\n",
    "    # Counts of kickout bad and kickout good\n",
    "    counts = df[\"Flag\"].value_counts()\n",
    "    if \"CB\" in df.values:\n",
    "        cb = counts.CB  # want more of these\n",
    "    else:\n",
    "        cb = 0\n",
    "    if \"IB\" in df.values:\n",
    "        ib = counts.IB  # want less of these\n",
    "    else:\n",
    "        ib = 0\n",
    "\n",
    "    if \"CG\" in df.values:\n",
    "        cg = counts.CG  # want more of these\n",
    "    else:\n",
    "        cg = 0\n",
    "\n",
    "    if \"IG\" in df.values:\n",
    "        ig = counts.IG  # want less of these\n",
    "    else:\n",
    "        ig = 0\n",
    "\n",
    "    # Target\n",
    "    total_bads = df[df[\"target\"] == 1].shape[0]\n",
    "    total_goods = df[df[\"target\"] == 0].shape[0]\n",
    "    pb = total_bads / (total_bads + total_goods)\n",
    "    pg = total_goods / (total_bads + total_goods)\n",
    "\n",
    "    kickout = (((cb / pb) - (ib / pb)) / total_bads) * (pb ** 2)\n",
    "    kickin = (((cg / pg) - (ig / pg)) / total_goods) * (pg ** 2)\n",
    "    weighted_total = kickout + kickin\n",
    "    return weighted_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
